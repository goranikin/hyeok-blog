[
  {
    "title": "Attention is all you need",
    "description": "Deep dive into the Transformer architecture, especially the implementation in each layers.",
    "slug": "attention-is-all-you-need",
    "publishDate": "2025-04-13",
    "thumbnailUrl": "/study/paper-review/attention-is-all-you-need/thum.jpeg",
    "content": "const{Fragment:l,jsx:e,jsxs:n}=arguments[0];function _createMdxContent(s){const r={a:\"a\",code:\"code\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",pre:\"pre\",span:\"span\",...s.components};return n(l,{children:[n(r.p,{children:[\"In this post, I will explain the \",e(r.a,{href:\"https://arxiv.org/abs/1706.03762\",children:\"transformer\"}),\" architecture and its implementation at the code level. I referred to a \",e(r.a,{href:\"https://github.com/jadore801120/attention-is-all-you-need-pytorch\",children:\"scratch\"}),\" implementation for the PyTorch code!\"]}),\"\\n\",e(r.p,{children:e(r.img,{src:\"/study/paper-review/attention-is-all-you-need/1.png\",alt:\"\"})}),\"\\n\",e(r.p,{children:\"The authors of the paper all left Google and founded their own companies, except for Lukasz Kaiser, who joined OpenAI. Five out of the 7 companies have become unicorns, with a combined valuation exceeding $17 billion. It's quite impressive.\"}),\"\\n\",e(r.p,{children:e(r.img,{src:\"/study/paper-review/attention-is-all-you-need/2.png\",alt:\"\"})}),\"\\n\",e(r.p,{children:\"This is Figure 1 from the paper. It illustrates the overall architecture of the Transformer.\"}),\"\\n\",e(r.p,{children:\"They designed the operation, called Scaled Dot-Product Attention, which calculates the attention scores between query and key vectors to reflect the context information between the tokens.\"}),\"\\n\",e(r.p,{children:\"They represented the formula like this.\"}),\"\\n\",e(r.p,{children:e(r.img,{src:\"/study/paper-review/attention-is-all-you-need/3.png\",alt:\"\"})}),\"\\n\",e(r.p,{children:\"How can we implement Scaled Dot-Product Attention in PyTorch? Let's start from scratch.\"}),\"\\n\",e(l,{children:e(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:n(r.code,{children:[n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" ScaledDotProductAttention\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),e(r.span,{style:{color:\"#24292E\"},children:\".\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, temperature, attn_dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0.1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),e(r.span,{style:{color:\"#24292E\"},children:\"().\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".temperature \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" temperature\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Dropout(attn_dropout)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, q, k, v, mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"None\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # q = [batch_size, n_head, len_q, d_k]\"})}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # k = [batch_size, n_head, len_k, d_k]\"})}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # v = [batch_size, n_head, len_k, d_v]\"})}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" torch.matmul(q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"/\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".temperature, k.transpose(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"3\"}),e(r.span,{style:{color:\"#24292E\"},children:\"))\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        if\"}),e(r.span,{style:{color:\"#24292E\"},children:\" mask \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"is\"}),e(r.span,{style:{color:\"#D73A49\"},children:\" not\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" None\"}),e(r.span,{style:{color:\"#24292E\"},children:\":\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"            attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" attn.masked_fill(mask \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"==\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" 0\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"-\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1e9\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout(F.softmax(attn, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dim\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=-\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"))\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        output \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" torch.matmul(attn, v)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),e(r.span,{style:{color:\"#24292E\"},children:\" output, attn\"})]})]})})}),\"\\n\",n(r.p,{children:[\"The original method uses the square root of \",e(r.code,{children:\"d_model\"}),\" for normalization, but this approach uses a learnable temperature parameter during training. A higher temperature value results in a more uniform attention distribution.\"]}),\"\\n\",n(r.p,{children:[e(r.code,{children:\"attn\"}),\" has a shape \",e(r.code,{children:\"[batch_size, n_head, len_q, len_k]\"}),\", which represents the attention scores between query and key vectors. For example, \",e(r.code,{children:\"a[:,:,1,2]\"}),\" is the attention score between the first query vector and the second key vector.\"]}),\"\\n\",e(r.p,{children:\"A mask is applied to ignore padding tokens in the sequence for efficiency and to ensure that self-attention in the decoder does not reference future tokens.\"}),\"\\n\",n(r.p,{children:[\"The output has a shape of \",e(r.code,{children:\"[batch_size, n_head, len_q, d_v]\"})]}),\"\\n\",e(r.p,{children:\"This is just one module used in the Multi-Head attention mechanism.\"}),\"\\n\",e(r.p,{children:e(r.img,{src:\"/study/paper-review/attention-is-all-you-need/4.png\",alt:\"\"})}),\"\\n\",e(l,{children:e(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:n(r.code,{children:[n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" MultiHeadAttention\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),e(r.span,{style:{color:\"#24292E\"},children:\".\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, n_head, d_model, d_k, d_v, dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0.1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),e(r.span,{style:{color:\"#24292E\"},children:\"().\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" n_head\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".d_k \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_k\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".d_v \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_v\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_qs \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(d_model, n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_k, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"bias\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"False\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_ks \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(d_model, n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_k, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"bias\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"False\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_vs \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(d_model, n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_v, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"bias\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"False\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".fc \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),e(r.span,{style:{color:\"#24292E\"},children:\" d_v, d_model, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"bias\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"False\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".attention \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" ScaledDotProductAttention(\"}),e(r.span,{style:{color:\"#E36209\"},children:\"temperature\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"d_k \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"**\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" 0.5\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Dropout(dropout)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".layer_norm \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.LayerNorm(d_model, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"eps\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1e-6\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, q, k, v, mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"None\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # q = [batch_size, len_q, d_model]\"})}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        d_k, d_v, n_head \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".d_k, \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".d_v, \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".n_head\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        sz_b, len_q, len_k, len_v \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" q.size(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0\"}),e(r.span,{style:{color:\"#24292E\"},children:\"), q.size(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"), k.size(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"), v.size(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        residual \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" q \"}),e(r.span,{style:{color:\"#6A737D\"},children:\"# enc_input\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # Separate different heads: [batch, len, n_head, d] (d_model / n_head = d_k)\"})}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_qs(q).view(sz_b, len_q, n_head, d_k)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        k \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_ks(k).view(sz_b, len_k, n_head, d_k)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        v \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_vs(v).view(sz_b, len_v, n_head, d_v)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # Transpose for attention dot product: [batch, len, n_head, d]\"})}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q, k, v \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" q.transpose(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),e(r.span,{style:{color:\"#24292E\"},children:\"), k.transpose(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),e(r.span,{style:{color:\"#24292E\"},children:\"), v.transpose(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        if\"}),e(r.span,{style:{color:\"#24292E\"},children:\" mask \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"is\"}),e(r.span,{style:{color:\"#D73A49\"},children:\" not\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" None\"}),e(r.span,{style:{color:\"#24292E\"},children:\":\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"            mask \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" mask.unsqueeze(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q, attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".attention(q, k, v, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"mask)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # Transpose to move the head dimension back: b x lq x n x dv\"})}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\"})}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" q.transpose(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\", \"}),e(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),e(r.span,{style:{color:\"#24292E\"},children:\").contiguous().view(sz_b, len_q, \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"-\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".fc(q))\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#6A737D\"},children:\"        # LayerNorm(x + Sublayer(x))\"})}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"+=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" residual\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        q \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".layer_norm(q)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),e(r.span,{style:{color:\"#24292E\"},children:\" q, attn\"})]})]})})}),\"\\n\",n(r.p,{children:[\"Wait, what is the layer function? And what does \",e(r.code,{children:\"fc\"}),\" mean? I recommend reading \",e(r.a,{href:\"https://thecho7.tistory.com/entry/PyTorch-nnLinear%EC%97%90-%EB%8C%80%ED%95%9C-%EC%A7%88%EB%AC%B8\",children:\"pytorch layer\"}),\" (this is a Korean blog), which provides a concise explanation.\"]}),\"\\n\",n(r.p,{children:[\"The Multi-Head Attention mechanism takes parameters \",e(r.code,{children:\"q\"}),\", \",e(r.code,{children:\"k\"}),\", and \",e(r.code,{children:\"v\"}),\". In practice, all of them are derived from the same input \",e(r.code,{children:\"x\"}),\". This is why the residual connection is applied to \",e(r.code,{children:\"q\"}),\".\"]}),\"\\n\",n(r.p,{children:[\"After performing Scaled Dot-Product attention and applying \",e(r.code,{children:\"transpose(1, 2).contiguous().view(sz_b, len_q, -1)\"}),\", \",e(r.code,{children:\"q\"}),\" has a shape of \",e(r.code,{children:\"[sz_batch, len_q, n_head * d_v]\"}),\". In the original architecture, \",e(r.code,{children:\"d_v = d_k\"}),\", but when \",e(r.code,{children:\"d_v != d_k\"}),\", it ensures that the output and input dimensions of the \",e(r.code,{children:\"fc\"}),\" layer remain consistent.\"]}),\"\\n\",e(l,{children:e(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:n(r.code,{children:[n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" PositionwiseFeedForward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),e(r.span,{style:{color:\"#24292E\"},children:\".\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, d_in, d_hid, dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0.1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),e(r.span,{style:{color:\"#24292E\"},children:\"().\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_1 \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(d_in, d_hid)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_2 \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Linear(d_hid, d_in)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".layer_norm \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.LayerNorm(d_in, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"eps\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"1e-6\"}),e(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" nn.Dropout(dropout)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, x):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        residual \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" x\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        x \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_2(F.relu(\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".w_1(x)))\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        x \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".dropout(x)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        x \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"+=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" residual\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        x \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".layer_norm(x)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),e(r.span,{style:{color:\"#24292E\"},children:\" x\"})]})]})})}),\"\\n\",e(r.p,{children:\"This is Feed Forward Network. It is a simple linear transformation followed by a ReLU activation function and a dropout layer.\"}),\"\\n\",e(l,{children:e(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:n(r.code,{children:[n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" EncoderLayer\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),e(r.span,{style:{color:\"#24292E\"},children:\".\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, d_model, d_inner, n_head, d_k, d_v, dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0.1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),e(r.span,{style:{color:\"#24292E\"},children:\"().\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".slf_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" MultiHeadAttention(n_head, d_model, d_k, d_v, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".pos_ffn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" PositionwiseFeedForward(d_model, d_inner, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, enc_input, slf_attn_mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"None\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        enc_output, enc_slf_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".slf_attn(\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"            enc_input, enc_input, enc_input, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"slf_attn_mask)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        enc_output \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".pos_ffn(enc_output)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),e(r.span,{style:{color:\"#24292E\"},children:\" enc_output, enc_slf_attn\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" DecoderLayer\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),e(r.span,{style:{color:\"#24292E\"},children:\".\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, d_model, d_inner, n_head, d_k, d_v, dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"0.1\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),e(r.span,{style:{color:\"#24292E\"},children:\"().\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),e(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".slf_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" MultiHeadAttention(n_head, d_model, d_k, d_v, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".enc_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" MultiHeadAttention(n_head, d_model, d_k, d_v, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".pos_ffn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\" PositionwiseFeedForward(d_model, d_inner, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"dropout\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),e(r.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),e(r.span,{style:{color:\"#24292E\"},children:\"(self, dec_input, enc_output, self_attn_mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"None\"}),e(r.span,{style:{color:\"#24292E\"},children:\", dec_enc_attn_mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\"None\"}),e(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        dec_output, dec_slf_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".slf_attn(\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"            dec_input, dec_input, dec_input, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"self_attn_mask\"})]}),\"\\n\",e(r.span,{className:\"line\",children:e(r.span,{style:{color:\"#24292E\"},children:\"        )\"})}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        dec_output, dec_enc_attn \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".enc_attn(\"})]}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"            dec_output, enc_output, enc_output, \"}),e(r.span,{style:{color:\"#E36209\"},children:\"mask\"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#24292E\"},children:\"dec_enc_attn_mask)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#24292E\"},children:\"        dec_output \"}),e(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),e(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),e(r.span,{style:{color:\"#24292E\"},children:\".pos_ffn(dec_output)\"})]}),\"\\n\",e(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:[e(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),e(r.span,{style:{color:\"#24292E\"},children:\" dec_output, dec_slf_attn, dec_enc_attn\"})]})]})})}),\"\\n\",n(r.p,{children:[\"In fact, the Encoder and Decoder are straightforward implementations of their respective layers. It's more an algorithmic problem - determining where to provide the input and how to obtain the output. Therefore, I'll skip the following code snippets. If you want to a deeper dive into the Transformer code, please refer to the \",e(r.a,{href:\"https://github.com/jadore801120/attention-is-all-you-need-pytorch\",children:\"scratch\"}),\".\"]}),\"\\n\",e(r.hr,{}),\"\\n\",e(r.p,{children:\"reference\"}),\"\\n\",n(r.ol,{children:[\"\\n\",n(r.li,{children:[\"Attention is all you need: \",e(r.a,{href:\"https://arxiv.org/abs/1706.03762\",children:\"https://arxiv.org/abs/1706.03762\"})]}),\"\\n\",n(r.li,{children:[\"transformer code: \",e(r.a,{href:\"https://github.com/jadore801120/attention-is-all-you-need-pytorch\",children:\"https://github.com/jadore801120/attention-is-all-you-need-pytorch\"})]}),\"\\n\"]}),\"\\n\",e(r.hr,{})]})}return{default:function(l={}){const{wrapper:n}=l.components||{};return n?e(n,{...l,children:e(_createMdxContent,{...l})}):_createMdxContent(l)}};",
    "permalink": "/research/attention-is-all-you-need"
  },
  {
    "title": "CLIP",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - Learning Transferable Visual Models From Natural Language Supervision",
    "slug": "clip",
    "publishDate": "2025-07-14",
    "thumbnailUrl": "/study/paper-review/clip/thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:l}=arguments[0];function _createMdxContent(s){const r={a:\"a\",br:\"br\",code:\"code\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",pre:\"pre\",span:\"span\",strong:\"strong\",table:\"table\",tbody:\"tbody\",td:\"td\",th:\"th\",thead:\"thead\",tr:\"tr\",ul:\"ul\",...s.components};return l(e,{children:[n(r.p,{children:\"7월부터 DSBA 연구실에서 인턴을 하는 중입니다. 저와 같은 AI 무지랭이를 위해 감사하게도 사전학습이란 제도가 있습니다. 연구실의 관심 분야인 CV(Computer Vision), NLP(Natural Language Process), TS(Time Series)에 대해 각각 분야 별 논문들을 읽고, Notion에 정리한 뒤, 기존 연구원 분들 앞에서 읽은 내용에 대해 1) 어떻게 읽었는지, 2) 느낀 점은 무엇인지, 3) 읽은 뒤 궁금한 점은 무엇인지 들을 이야기하는 시스템입니다.\"}),\"\\n\",n(r.p,{children:\"이와 관련하여 인턴 동안 Notion에 적었던 내용을 블로그에 옮기려 합니다. 이후 마지막 단락에서 짧은 개인적인 생각을 남겨보겠습니다.\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.p,{children:\"*This is a same content with the above paragraph.\"}),\"\\n\",n(r.p,{children:\"I've been interning at the DSBA lab at SNU since July. Thankfully, there is a preparatory program for freshman and intern who have little to no background in AI research. The program focuses on DSBA's main research interests: Computer Vision (CV), Natural Language Process (NLP), and Time Series (TS). As part of the program, I read papers in each field, summarize them in Notion, and then present to the current researchers about 1) how I approached the paper, 2) what I thought or felt about it, and 3) any questions that came up during my reading.\"}),\"\\n\",n(r.p,{children:\"I'm planning to transfer the contents I wrote in Notion to my blog. At the end of each post, I'll also share some brief personal thoughts.\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.p,{children:\"다음 구분 선까지 Notion 내용입니다.\"}),\"\\n\",n(r.p,{children:n(r.a,{href:\"https://arxiv.org/abs/2103.00020\",children:\"Paper Link\"})}),\"\\n\",n(r.h2,{children:\"1. Introduction\"}),\"\\n\",l(r.ol,{children:[\"\\n\",n(r.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"Vision model training with natural language\"}),\"\\n\",n(r.li,{children:\"Multi-modal models\"}),\"\\n\",n(r.li,{children:\"Natural language supervision and weakly-supervised training\"}),\"\\n\",n(r.li,{children:\"Transfer learning\"}),\"\\n\",n(r.li,{children:\"Zero-shot and few-shot transfer task capabilities\"}),\"\\n\"]}),\"\\n\",l(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"Limitations of previous studies in this task\"}),\"\\n\"]}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"Existing datasets for training vision models are limited in size, as they are labeled data.\"}),\"\\n\",n(r.li,{children:\"Existing models lack the ability to perform zero-shot transfer on various downstream tasks.\"}),\"\\n\"]}),\"\\n\",l(r.ol,{start:\"3\",children:[\"\\n\",n(r.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",l(r.ul,{children:[\"\\n\",l(r.li,{children:[\"Presenting a large-scale dataset and a pre-training methodology based on weakly-supervised training)\",\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"(The paper states that) Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"Focusing on the capability to solve task using zero-shot transfer\",\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"While existing models focus on learning better representations, CLIP is designed to directly solve tasks (e.g., shifting from 'how to represent a cat image well' to 'how to classify a cat image well')\"}),\"\\n\",n(r.li,{children:\"(The paper states that) While much research in the field of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task-learning capabilities of machine learning systems.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"2. Related Work\"}),\"\\n\",l(r.ul,{children:[\"\\n\",l(r.li,{children:[\"Improved deep metric learning with multi-class n-pair loss objective, Advances in neural information processing systems 2016\",\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"The N-pair loss was intoduced to efficiently learn representations by considering samples from multiple classes at once.\"}),\"\\n\",n(r.li,{children:\"CLIP employed the N-pair contrastive loss\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"Learning Visual Features from Large Weakly Supervised Data, ECCV 2016\",\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"A CNN-based model was trained using a large-scale weakly-supervised dataset.\"}),\"\\n\",n(r.li,{children:\"The model could achieve generalization even when the labels were weak (contaminated or incomplete), as long as there was enough data.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"Learning Visual N-grams from Web Data, IEEE 2017\",\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"The model is trained to associate N-grams from images and text\"}),\"\\n\",n(r.li,{children:\"It models concepts not as single word, but as phrases or even entire sentences.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.li,{children:\"VirTex, ICMLM, ConVIRT\\n_ These models also use a text-image architeccture, which served as an inspiration for CLIP.\"}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"3. Proposed Methodology\"}),\"\\n\",n(r.h3,{children:\"Main Idea & Contribution\"}),\"\\n\",l(r.ol,{children:[\"\\n\",n(r.li,{children:\"Architecture\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/clip/1.png\",alt:\"image.png\"})}),\"\\n\",n(r.p,{children:\"Text encoder: Transformer - 63M parameter, 12 layer, 512 wide model with 8 heads\"}),\"\\n\",n(r.p,{children:\"Image encoder\"}),\"\\n\",l(r.table,{children:[n(r.thead,{children:l(r.tr,{children:[n(r.th,{}),n(r.th,{children:\"Base style\"}),n(r.th,{children:\"EfficientNet style\"}),n(r.th,{})]})}),n(r.tbody,{children:l(r.tr,{children:[n(r.td,{children:\"ResNe\"}),n(r.td,{children:\"ResNet-50, ResNet-101\"}),n(r.td,{children:\"RN50x4, RN50x16, RN50x64\"}),n(r.td,{})]})})]}),\"\\n\",l(r.table,{children:[n(r.thead,{children:l(r.tr,{children:[n(r.th,{}),n(r.th,{children:\"Base\"}),n(r.th,{children:\"Large\"})]})}),n(r.tbody,{children:l(r.tr,{children:[n(r.td,{children:\"ViT\"}),n(r.td,{children:\"16, 32\"}),n(r.td,{children:\"14, 14@336px\"})]})})]}),\"\\n\",n(r.p,{children:\"CLIP model used ViT-L/14@336px, which showed the best performance)\"}),\"\\n\",n(r.p,{children:\"For the largest model, RN50x64 was trained on 592 V100 GPUs for 18 days, while ViT-L/14 was trained on 256 V100 GPUs for 12 days.\"}),\"\\n\",l(r.p,{children:[\"An implementaion of the code. (\",n(r.a,{href:\"https://github.com/openai/CLIP\",children:\"https://github.com/openai/CLIP\"}),\")\"]}),\"\\n\",n(r.p,{children:\"Skip an unnecassary code.\"}),\"\\n\",n(e,{children:n(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(r.code,{children:[l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"class\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\" CLIP\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),n(r.span,{style:{color:\"#24292E\"},children:\".\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),n(r.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(self,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 embed_dim: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"                 # vision\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 image_resolution: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 vision_layers: Union[Tuple[\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\"], \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\"],\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 vision_width: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 vision_patch_size: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"                 # text\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 context_length: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 vocab_size: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 transformer_width: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 transformer_heads: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"                 transformer_layers: \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"int\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"                 ):\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#005CC5\"},children:\"        super\"}),n(r.span,{style:{color:\"#24292E\"},children:\"().\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),n(r.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"        if\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" isinstance\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(vision_layers, (\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"tuple\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"list\"}),n(r.span,{style:{color:\"#24292E\"},children:\")):\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"            vision_heads \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" vision_width \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" 32\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" //\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" 64\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#005CC5\"},children:\"            self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".visual \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" ModifiedResNet(\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                layers\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_layers,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                output_dim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"embed_dim,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                heads\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_heads,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                input_resolution\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"image_resolution,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                width\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_width\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"            )\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"        else\"}),n(r.span,{style:{color:\"#24292E\"},children:\":\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"            vision_heads \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" vision_width \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"//\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" 64\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#005CC5\"},children:\"            self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".visual \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" VisionTransformer(\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                input_resolution\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"image_resolution,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                patch_size\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_patch_size,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                width\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_width,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                layers\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_layers,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                heads\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"vision_heads,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"                output_dim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"embed_dim\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"            )\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".transformer \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" Transformer(\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"            width\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"transformer_width,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"            layers\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"transformer_layers,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"            heads\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\"transformer_heads,\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"            attn_mask\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".build_attention_mask()\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"            \"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(r.span,{style:{color:\"#24292E\"},children:\" encode_image(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\", image):\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".visual(image.type(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".dtype))\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(r.span,{style:{color:\"#24292E\"},children:\" encode_text(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\", text):\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".token_embedding(text).type(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".dtype)  \"}),n(r.span,{style:{color:\"#6A737D\"},children:\"# [batch_size, n_ctx, d_model]\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" x \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"+\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".positional_embedding.type(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".dtype)\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" x.permute(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),n(r.span,{style:{color:\"#24292E\"},children:\")  \"}),n(r.span,{style:{color:\"#6A737D\"},children:\"# NLD -> LND\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".transformer(x)\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" x.permute(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#005CC5\"},children:\"2\"}),n(r.span,{style:{color:\"#24292E\"},children:\")  \"}),n(r.span,{style:{color:\"#6A737D\"},children:\"# LND -> NLD\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".ln_final(x).type(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".dtype)\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"        # x.shape = [batch_size, n_ctx, transformer.width]\"})}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"        # take features from the eot embedding (eot_token is the highest number in each sequence)\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        x\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" x[torch.arange(x.shape[\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(r.span,{style:{color:\"#24292E\"},children:\"]), text.argmax(\"}),n(r.span,{style:{color:\"#E36209\"},children:\"dim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=-\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\")] \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"@\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".text_projection\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(r.span,{style:{color:\"#24292E\"},children:\" x\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"            \"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(r.span,{style:{color:\"#24292E\"},children:\" forward(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(r.span,{style:{color:\"#24292E\"},children:\", image, text):\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        image_features\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".encode_image(image)\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        text_features\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".encode_text(text)\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"        # normalized features\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        image_features\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" image_features \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"/\"}),n(r.span,{style:{color:\"#24292E\"},children:\" image_features.norm(\"}),n(r.span,{style:{color:\"#E36209\"},children:\"dim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#E36209\"},children:\"keepdim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"True\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        text_features\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" text_features \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"/\"}),n(r.span,{style:{color:\"#24292E\"},children:\" text_features.norm(\"}),n(r.span,{style:{color:\"#E36209\"},children:\"dim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\", \"}),n(r.span,{style:{color:\"#E36209\"},children:\"keepdim\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"True\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"        # cosine similarity as logits\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        logit_scale\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(r.span,{style:{color:\"#24292E\"},children:\".logit_scale.exp()\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        logits_per_image\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" logit_scale \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"*\"}),n(r.span,{style:{color:\"#24292E\"},children:\" image_features \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"@\"}),n(r.span,{style:{color:\"#24292E\"},children:\" text_features.t()\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#E36209\"},children:\"        logits_per_text\"}),n(r.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(r.span,{style:{color:\"#24292E\"},children:\" logits_per_image.t()\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"        # shape = [global_batch_size, global_batch_size]\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(r.span,{style:{color:\"#24292E\"},children:\" logits_per_image, logits_per_text\"})]})]})})}),\"\\n\",n(r.p,{children:\"You can find clean and well-organized from-scratch implementations of ResNet, ViT, and Transformer in the CLIP/clip/model.py file of the repository. It's a great reference if you want to see how these models are implemented from the ground up.\"}),\"\\n\",l(r.ol,{children:[\"\\n\",n(r.li,{children:\"Natural Language Supervision\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"(The paper states that) It’s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification. (…ellipsis) methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet.\"}),\"\\n\",n(r.p,{children:\"OpenAI, known for gathering massive datasets for GPT training, demonstrates impressive web crawling capabilities.\"}),\"\\n\",l(r.ol,{children:[\"\\n\",l(r.li,{children:[\"\\n\",n(r.p,{children:\"They set up 500K queries and crawled (image, text) pairs for each.\"}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"\\n\",n(r.p,{children:\"For each query, they collected up to 20,000 pairs, ensuring balanced class distribution.\"}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"\\n\",n(r.p,{children:\"In total, they gathered 400 million paris--comparable in total word count to the WebText dataset used for GPT-2 training.\"}),\"\\n\"]}),\"\\n\",l(r.li,{children:[\"\\n\",n(r.p,{children:\"Training Method\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"(The paper states that) Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N ×N possible (image, text) pairings across a batch actually occurred.\"}),\"\\n\",n(r.p,{children:\"This results in an N x N matrix of image and text pairs, where only the diagonal elements are considered positive examples, and all others are treated as negatives during training.\"}),\"\\n\",l(r.p,{children:[\"Q) How does the model handle false negatives?\",n(r.br,{}),\"\\n\",\"A) As long as the dataset is sufficiently large, even if some data is low-quality, it does not significantly hinder training. (This demonstrates the strength of weak supervision.)\"]}),\"\\n\",n(r.p,{children:\"A key question to verify through experiments:\\nWill zero-shot transfer performance truly improve during pre-training, as it did with GPT?\"}),\"\\n\",n(r.h2,{children:\"4. Expreiments and Results\"}),\"\\n\",n(r.p,{children:\"This paper is quite lengthy, with most of its content dedicated to experimental design, results, and their significance. Therefore, I will go through each experiment one by one.\"}),\"\\n\",n(r.h3,{children:\"1) vs Visual N-Grams\"}),\"\\n\",l(r.p,{children:[n(r.img,{src:\"/study/paper-review/clip/2.png\",alt:\"\"}),\"\\nAs mentioned in the paper, Visual N-Grams and CLIP are not directly comparable models due to their architecture, dataset what they train, and so on. However, this table demonstrates just how much zero-shot performance has improved.\"]}),\"\\n\",n(r.h3,{children:\"2) Prompt Engineering for Image Encoding\"}),\"\\n\",n(r.p,{children:\"During pre-training, the text in CLIP's dataset consist of phrases or sentences (captions) that describe the image. However, in some evaluation datasets, only the class label is provided. In such cases, even simple prompt engineering can lead to significant performance improvements.\"}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:'Basic template: \"A photo of a label\" -> 1.3% improvement on ImageNet dataset'}),\"\\n\",n(r.li,{children:'Oxford-IIIT Pets: \"A photo of a lable, a type of pet\"'}),\"\\n\",n(r.li,{children:'OCR: Add quotes around characters or numbers, e.g., 1, a -> \"1\", \"a\"'}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Beyond these, various context prompts led to a total improvement of 3.5% on ImageNet. (The promptes used are summarized in CLIP/data/prompts.md on GitHub.)\"}),\"\\n\",n(r.p,{children:\"Examples of prompt.\"}),\"\\n\",n(e,{children:n(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(r.code,{children:[l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"templates \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" [\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a blurry photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a black and white photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a low contrast photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a high contrast photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a bad photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a good photo of a {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of a small {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of a big {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a blurry photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a black and white photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a low contrast photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a high contrast photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a bad photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a good photo of the {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of the small {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#032F62\"},children:\"    'a photo of the big {}.'\"}),n(r.span,{style:{color:\"#24292E\"},children:\",\"})]}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#24292E\"},children:\"]\"})})]})})}),\"\\n\",n(r.p,{children:\"The prompts are ensembeld by pooling the embeddings obtained from each individual prompt input.\"}),\"\\n\",l(r.p,{children:[\"Q) Does this increase inference time?\",n(r.br,{}),\"\\n\",\"A) The text encoder computes embeddings for the ground-truth labels in advance, and during inference, only the similarity with the image embedding needs to be calculated. Therefore, as long as the cached values are used, increasing the number of prompt templates does not proportionally increase inference cost!\"]}),\"\\n\",n(r.h3,{children:\"3) Zero-Shot CLIP Performance\"}),\"\\n\",l(r.p,{children:[n(r.strong,{children:\"vs fully Supervised models\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/3.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"On quite a few datasets, CLUP outperforms the fully supervised baselines.\"}),\"\\n\",n(r.p,{children:\"Which datasets showed poor performance?\"}),\"\\n\",n(r.p,{children:\"→ (The paper states that) we see that zero-shot CLIP is quite weak on several specialized, complex, or abstract tasks such as satellite image classification (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes (CLEVRCounts), self-driving related tasks such as German traffic sign recognition (GTSRB), recognizing distance to the nearest car (KITTI Distance).\"}),\"\\n\",n(r.p,{children:\"For highly specialized or complex image recognition tasks, transfer performance tends to be poor.\"}),\"\\n\",n(r.p,{children:\"However, these cases likely involve classes that were ralely, if ever, encountered during pre-training. Considering the performance of non-expert humans on such zero-shot tasks, this result is understandable.\"}),\"\\n\",l(r.p,{children:[n(r.strong,{children:\"vs Few-shot models\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/4.png\",alt:\"\"}),\"\\nLinear Probe CLIP is evaluated after being trained with Logistic Regression.\"]}),\"\\n\",n(r.p,{children:\"What's remarkable is that, up to 4-shot, CLIP's zero-shot performance remains higher.\"}),\"\\n\",n(r.p,{children:\"→ (The paper states that) Context-less example-based learning has the drawback that many different hypotheses can be consistent with the data, especially in the one-shot case.\"}),\"\\n\",n(r.p,{children:\"Q) why it does?\"}),\"\\n\",n(r.p,{children:\"A)\"}),\"\\n\",l(r.ol,{children:[\"\\n\",n(r.li,{children:\"Images exhibit much higher variance compared to text. Even within the same class, factors like lighting, angle, background, resolution, style, and noise can make images look completely different.\"}),\"\\n\",l(r.li,{children:[\"In contrast, text is structured and conveys meaning clearly.\",n(r.br,{}),\"\\n\",\"-> The difference in clarity between presenting a cat as a photo versus as a word 'cat'.\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"To sufficiently learn the visual features of images, a large number of few-shot samples is required.\"}),\"\\n\",n(r.p,{children:\"Q) Does this mean that as image resolution increases, even more few-shot samples are needed?\"}),\"\\n\",n(r.p,{children:\"A) (by GPT in Korean)\"}),\"\\n\",n(e,{children:n(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(r.code,{children:[l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"네,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 맞습니다!\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"**\"}),n(r.span,{style:{color:\"#24292E\"},children:\"이미지의 픽셀(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"해상도\"}),n(r.span,{style:{color:\"#24292E\"},children:\")이 늘어날수록, few-shot 학습에서 더 많은 샘플이 필요해지는 경향이 커집니다.\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"**\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"## 이유를 자세히 설명하면\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"### 1. **고차원 공간의 일반화 어려움**\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이미지의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 해상도가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 높아질수록,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 각\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 가지는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 정보\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"특징\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 훨씬\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많아집니다.\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 예를\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 들어,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 32×32\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 픽셀\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이미지는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 1,024차원,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 224×224\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 픽셀\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이미지는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 50,176차원입니다.\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 고차원\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 공간에서는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 데이터가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 희박\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"sparse\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"해지고,\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  클래스\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 경계를\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 잘\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 잡으려면\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 예시가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 필요합니다.\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"### 2. **시각적 다양성 증가**\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 해상도가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 높아지면,\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  같은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 클래스\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 내에서도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 다양한\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 세부적인\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 특징\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"무늬,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 질감,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 배경\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 등\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 나타날\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 있습니다.\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 모델이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이런\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 세부적인\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 차이를\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 일반화하려면\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 필요합니다.\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"### 3. **오버피팅 위험 증가**\"})}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 수가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 적은데\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 픽셀\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"특징\"}),n(r.span,{style:{color:\"#24292E\"},children:\") \"}),n(r.span,{style:{color:\"#032F62\"},children:\"수가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많으면,\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  모델이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 소수\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 세부적인\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 특징에\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 과도하게\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 적합\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"overfit\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"될\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 위험이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 커집니다.\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이를\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 방지하려면\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플로\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 클래스의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 전체적인\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 특징을\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 학습해야\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 합니다.\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"## 실제로 어떻게 나타나는가?\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(r.span,{style:{color:\"#032F62\"},children:\"해상도가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 높을수록\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 같은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 클래스의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 다양한\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 모습\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"포즈,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 배경,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 조명\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 등\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"을\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 포착할\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 있음\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 모델이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이런\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 다양성을\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 학습하려면\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 예시가\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 필요함\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(r.span,{style:{color:\"#032F62\"},children:\"few-shot\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 학습에서\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(r.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 저해상도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이미지\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"예:\"}),n(r.span,{style:{color:\"#032F62\"},children:\" MNIST,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" CIFAR-10\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 1~5장만으로도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 어느\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 정도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 일반화\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 가능\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 고해상도\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이미지\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"예:\"}),n(r.span,{style:{color:\"#032F62\"},children:\" ImageNet,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 의료\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 영상\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 등\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 10장\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이상,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 때로는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 수십~수백\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 장이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 필요할\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 있음\"})]}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",n(r.span,{className:\"line\",children:n(r.span,{style:{color:\"#6A737D\"},children:\"## 결론\"})}),\"\\n\",n(r.span,{className:\"line\"}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(r.span,{style:{color:\"#032F62\"},children:\"이미지의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 픽셀\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\"해상도\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"}),n(r.span,{style:{color:\"#032F62\"},children:\"이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 늘어날수록,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" few-shot\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 학습에서\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 많은\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 샘플이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 필요해지는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 경향이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 커진다\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(r.span,{style:{color:\"#032F62\"},children:\"는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 것이\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 맞습니다.\"})]}),\"\\n\",l(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 이는\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 고차원\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 데이터의\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 일반화\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 어려움,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 시각적\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 다양성,\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 오버피팅\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 위험\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 등\"}),n(r.span,{style:{color:\"#032F62\"},children:\" 때문입니다.\"})]})]})})}),\"\\n\",l(r.p,{children:[n(r.strong,{children:\"the # of samples to deal with zero-shot model\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/5.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"The number of few-shot samples required to match zero-shot performance for each dataset.\"}),\"\\n\",n(r.p,{children:\"Let's take a closer look at the top four datasets:\"}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"FER2013: Facial expression recognition -> It is difficult to learn consistent class representations due to the different ways people express emotions.\"}),\"\\n\",n(r.li,{children:\"CIFAR10: Low resolution and high intra-class variation.\"}),\"\\n\",n(r.li,{children:\"OxfordPets: Likely struggled with distinguishing between similar breeds.\"}),\"\\n\",n(r.li,{children:\"Food101: Even the same dish can look completely different depending on the recipe or the angle of the photo.\"}),\"\\n\"]}),\"\\n\",l(r.p,{children:[\"That said, I haven't examined each dataset in detail--these are just general observations based on their characteristics.\",n(r.br,{}),\"\\n\",\"If a dataset has a large number of features that need to be captured within a class (as in FER2013), or contains a lot of background information unrelated to the class (as in Food101), few-shot learning becomes more challenging.\"]}),\"\\n\",l(r.p,{children:[n(r.strong,{children:\"A theoretical limit performance and the real performances\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/6.png\",alt:\"\"}),\"\\nA comparison between theoretical performance and zero-shot results\\n\",n(r.img,{src:\"/study/paper-review/clip/7.png\",alt:\"\"}),\"\\nComparison of model size and performance -- performance steadily increases, suggesting that larger models can achieve even better results.\"]}),\"\\n\",n(r.h3,{children:\"4) Representation Learning\"}),\"\\n\",n(r.p,{children:\"Although the authors emphasize task learning capability over representation learning capability, they still evaluate the modles using conventional methods.\"}),\"\\n\",n(r.p,{children:\"Experimental setup:\"}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"The projection to the classification space is removed from each model\"}),\"\\n\",n(r.li,{children:\"scikit-learn's L-BFGS implementation is used\"}),\"\\n\",n(r.li,{children:\"The most appropriate metric for each dataset is chosen for evalution (see the images below and Appendix A in the papar for details)\"}),\"\\n\"]}),\"\\n\",l(r.p,{children:[n(r.strong,{children:\"Dataset and meta-information\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/8.png\",alt:\"\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/9.png\",alt:\"\"})]}),\"\\n\",l(r.p,{children:[\"Visualization of the results:\\n\",n(r.img,{src:\"/study/paper-review/clip/10.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"The large-parameter CLIP model demonstrates excellent embedding capabilities!\"}),\"\\n\",n(r.p,{children:\"Q) Is linear probing a good indicaotr of embedding quality?\\nA) If an embedding achieves strong performance with just a simple linear layer, it suggests that the embedding captures a wide range of visual and abstract features across images. Rather than being tailored to specific classes, it provieds a general-purpose representation. In other words, the model excels at extracting features for a variety of classes within images--indicating high-quality embedding representations!\"}),\"\\n\",n(r.h3,{children:\"5) Robustness to Natural Distribution Shift\"}),\"\\n\",n(r.p,{children:\"This section presents experiments on transfer capability, which has been a recurring theme since the introduction.\"}),\"\\n\",n(r.p,{children:\"The paper describes two types of robustness.\"}),\"\\n\",l(r.ol,{children:[\"\\n\",n(r.li,{children:\"Relative Robustness: it captures any improvement in out-of-distribution accuracy.\"}),\"\\n\",n(r.li,{children:\"Effective Robustness: it measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy.\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Of course, for a model to be considered robust, both types of robustness should be high.\"}),\"\\n\",l(r.p,{children:[n(r.img,{src:\"/study/paper-review/clip/11.png\",alt:\"\"}),\"\\nCLIP demonstrates even higher performance on different distributions. For example, on ImageNet-R, while performance of ImageNet-trained ResNet101 drops by about 40%, CLIP's performance actually inreases by 12%. This highlights how powerful it is to understand image structure through natural language--a precedent already set by GPT.\"]}),\"\\n\",l(r.p,{children:[n(r.img,{src:\"/study/paper-review/clip/12.png\",alt:\"\"}),\"\\nWhen trained specifically on ImageNet, robustness to other datsets decreases. However, if you align the class names for zero-shot CLIP, you can see significant performance improvements on certain datasets.\"]}),\"\\n\",l(r.p,{children:[n(r.img,{src:\"/study/paper-review/clip/13.png\",alt:\"\"}),\"\\nZero-shot is much more robust than few-shot.\"]}),\"\\n\",n(r.h3,{children:\"6) Comparison to Human Performance\"}),\"\\n\",n(r.p,{children:\"CLIP is better\"}),\"\\n\",n(r.h3,{children:\"7) Data overlap\"}),\"\\n\",n(r.p,{children:\"One of the challenges in evaluating pre-trained models is that evaluation datasets can sometimes overlap with training data. In this paper, the authors created an overlap detector to construct a clean subset, and fount that, except for a few datasets, the overlap was within acceptable limits. However, for more precise evaluation, it will be necessay to use evaluation datasets created after CLIP.\"}),\"\\n\",n(r.p,{children:\"(The paper states that) Creating a new benchmark of tasks designed explicitly to evaluate broad zero-shot transfer capabilities, rather than re-using existing supervised datasets, would help address these issues.\"}),\"\\n\",n(r.h3,{children:\"8) Bias\"}),\"\\n\",n(r.p,{children:\"Since the model is trained on large-scale internet data, it inevitably contains biases related to sexuality, race, and occupation.\"}),\"\\n\",l(r.p,{children:[n(r.img,{src:\"/study/paper-review/clip/14.png\",alt:\"\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/clip/15.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"Just like with InstructionGPT, effective instruction tuning will be necessary.\"}),\"\\n\",n(r.h2,{children:\"5. Conclusion (Lessons learned)\"}),\"\\n\",n(r.p,{children:\"Thanks to the abundance of insights gained from leveraging massive resources, I found this paper highly engaging despite its considerable length. Each experiment could practically be a research topic on its own.\"}),\"\\n\",l(r.p,{children:[\"On a personal note, I've also been reading papers in the speech-to-text domain. Similar to CLIP, Whisper, which is audio model made by OpenAI, presents a pre-trained model built on a natural supervision dataset and shares a variety of experimental results. Along with GPT-3, it's clear that OpenAI has laid the foundation for most of today's AI models.\",n(r.br,{}),\"\\n\",\"What stood out in this paper was the clear focus on 'task learning capability' rather than just 'representation learning capability,' and the strong intention to build AI models that are meaningful in the real world.\",n(r.br,{}),\"\\n\",'Although OpenAI has since shifted to closed models, their ahcievements so far truly embody the spirit of \"Open\" AI. I was especially moved by the paragraph on the right side of page 25.']}),\"\\n\",l(r.ul,{children:[\"\\n\",n(r.li,{children:\"We hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models, and we are excited to engage with the research community on such questions.\"}),\"\\n\"]}),\"\\n\",n(r.hr,{}),\"\\n\",l(r.p,{children:[\"정말 재미있게 읽었다. 꽤 긴 분량이라 처음에는 어떻게 읽나 싶었는데... 하나하나 살펴보는 재미가 있었고 금방 다 읽었다. 이 논문 하나만으로도 정말 배운 게 많았음.\",n(r.br,{}),\"\\n\",\"실은 OpenAI가 단순히 ChatGPT로 유명한 줄 알았다만, 초창기에 추구했던 가치와 의미를 보니 정말 멋있는 걸 넘어서 애초에 현재 LLM 시대의 완전한 바탕인 셈이었다.\"]})]})}return{default:function(e={}){const{wrapper:l}=e.components||{};return l?n(l,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/clip"
  },
  {
    "title": "DDPM",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - Denoising Diffusion Probabilistic Models",
    "slug": "ddpm",
    "publishDate": "2025-07-17",
    "thumbnailUrl": "/study/paper-review/ddpm/thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:i}=arguments[0];function _createMdxContent(r){const t={br:\"br\",code:\"code\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",pre:\"pre\",ul:\"ul\",...r.components};return i(e,{children:[n(t.h2,{children:\"1. Introduction\"}),\"\\n\",i(t.ol,{children:[\"\\n\",n(t.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Image Generation\"}),\"\\n\"]}),\"\\n\",i(t.ol,{start:\"2\",children:[\"\\n\",n(t.li,{children:\"Limitations of previous studies in this task\"}),\"\\n\"]}),\"\\n\",i(t.ul,{children:[\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"GAN (Generative Adversarial Networks)\"}),\"\\n\",i(t.ul,{children:[\"\\n\",i(t.li,{children:[\"Structural flaws in adversarial architectures:\",\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"The learning process is incomplete and can be easliy disrupted by experimental factors.\"}),\"\\n\",n(t.li,{children:\"GANs are unable to generate a diverse range of images.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"VAE (Variational Autoencoder)\"}),\"\\n\",i(t.ul,{children:[\"\\n\",i(t.li,{children:[\"Limited feature representation:\",\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Images generated by VAEs lack detail and often appear blurry.\"}),\"\\n\",n(t.li,{children:\"VAEs struggle to model complex data distribution.\"}),\"\\n\",n(t.li,{children:\"There is a trade-off: higher image quality often leads to lower diversity.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"Autoregressive model (ex. PixelCNN, PixelRNN)\"}),\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Extreamely high computational cost.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.ol,{start:\"3\",children:[\"\\n\",n(t.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Learning stability\"}),\"\\n\",n(t.li,{children:\"Quality of the samples\"}),\"\\n\",n(t.li,{children:\"Diversity\"}),\"\\n\"]}),\"\\n\",n(t.h2,{children:\"2. Related Work\"}),\"\\n\",n(t.p,{children:\"Skip\"}),\"\\n\",n(t.h2,{children:\"3. Methodology\"}),\"\\n\",n(t.h3,{children:\"Main Idea\"}),\"\\n\",n(t.p,{children:\"The authors propose an architecture of Diffusion Models.\"}),\"\\n\",i(t.ol,{children:[\"\\n\",i(t.li,{children:[\"Forward Process\\n\",n(t.img,{src:\"/study/paper-review/ddpm/1.png\",alt:\"\"}),\"\\n\",n(t.img,{src:\"/study/paper-review/ddpm/2.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\",n(t.p,{children:\"Why?\"}),\"\\n\",n(t.p,{children:n(t.img,{src:\"/study/paper-review/ddpm/3.png\",alt:\"\"})}),\"\\n\",n(t.p,{children:\"Q) Why did they use these alpha and beta?\\nA) There are sevaral methods to make Markov Chain converge to a normal distribution. The authors simply chose this approach, likely because it showed the best performance.\"}),\"\\n\",n(t.p,{children:\"Q) Why do they use a standard normal distribution?\\nA) For convenience. A standard normal distribution is the easiest to work with.\"}),\"\\n\",i(t.ol,{start:\"2\",children:[\"\\n\",i(t.li,{children:[\"\\n\",i(t.p,{children:[\"Reverse Process\\n\",n(t.img,{src:\"/study/paper-review/ddpm/4.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",i(t.p,{children:[\"Loss\\nThe ELBO\\n\",n(t.img,{src:\"/study/paper-review/ddpm/5.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.p,{children:[\"Derivation of Loss\\n\",n(t.img,{src:\"/study/paper-review/ddpm/6.png\",alt:\"\"}),\"\\n\",n(t.img,{src:\"/study/paper-review/ddpm/7.png\",alt:\"\"})]}),\"\\n\",i(t.p,{children:[\"As a result, the loss only depends on the noise(epsilon)! This makes it quite simple and convenient.\",n(t.br,{}),\"\\n\",\"(The paper states that) To summarize, we can train the reverse process mean function approximator mu_theta to predict mu tilda_t, or by modifying its parameterization, we can train it to predict epsilon.\"]}),\"\\n\",i(t.ol,{start:\"4\",children:[\"\\n\",i(t.li,{children:[\"\\n\",i(t.p,{children:[\"Data scaling, Reverse process decoder, and L0\\n\",n(t.img,{src:\"/study/paper-review/ddpm/8.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",i(t.p,{children:[\"Real-Training Objective (the Loss)\\n\",n(t.img,{src:\"/study/paper-review/ddpm/9.png\",alt:\"\"}),\"\\nThe authors remove parameters that depend on t, which improves performance.\\n(The paper states that) These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger terms. We will see in our experiments that this reweighting leads to better sample quality.\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.p,{children:[\"With this objective, DMs are trained using the following algorithm:\\n\",n(t.img,{src:\"/study/paper-review/ddpm/10.png\",alt:\"\"}),\"\\nDuring training, it is sufficient to select only a few values of (t) from 1 to (T). Then, the gradient of L_simple can be calculated based on (x_0), and the model is updated using this gradient for backpropagation.\"]}),\"\\n\",n(t.h2,{children:\"4. Experiments and Results\"}),\"\\n\",n(t.h3,{children:\"Datasets\"}),\"\\n\",n(t.p,{children:\"CIFAR10 (Candian Institute For Advanced Research)\"}),\"\\n\",n(t.h3,{children:\"Results\"}),\"\\n\",n(t.p,{children:n(t.img,{src:\"/study/paper-review/ddpm/11.png\",alt:\"\"})}),\"\\n\",n(t.p,{children:\"The simple objective used in DMs works well!\"}),\"\\n\",i(t.p,{children:[n(t.img,{src:\"/study/paper-review/ddpm/12.png\",alt:\"\"}),\"\\nFurthermore, reparameterizing from from mu to eplsion leads to even better performance.\"]}),\"\\n\",n(t.p,{children:\"After this, the authors discuss progressive coding and generation based on information theory. Since I don't have much background in these topics, I asked AI for clarification.\"}),\"\\n\",n(t.p,{children:\"(In Korean)\"}),\"\\n\",n(t.pre,{children:n(t.code,{className:\"language-Bash\",children:'Progressive Coding 파트 핵심 요약\\n\\n- **Diffusion 모델의 샘플 생성 과정을 정보이론적으로 해석**\\n    \\n    → 이미지를 여러 단계(timestep)에 걸쳐 점진적으로 압축하고 복원하는 과정으로 본다.\\n    \\n- **각 단계별로 필요한 정보량(KL divergence)을 계산**\\n    \\n    → 전체 압축 효율(rate)과 손실(distortion)을 평가할 수 있다.\\n    \\n- **알고리즘적으로, 이미지를 점진적으로 노이즈화(encoding)하고,\\n다시 점진적으로 복원(decoding)하는 방식**\\n→ 이 과정에서 필요한 정보만 코드로 저장/전송하면 효율적인 손실 압축이 가능하다.\\n- **결론:**\\n    \\n    Diffusion 모델은 \"점진적 손실 압축기(progressive lossy compressor)\"로 해석할 수 있으며,\\n    \\n    정보이론적 관점에서 rate-distortion trade-off를 분석할 수 있다.\\n    \\n---\\n\\nProgressive Generation\\n\\n- **핵심:**  \\n  Diffusion 모델의 샘플링(reverse process)은 이미지를 점진적으로 복원하는 과정이다.\\n- **의미:**  \\n  완전히 노이즈 상태에서 시작해, 여러 단계에 걸쳐 점점 더 선명한 이미지를 생성한다.\\n- **장점:**  \\n  중간 단계에서 이미지 품질을 조절하거나, 샘플링 속도를 개선할 수 있는 가능성이 있다.\\n\\n---\\n\\nConnection to Autoregressive Decoding\\n\\n- **핵심:**  \\n  Diffusion 모델의 점진적 생성 방식은 Autoregressive 모델(예: PixelCNN, GPT 등)의 순차적 디코딩과 구조적으로 유사하다.\\n- **의미:**  \\n  두 모델 모두 데이터를 여러 단계/순서에 따라 생성한다.\\n- **인사이트:**  \\n  Diffusion 모델과 Autoregressive 모델의 장단점, 효율성, 품질을 비교/분석할 수 있는 연구적 연결고리를 제공한다.\\n'})}),\"\\n\",n(t.p,{children:\"Additional details can be found in the appendix.\"}),\"\\n\",n(t.p,{children:\"Increasing the number of diffusion steps destroys more structure in the source images, which the model completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.\"}),\"\\n\",n(t.p,{children:n(t.img,{src:\"/study/paper-review/ddpm/13.png\",alt:\"\"})}),\"\\n\",n(t.h2,{children:\"5. Conclusions\"}),\"\\n\",i(t.ul,{children:[\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"What a creative and marvelous process of induction. How do the authors come up with ideas like this?\"}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"There is a fundamental difference between the NLP and image domains.\"}),\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"In most cases, NLP uses cross-entropy loss\"}),\"\\n\",n(t.li,{children:\"In image generation, models use KL divergence, which effectively removes the original data distribution from corss-entropy.\"}),\"\\n\",n(t.li,{children:\"Natural language is highly structured, so we can assume that the real data distribution can be approximated from samples.\"}),\"\\n\",n(t.li,{children:\"However, in image gerenation, we have no information about the distribution of the original images.\"}),\"\\n\",n(t.li,{children:\"Therefore, KL divergence in used to eliminate the unknown original distribution.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(t.li,{children:[\"\\n\",n(t.p,{children:\"A fundamental difference between NLP and Image field.\"}),\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Most cases, NLP uses cross-entropy.\"}),\"\\n\",n(t.li,{children:\"In image generation field, models adopt KL divergence, which remove the original data distribution from cross-entropy\"}),\"\\n\",n(t.li,{children:\"Natural language is quite structural. So, there is a premise that we can approximate the real data distribution from the samples.\"}),\"\\n\",n(t.li,{children:\"But in image generation, we don't have any information about origin images.\"}),\"\\n\",n(t.li,{children:\"Therefore, we have to use KL divergence with removing the origin distribution.\"}),\"\\n\",i(t.li,{children:[\"It tells us some zero-shot CLIP's capability.\",\"\\n\",i(t.ul,{children:[\"\\n\",n(t.li,{children:\"Only one image is inadequate to learn the image distribution, not like NLP.\"}),\"\\n\",n(t.li,{children:\"e.t.c., a 'cat' word and a 'cat image'. Person who have been seeing cat, can't distinguish the cat from the image, containing the background and other obejcts.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(t.hr,{})]})}return{default:function(e={}){const{wrapper:i}=e.components||{};return i?n(i,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/ddpm"
  },
  {
    "title": "GAN",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - Generative Adversarial Networks",
    "slug": "gan",
    "publishDate": "2025-07-24",
    "thumbnailUrl": "/study/paper-review/gan/thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:i}=arguments[0];function _createMdxContent(t){const r={a:\"a\",br:\"br\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",ul:\"ul\",...t.components};return i(e,{children:[n(r.h2,{children:\"1. Introduction\"}),\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Training Archtecture\"}),\"\\n\",n(r.li,{children:\"Image generation\"}),\"\\n\"]}),\"\\n\",i(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"The authors design an architecture of adversarial nets that G and D; a generative model Q and a discriminative model D.\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"G captures the data distribution\"}),\"\\n\",n(r.li,{children:\"D estimates the probability that a sample came from the training data rather than G\"}),\"\\n\",n(r.li,{children:\"G maximizes the probability of D making a mistake\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"2. Methodology\"}),\"\\n\",n(r.h3,{children:\"Main Idea\"}),\"\\n\",n(r.p,{children:\"(본문 내용)  The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\"}),\"\\n\",n(r.p,{children:\"notations:\"}),\"\\n\",n(r.p,{children:\"G(z;theta_g): Generator with parameter theta_g\\nD(x;theta_d): Discriminator with parameter theta_d\"}),\"\\n\",n(r.p,{children:\"The goal is to learn the generator’s distribution over data samples.\"}),\"\\n\",i(r.p,{children:[\"This is a quite good figure to understand the training concept of adversarial nets.\\n\",n(r.img,{src:\"/study/paper-review/gan/1.png\",alt:\"\"})]}),\"\\n\",i(r.p,{children:[\"Value Function V(G,D)\\n\",n(r.img,{src:\"/study/paper-review/gan/2.png\",alt:\"\"})]}),\"\\n\",i(r.p,{children:[\"D(x) is the estimate of the probability that x is a real data sample from the true data distribution.\",n(r.br,{}),\"\\n\",\"1 - D(G(z)) is the probability that the discriminator thinks the sample is fake.\"]}),\"\\n\",i(r.p,{children:[\"Training Algorithm\\n\",n(r.img,{src:\"/study/paper-review/gan/3.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"It updates the D for k steps first, then does the G, where the way is explained above the Figure 1.\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[\"Early stage of learning, G is so poor that D can reject samples with high confidence.\",\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:\"log(1 - D(G(z))) is saturates\"}),\"\\n\",n(r.li,{children:\"we can train G to maximize log(D(G(z)),  rather than training G to minimize log(1-D(G(z))).\"}),\"\\n\",n(r.li,{children:\"b. gives the generator stronger gradients when it is still producing poor samples.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"They show that the value function and the algorithm make G and D optimal states.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/gan/4.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"It shows that D can reach to D^*, which is optimal value. Therefore, C(G) could be minimum when p_data equals p_g.\"}),\"\\n\",n(r.p,{children:\"We need to one more proposition to verify that C(G) can converge to global minimum to make p_g equal to p_data, mentioned on the above.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/gan/5.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"Done!\"}),\"\\n\",n(r.h3,{children:\"Contribution\"}),\"\\n\",n(r.p,{children:\"They suggest the architecture of adversarial nets, which have been used in the ML field widely and frequently due to its simplicity.\"}),\"\\n\",n(r.h2,{children:\"3. Experiments and Results\"}),\"\\n\",n(r.h3,{children:\"Dataset\"}),\"\\n\",n(r.p,{children:\"MNIST(Modified National Institute of Standards and Technology): handwritten digits\"}),\"\\n\",n(r.p,{children:\"TFD(Toronto Face Database): a facial expression recognition\"}),\"\\n\",n(r.h3,{children:\"Baseline\"}),\"\\n\",n(r.p,{children:\"DBN(Deep Belief Network): composed of multiple layers of Restricted Boltzmann Machines(RBMs)\"}),\"\\n\",n(r.p,{children:\"Stracked CAE(Convolutions Autoencoder): multiple convolutional autoencoders\"}),\"\\n\",n(r.p,{children:\"Deep GSN(Deep Generative Stochastic Network): ?\"}),\"\\n\",n(r.h3,{children:\"결과\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/gan/6.png\",alt:\"\"})}),\"\\n\",n(r.h2,{children:\"4. Conclusions\"}),\"\\n\",n(r.p,{children:n(r.a,{href:\"https://github.com/goranikin/DSBA-intern-study/tree/main/GAN\",children:\"https://github.com/goranikin/DSBA-intern-study/tree/main/GAN\"})}),\"\\n\",i(r.p,{children:[\"Scratch로 GAN 구현을 했는데, VAE때와 다르게 print로 출력되는 loss가 요동치는 것을 확인. wandb로 찍어보니…\\n\",n(r.img,{src:\"/study/paper-review/gan/7.png\",alt:\"\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/gan/8.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"epoch 30과 50의 결과. 그닥 다를 것도 없다. 굳이 50번까지 갈 필요는 없는 것 같기도.\"}),\"\\n\",n(r.p,{children:\"논문에서는 G에 gradient를 적용하기 전, D를 k번 학습시키는 알고리듬으로 설명했다. 다만 대부분의 구현에서는 k=1 이었는데 이것 때문인가 싶어 k=4로 늘려본 결과.\"}),\"\\n\",i(r.p,{children:[n(r.img,{src:\"/study/paper-review/gan/9.png\",alt:\"\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/gan/10.png\",alt:\"\"}),\"\\n오히려 y-scale이 늘어남…\"]}),\"\\n\",n(r.p,{children:\"실제로 생성한 샘플들을 눈으로 보는데, 15000 steps보다 G_loss가 높은 마지막 step이 그렇게 구린 것도 아니어서… 잘 모르겠다. FID score를 찍어보려 했는데 ImageNet을 기반으로 만들어진 평가 방법이라 해상도든 뭐든 MNIST와는 거리 가 좀 먼 metric이라 판단.\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[\"Adversarial Learning architecture를 추상적으로 Generator와 Discriminator로만 알고 있었다가 제대로 보니까 또 다름을 느낌.\",\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:\"정확히 어떤 식으로 Loss를 설정했는지\"}),\"\\n\",n(r.li,{children:\"해당 loss의 이론적 근거\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.li,{children:\"‘두 모델이 서로에게서 배운다’는 추상적인 사고가 매우 매력적이고, 어떤 레이어를 갖다 놓든 Generator와 Discriminator라는 개념 자체는 변하지 않으므로… 지금까지 그렇게 다양한 변형이 왜 생겼는지 알 것 같았다. 물론 기본적으로 성능이 좋은 것도 있겠지만.\"}),\"\\n\"]}),\"\\n\",n(r.hr,{})]})}return{default:function(e={}){const{wrapper:i}=e.components||{};return i?n(i,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/gan"
  },
  {
    "title": "Latent Diffusion",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - High-Resolution Image Synthesis with Latent Diffusion Models",
    "slug": "latent-diffusion",
    "publishDate": "2025-07-21",
    "thumbnailUrl": "/study/paper-review/latent-diffusion/thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:i}=arguments[0];function _createMdxContent(t){const r={h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",strong:\"strong\",table:\"table\",tbody:\"tbody\",td:\"td\",th:\"th\",thead:\"thead\",tr:\"tr\",ul:\"ul\",...t.components};return i(e,{children:[n(r.p,{children:\"영어로 다 바꾸기 너무 귀찮아서... 달파 입사 전에 해야 할 것들이 많으므로 Latent Diffusion은 원문 그대로 복붙.\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"1. Introduction\"}),\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Image generation\"}),\"\\n\",n(r.li,{children:\"Auto encoder + Diffusion\"}),\"\\n\",n(r.li,{children:\"Conditional model (sampling + other info. about an image)\"}),\"\\n\"]}),\"\\n\",i(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"Limitations of previous studies in this task\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Diffusion Model(DM)은 pixel 단위의 noise 추가 및 제거 → 연산량이 너무 많다!\"}),\"\\n\",n(r.li,{children:\"연산량을 줄이면서 DM의 생성 능력은 유지할 수 있을까?\"}),\"\\n\",n(r.li,{children:\"학습 시 초기 이미지 데이터 x를 latent space로 축소시킨 다음, DM process를 거쳐보자!\"}),\"\\n\"]}),\"\\n\",i(r.ol,{start:\"3\",children:[\"\\n\",n(r.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Auto Encoder → Latent Representation + Diffusion process 구조 제시\"}),\"\\n\",n(r.li,{children:\"Latent Representation으로 noising/denoising을 통한 Efficiency of the Computational Cost\"}),\"\\n\",n(r.li,{children:\"본인들이 제안한 Latent DM에 Condition(Guidance)를 제공하도록 U-Net에 Cross-Attention 구조 제시\"}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"2. Related Work\"}),\"\\n\",n(r.p,{children:\"DDPM\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Latent DM의 denoising U-net 구조 설계 및 제안\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Auto Encoder\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"original represenation을 더 낮은 차원의 latent로 축소시켜 유의미한 feature 학습\"}),\"\\n\",i(r.li,{children:[\"이를 기반으로 VAE(Variational Auto Encoder)라는 이미지 생성 모델 존재\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Encoding 시 a standard normal distribution의 variational value를 통해 latent space를 standard normal distribution으로 근사\"}),\"\\n\",n(r.li,{children:\"이 과정에서 Bayesian background를 통해 ELBO(Evidence Loss Bound)를 제시 → 원본 이미지 분포를 몰라도 간접적으로 denoising의 sampling distribution 차이를 계산하는 KL-Divergence을 통해 학습\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"GAN\"}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"Adversarial architecture를 통해 이미지 생성\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Generator가 Discriminator를 압도하면 학습 종료\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"3. Methodology\"}),\"\\n\",n(r.h3,{children:\"Main Idea\"}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"DDPM + latent space architecture\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/1.png\",alt:\"\"})]}),\"\\n\",n(r.li,{children:\"이미지를 AutoEncoder로 압축한 뒤, Latent representation을 noise 추가/제거해보자!\"}),\"\\n\",n(r.li,{children:\"Auto Encoder를 통해 원본 이미지에서 latent representation을 얻고, 해당 space에서 noise 추가/제거 과정을 학습하기 → 이후 sampling 또한 latent space에서 시행\"}),\"\\n\",n(r.li,{children:\"Condition(Guidance)를 Cross-Attention으로 제시하여 학습 및 이미지 생성 도움\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:n(r.strong,{children:\"→ Computational Cost를 줄이면서 동시에 이미지 품질 손상을 막거나 되려 더 올려보자!\"})}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[\"Training loss\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/2.png\",alt:\"\"}),\"\\n기존 loss에서\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/3.png\",alt:\"\"}),\"\\n로 noise의 distribution을 latent space로 변경만 한 꼴\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[\"Conditional loss\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/4.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",n(r.p,{children:\"τθ(y): Domain specific embedding by τθ → condition y를 latent\"}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",n(r.p,{children:\"ϕi(zt): intermediate representation of the UNet implementing ϵθ → noise의 중간 embedding\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h3,{children:\"Contributions\"}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"Latent DM은 Computational cost 면에서 매우 효율적! (noise 추가, 제거 시 훨씬 낮은 차원에서 이루어지므로…)\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"(본문 내용) training the most powerful DMs often takes hundreds of GPU days (e.g. 150 - 1000 V100 days in)\"}),\"\\n\",n(r.li,{children:\"(본문 내용) producing 50k samples takes approximately 5 days on a single A100 GPU\"}),\"\\n\",n(r.li,{children:n(r.img,{src:\"/study/paper-review/latent-diffusion/5.png\",alt:\"\"})}),\"\\n\",n(r.li,{children:\"LDM-4-G의 경우 inference Throughput (Samples/sec)이 0.4로 LDM 중 가장 무거움. 그런데 본문 내용대로 단순 계산하면 DDPM 기반 모델일 경우 0.116으로 약 3.7배 차이!\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"latent space 외에도 Text, Image, Class 등 Condition을 받아 Cross-attention 구조로 이미지 생성 품질을 높임\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"(바로 위 장표의 LDM-O-G 에서 G가 Guidance(condition)을 지칭\"}),\"\\n\",n(r.li,{children:\"FID가 매우 낮고 IS는 매우 높음을 알 수 있음\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"4. Experiments and Results\"}),\"\\n\",n(r.h3,{children:\"Model architecture\"}),\"\\n\",n(r.p,{children:\"Factor: the compression ratio of the latent representation\"}),\"\\n\",n(r.p,{children:\"KL: Indicates that the model does not use the VQ method to compress a latent representation.\"}),\"\\n\",n(r.p,{children:\"G: Guidance, which is equivalent to a condition. They use a BERT tokenizer and a transformer to infer a latent code.\"}),\"\\n\",n(r.h3,{children:\"Datasets\"}),\"\\n\",n(r.p,{children:\"CelebA-HQ: A celebrity face image dataset\"}),\"\\n\",n(r.p,{children:\"FFHQ(Flickr-Faces-HQ): A normal face image dataset\"}),\"\\n\",n(r.p,{children:\"LSUN-Churches: church images\"}),\"\\n\",n(r.p,{children:\"LSUN-Bedrooms: bedroom images\"}),\"\\n\",n(r.p,{children:\"ImageNet\"}),\"\\n\",n(r.h3,{children:\"Baseline\"}),\"\\n\",i(r.p,{children:[\"w/o condition\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/6.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"Summary table\"}),\"\\n\",i(r.table,{children:[n(r.thead,{children:i(r.tr,{children:[n(r.th,{children:\"Model\"}),n(r.th,{children:\"Type\"}),n(r.th,{children:\"Key Idea/Role\"})]})}),i(r.tbody,{children:[i(r.tr,{children:[n(r.td,{children:\"DC-VAE\"}),n(r.td,{children:\"VAE\"}),n(r.td,{children:\"Deep conv. VAE for image synthesis\"})]}),i(r.tr,{children:[n(r.td,{children:\"VQGAN+T\"}),n(r.td,{children:\"Hybrid\"}),n(r.td,{children:\"VQGAN + Transformer for text/image generation\"})]}),i(r.tr,{children:[n(r.td,{children:\"PGGAN\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Progressive training for high-res images\"})]}),i(r.tr,{children:[n(r.td,{children:\"LSGM\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Latent space score-based diffusion\"})]}),i(r.tr,{children:[n(r.td,{children:\"UDM\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Unified framework for diffusion models\"})]}),i(r.tr,{children:[n(r.td,{children:\"ImageBART\"}),n(r.td,{children:\"Transformer\"}),n(r.td,{children:\"BART for image token sequence generation\"})]}),i(r.tr,{children:[n(r.td,{children:\"U-Net GAN\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"U-Net generator for detail preservation\"})]}),i(r.tr,{children:[n(r.td,{children:\"StyleGAN\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Style-based control over image features\"})]}),i(r.tr,{children:[n(r.td,{children:\"ProjectedGAN\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Discriminator on projected features\"})]})]})]}),\"\\n\",i(r.p,{children:[\"w text-condition\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/7.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"Summary table\"}),\"\\n\",i(r.table,{children:[n(r.thead,{children:i(r.tr,{children:[n(r.th,{children:\"Model\"}),n(r.th,{children:\"Type\"}),n(r.th,{children:\"Key Idea/Role\"}),n(r.th,{children:\"Application\"})]})}),i(r.tbody,{children:[i(r.tr,{children:[n(r.td,{children:\"CogView\"}),n(r.td,{children:\"Transformer\"}),n(r.td,{children:\"Text-to-image via image token transformer\"}),n(r.td,{children:\"Text-to-image (Chinese)\"})]}),i(r.tr,{children:[n(r.td,{children:\"LAFITE\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Text-to-image without explicit text supervision\"}),n(r.td,{children:\"Zero-shot text-to-image\"})]}),i(r.tr,{children:[n(r.td,{children:\"GLIDE\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Text-guided diffusion, classifier-free guidance\"}),n(r.td,{children:\"High-quality text-to-image\"})]}),i(r.tr,{children:[n(r.td,{children:\"Make-A-Scene\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Text + layout for compositional generation\"}),n(r.td,{children:\"Scene-aware image synthesis\"})]})]})]}),\"\\n\",i(r.p,{children:[\"w class-condition\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/8.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"Summary Table\"}),\"\\n\",i(r.table,{children:[n(r.thead,{children:i(r.tr,{children:[n(r.th,{children:\"Model\"}),n(r.th,{children:\"Type\"}),n(r.th,{children:\"Key Feature\"})]})}),i(r.tbody,{children:[i(r.tr,{children:[n(r.td,{children:\"BigGAN-deep\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Deep, class-conditional\"})]}),i(r.tr,{children:[n(r.td,{children:\"ADM\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Class-conditional denoising\"})]}),i(r.tr,{children:[n(r.td,{children:\"ADM-G\"}),n(r.td,{children:\"Diffusion\"}),n(r.td,{children:\"Classifier-guided denoising\"})]})]})]}),\"\\n\",i(r.p,{children:[\"Inpainting\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/9.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"Summary Table\"}),\"\\n\",i(r.table,{children:[n(r.thead,{children:i(r.tr,{children:[n(r.th,{children:\"Model\"}),n(r.th,{children:\"Type\"}),n(r.th,{children:\"Key Feature/Idea\"})]})}),i(r.tbody,{children:[i(r.tr,{children:[n(r.td,{children:\"LaMa\"}),n(r.td,{children:\"CNN (FFC-based)\"}),n(r.td,{children:\"Fourier Convolutions, large masks\"})]}),i(r.tr,{children:[n(r.td,{children:\"CoModGAN\"}),n(r.td,{children:\"GAN\"}),n(r.td,{children:\"Contextual modulation\"})]}),i(r.tr,{children:[n(r.td,{children:\"RegionWise\"}),n(r.td,{children:\"CNN\"}),n(r.td,{children:\"Region-wise normalization/attention\"})]}),i(r.tr,{children:[n(r.td,{children:\"DeepFillv2\"}),n(r.td,{children:\"GAN (Gated Conv)\"}),n(r.td,{children:\"Gated conv, contextual attention\"})]}),i(r.tr,{children:[n(r.td,{children:\"EdgeConnect\"}),n(r.td,{children:\"Two-stage (Edge + GAN)\"}),n(r.td,{children:\"Edge prediction + inpainting\"})]})]})]}),\"\\n\",n(r.h3,{children:\"결과\"}),\"\\n\",n(r.p,{children:\"Already, LDMs show the performances mentioned above, achieving SOTA results on the CelebA-HQ 256x256 dataset. Therefore, the text-conditional image synthesis model achieved better FID and IS scores than the original DMs. LDMs also prove the quality on class-conditional generating and inpainting tasks.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/latent-diffusion/10.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"These results show that factor values of 4, 8, and 16 are suitible. 32 is too high to adequately represent the original information in the latent representation.\"}),\"\\n\",n(r.p,{children:\"I assume that Pixel-based models do not focus on important feature.\"}),\"\\n\",i(r.p,{children:[n(r.img,{src:\"/study/paper-review/latent-diffusion/11.png\",alt:\"\"}),\"\\nFor CelebA-Hq, LDM-32 performs better than other models, but it performs poorly on ImageNet.\"]}),\"\\n\",n(r.p,{children:\"Q) Why does a higher factor lead to better performance for CelebA-HQ?\"}),\"\\n\",n(r.p,{children:\"A) CelebA-HQ is a simple face image dataset. Therefore, it can be generated well with lower-dimensional latent representation.\"}),\"\\n\",n(r.p,{children:\"LDM-1 consistently receives poor scores. I suppose that pixel-based model have to deal with too many features to generate high-quality images.\"}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Generating images on each dataset.\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/12.png\",alt:\"\"})]}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Super-Resolution\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/13.png\",alt:\"\"}),\"\\nThe SR3 model is a type of diffusion model for super-resolution. The figure shows that SR3 is able to capture fine structures.\"]}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Inpainting\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/14.png\",alt:\"\"}),\"\\nAs mentioned above, LDMs achieved SOTA FID scores on the inpainting task.\"]}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"human evaluation on SR and Inpainting\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/15.png\",alt:\"\"}),\"\\nLDM showed dominant results. 😮\"]}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Total organization of the architectures.\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/latent-diffusion/16.png\",alt:\"\"}),\"\\nThey trained on OpenImages, evaluated on ImageNet-Val.\\nWe can see that there is a huge gap between f-32 and f-16 models.\"]}),\"\\n\",n(r.h2,{children:\"5. Conclusions\"}),\"\\n\",n(r.p,{children:\"상당히 많은 실험들\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"VAE, DDPM 등에서는 볼 수 없는 풍부한 task performance와 다양한 Baseline과의 비교\"}),\"\\n\",n(r.li,{children:\"DDPM을 기반으로 다양한 아키텍쳐를 제시한 만큼, 각각의 하이퍼파라미터에 따른 상세한 ablation study\"}),\"\\n\"]}),\"\\n\",n(r.hr,{})]})}return{default:function(e={}){const{wrapper:i}=e.components||{};return i?n(i,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/latent-diffusion"
  },
  {
    "title": "Reading papers",
    "description": "Read papers about MT-DNN, MASS, UNILM, XLNet, RoBERTa, ALBERT.",
    "slug": "reading-papers",
    "publishDate": "2025-04-27",
    "thumbnailUrl": "/study/paper-review/reading-papers-1/thum.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:t}=arguments[0];function _createMdxContent(r){const i={a:\"a\",blockquote:\"blockquote\",hr:\"hr\",img:\"img\",p:\"p\",...r.components};return t(e,{children:[n(i.p,{children:\"During my previous studies, I examined research papers on various language models, including seq2seq, Transformer, and BERT (though I didn't write about BERT). My objective is gaining knowledge in the LLM field and also have an assignment requiring me to create a video presentation explaining the concepts I've learned in class to my peers. Previous posts are the result of the assignment. Therefore, this post will be too. :D\"}),\"\\n\",n(i.p,{children:\"This post covers MT-DNN, MASS, UNILM, XLNet, RoBERTa and ALBERT. I won't describe the details of each model, but rather focus on the fundamental concepts (due to my schedule :p).\"}),\"\\n\",n(i.hr,{}),\"\\n\",t(i.p,{children:[n(i.a,{href:\"https://arxiv.org/abs/1901.11504\",children:\"MT-DNN\"}),\" (Multi-Task Deep Neural Networks for natural language understanding)\"]}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/1.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"In abstract, there is a key purpose of MT-DNN.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"And in the introduction, they explain the motivation behind MT-DNN, which is the basic principle of transfer learning.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"Multi-Task Learning (MTL) is inspired by human learning activities where people often apply the knowledge learned from previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who does not.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"They propose a diverse range of tasks to train the model:\"}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/2.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"However, I think the intricate details of these tasks are not essential for my current understanding. At this point, I'm focused on grasping the fundamental concepts rather than delving too deeply into the specifics.\"}),\"\\n\",t(i.p,{children:[\"The benchmark is following:\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/3.png\",alt:\"\"})]}),\"\\n\",n(i.hr,{}),\"\\n\",t(i.p,{children:[n(i.a,{href:\"https://arxiv.org/abs/1905.02450\",children:\"MASS\"}),\" (MAsked Sequence to Sequence learning)\"]}),\"\\n\",n(i.p,{children:\"In abstract:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. In this paper, inspired by BERT, we propose a novel objective for pre-training: MAsked Sequence to Sequence learning (MASS) for language generation.\"}),\"\\n\"]}),\"\\n\",t(i.p,{children:[\"What does that mean? See the figure below:\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/4.png\",alt:\"\"})]}),\"\\n\",n(i.p,{children:\"MASS masks continuous sequences of tokens within the source sentence. Subsequently, the decoder predicts these masked tokens, generating the output seqeuntially, one token at a time.\"}),\"\\n\",t(i.p,{children:[\"This provides a detailed mathematical formulation representing the loss function derived from the log-likelihood function:\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/5.png\",alt:\"\"})]}),\"\\n\",t(i.p,{children:[\"Researchers assert that, under specific constraint condtions, MASS can effectively function as both BERT and GPT architectures.\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/6.png\",alt:\"\"})]}),\"\\n\",n(i.p,{children:\"However, MASS is the encoder-decoder framework to generate tokens and there is a cross-attention between the encoder and decoder. It is the fundamental difference from BERT (which is a single encoder architecture) and GPT (which is a single decoder architecture).\"}),\"\\n\",n(i.hr,{}),\"\\n\",t(i.p,{children:[n(i.a,{href:\"https://arxiv.org/abs/1905.03197\",children:\"UNILM\"}),\" (UNIfied Language Model pre-training for natural language understanding and generation)\"]}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/7.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"The authors describe UNILM's foundational architecture as a 'multi-layer Transformer network'.\"}),\"\\n\",n(i.p,{children:\"However, upon examination of Figure 1, it appears identical to the encoder layer of a standard Transformer. (In fact, its architecture is precisely that of BERT Large.)\"}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/8.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"They implement a strategy that emulates language modeling techniques through the strategic manipulation of self-attention masks.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"Specifically, within one training batch, 1/3 of the time we use the bidirectional LM objective, 1/3 of the time we employ the sequence-to-sequence LM objective, and both left-to-right and right-to-left LM objectives are sampled with rate of 1/6.\"}),\"\\n\"]}),\"\\n\",n(i.hr,{}),\"\\n\",t(i.p,{children:[n(i.a,{href:\"https://arxiv.org/abs/1906.08237\",children:\"XLNet\"}),\" (XLNet: Generalized Autoregressive Pretraining for Language Understanding)\"]}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/9.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"The name 'XLNet does not appear explicitly in the paper's title. However, the authors clarify the origin of this designation in the abstract.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"The nomenclature derives from their inspiration by Transformer-XL\"}),\"\\n\",n(i.p,{children:\"What are the distinctive characteristics of this approach?\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"They propose an enhanced pre-training methodology that leverages permutations of token order. How exactly does this mechanism function?\"}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/10.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"(a) shows the normal self-attention mechanism, and (b) shows the permutation self-attention what the researchers claim.\"}),\"\\n\",n(i.p,{children:\"For instance, consider the sentence 'New York is a city' which would be masked as '[Mask] [Mask] is a city' in traditional pre-training. BERT's approach involves predicting 'New' and 'York' independently, calculating p(New | is a city) and p(York | is a city) separately. In contrast, XLNet employs a sequential prediction method: first calculating p(New | is a city), then p(York | New, is a city), thereby effectively capturing the relationship between 'New' and 'York'.\\\"\"}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/12.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"For those familiar with the inherent limitations of transformer architectures, particularly their substantial computational overhead, it comes as no surprise that XLNet inherits these same challenges. The authors were acutely aware of this computational burden.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"While the permutation language modeling objective (3) has several benefits, it is a much more challenging optimization problem due to the permutation and causes slow convergence in preliminary experiments. To reduce the optimization difficulty, we choose to only predict the last tokens in a factorization order.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/11.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"To mitigate this issue, they strategically implemented the permutation method on only a subset of the sequence, rather than applying it exhaustively.\"}),\"\\n\",n(i.hr,{}),\"\\n\",n(i.p,{children:n(i.a,{href:\"https://arxiv.org/abs/1907.11692\",children:\"RoBERTa\"})}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/13.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"In abstract:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"So... the RoBERTa paper represents the better hyperparameter optimization. Our professor emphasized that despite its seemingly straightforward tuning methodology, this research emerged during a period saturated with numerous approaches and architectural innovations aimed at enhancing LLM performance. I concur with this assessment, as my review of several related papers reveals a predominant focus on architectural innovations.\"}),\"\\n\",n(i.p,{children:\"From this perspective, the authors assert the following:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"They also implement an masking strategy called dynamic masking, which randomly masks tokens throughout the training process rather than consistently masking identical token positions.\"}),\"\\n\",n(i.p,{children:\"Intuitively, as most would likely concur, dynamic masking appears inherently more effective than static masking approaches. However, the empirical results indicate:\"}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/14.png\",alt:\"\"})}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\":D\"}),\"\\n\",n(i.p,{children:\"The followings are all approaches:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"Specifically, RoBERTa is trained with\\ndynamic masking (Section 4.1)\\nFULL-SENTENCES without NSP loss (Section 4.2)\\nlarge mini-batches (Section 4.3)\\na larger byte-level BPE (Section 4.4)\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/15.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"And they assert that BPE has advantages although it doesn't show a better performance.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work.\"}),\"\\n\"]}),\"\\n\",n(i.hr,{}),\"\\n\",n(i.p,{children:n(i.a,{href:\"https://arxiv.org/abs/1909.11942\",children:\"ALBERT\"})}),\"\\n\",n(i.p,{children:n(i.img,{src:\"/study/paper-review/reading-papers-1/16.png\",alt:\"\"})}),\"\\n\",n(i.p,{children:\"In abstract:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"we present two parameter reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"I'll just introduce the methodologies of ALBERT brifely.\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding.\"}),\"\\n\"]}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network.\"}),\"\\n\"]}),\"\\n\",n(i.p,{children:\"These approaches yield efficiency gains:\"}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster.\"}),\"\\n\"]}),\"\\n\",t(i.p,{children:[\"The first one:\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/18.png\",alt:\"\"})]}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V × H) to O(V × E + E × H).\"}),\"\\n\"]}),\"\\n\",t(i.p,{children:[\"The second:\\n\",n(i.img,{src:\"/study/paper-review/reading-papers-1/17.png\",alt:\"\"})]}),\"\\n\",t(i.blockquote,{children:[\"\\n\",n(i.p,{children:\"We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE.\"}),\"\\n\"]}),\"\\n\",n(i.hr,{})]})}return{default:function(e={}){const{wrapper:t}=e.components||{};return t?n(t,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/reading-papers"
  },
  {
    "title": "최근 읽은 논문들 간단 정리",
    "description": "transformer-xl, wav2letter, wav2vec",
    "slug": "simple-summaries-1",
    "publishDate": "2025-06-30",
    "thumbnailUrl": "/study/paper-review/simple-summaries-1/thm.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:t}=arguments[0];function _createMdxContent(i){const r={code:\"code\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",ul:\"ul\",...i.components};return t(e,{children:[n(r.p,{children:\"이래 저래 여러 논문과 블로그를 보는데... 끝이 없다. 그리고 논문 리뷰를 블로그에 열심히 작성하는 것도 뭔가 애매한 듯하고. 약간 개발 블로그처럼, 모두가 논문 리뷰를 하지만 거기에 본인만의 특색이 있는 글은 많지 않다. 심지어 어떤 글들은 인터넷에 있는 정보를 그대로 가져다 붙인 수준이기도 하고.\"}),\"\\n\",n(r.p,{children:\"차라리 읽은 논문 중 얻은 인사이트를 가볍게 뽑아서 정리하는 게 훨씬 좋을 듯하다고 느꼈다. 어차피 요약이나 의의 등은 나보다 AI가 잘 해주니까. 그래서 그냥 최근에 읽은 것들에 대해 간단한 요약을 적고, 나름대로 영어 공부 겸 AI의 정리를 옮겨 적으며 다시금 되짚는 용도로 글을 써볼 생각!\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"Transformer-XL\"}),\"\\n\",n(r.p,{children:\"XL means extra-large. Transformer-XL can handle a long context window. The main idea is similar to an RNN. It separates a long context into segments and uses the hidden state of each segment to retain information from previous segments.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/simple-summaries-1/1.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"Therefore, it adopts a relative positional encoding.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/simple-summaries-1/2.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"I didn't understand why the 3 terms are added to calculate the attention score. Here's an explanation of each term.\"}),\"\\n\",t(r.ol,{children:[\"\\n\",n(r.li,{children:\"Content score: Q * K^T - A simple attention score\"}),\"\\n\",n(r.li,{children:\"Position score: Q * R^T - An inner product of query and relative positional embedding\"}),\"\\n\",n(r.li,{children:\"Content bias = U * K^T\"}),\"\\n\",n(r.li,{children:\"Positional bias = V * R^T\"}),\"\\n\"]}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"Bias terms adjust the important score. But... they're just bias terms to improve better performance.\"}),\"\\n\"]}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"Wav2Letter\"}),\"\\n\",n(r.p,{children:\"Wav2letter is designed as a simple end-to-end system for speech recognition, directly converting a speech signal into a text transcription. A key feature is that its acoustic model is trained directly using letters (graphemes). This breaks away from the classical pipeline.\"}),\"\\n\",n(r.h3,{children:\"Architecture\"}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"The system is built around a standard 1D convolutional neural network (ConvNet) for acoustic modeling.\"}),\"\\n\",n(r.li,{children:\"It uses striding convolutions instead of pooling layers to allow the network to perceive a larger context without increasing the number of parameters.\"}),\"\\n\",n(r.li,{children:\"The architecture for processing raw audio, as illustrated in the paper, involves a series of convolutional layers with specific kernel sizes and strides.\"}),\"\\n\",n(r.li,{children:\"The final layer of the convolutional network outputs a score for each letter in the predefined dictionary. The overall stride of the network is 320, generating a label every 20 milliseconds.\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"I don't discuss its architecture in detail, but just highlight the significance of this research.\"}),\"\\n\",t(r.ol,{children:[\"\\n\",n(r.li,{children:\"End-to-End architecture\"}),\"\\n\"]}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"it directly converts a speech signal into a text transcription. This approach eliminates the need for intermediate phonetic transcriptions or forced alignment of phonemes. It generates a grapheme output directly.\"}),\"\\n\"]}),\"\\n\",t(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"Introduction of automatic segmentation criterion (ASG)\"}),\"\\n\"]}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"ASG is a simpler sequence criterion that trains models from sequence annotation without requiring alignment. It has few differences from CTC(Connected Temporal Classification). The most key point is a transition probability. It occurs between letters.\"}),\"\\n\"]}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"Wav2Vec\"}),\"\\n\",n(r.p,{children:\"It applies unsupervised pre-training to learn robust representations from unlabeled audio, which are much easier to collect thana labeled data.\"}),\"\\n\",n(r.h3,{children:\"Architecture\"}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"A multi-layer convolutional neural network (ConvNet) that takes raw audio as input.\"}),\"\\n\",t(r.li,{children:[\"It consists of two main stacked networks. Encoder Network \",n(r.code,{children:\"f\"}),\" and Context Network \",n(r.code,{children:\"g\"}),\". f embeds the raw audio signal into a low-frequency feature representation \",n(r.code,{children:\"z\"}),\". g takes multiple time-steps of the encoder's output \",n(r.code,{children:\"z_i...z_i-v\"}),\" and combines them into a single contextualized tensor \",n(r.code,{children:\"c_i\"})]}),\"\\n\"]}),\"\\n\",n(r.h3,{children:\"Objective\"}),\"\\n\",t(r.ul,{children:[\"\\n\",n(r.li,{children:\"To distinguish a sample z_i+k that is k stpes in the future from distractor samples z tilda drawn from a proposal distribution p_n, by minimizing the contrastive loss for each step k = 1, ..., K\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"It is trained by discrimanating the correct sample among distractor samples.\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"VQ-Wav2Vec\"}),\"\\n\",n(r.p,{children:\"VQ means vector quantizing. It makes a discrete data to use NLP.\"}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"wav2vec 2.0\"}),\"\\n\",n(r.p,{children:\"It is the successor to wav2vec and VQ-wav2vec. wav2vec 2.0 uses self-supervised contrastive learning and discrete representations.\"}),\"\\n\",n(r.p,{children:\"One thing I would like to discuss is the contrastive loss used in this paper.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/simple-summaries-1/2.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"A latent representation is fed to two layers: a context layer and a quantization layer.\"}),\"\\n\",n(r.p,{children:\"c_t is the masked output of the context layer and q_t is the quantized output of the quantizing layer. The loss calculates the similarity between c_t and q_t so that context representation is close to q_t and far from the negative samples, q_t tilda.\"}),\"\\n\",n(r.hr,{})]})}return{default:function(e={}){const{wrapper:t}=e.components||{};return t?n(t,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/simple-summaries-1"
  },
  {
    "title": "Let's learn seq2seq!",
    "description": "Learn how to use seq2seq models using PyTorch",
    "slug": "tutorial-seq2seq",
    "publishDate": "2025-04-12",
    "thumbnailUrl": "/study/paper-review/tutorial-seq2seq/thum.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:l}=arguments[0];function _createMdxContent(s){const c={a:\"a\",code:\"code\",hr:\"hr\",img:\"img\",p:\"p\",pre:\"pre\",span:\"span\",...s.components};return l(e,{children:[n(c.p,{children:\"Recently, I've been studying large language models(LLMs). I understand the algorithms behind what they are and how they work, but I don't have a precise understanding of their implementation.\"}),\"\\n\",n(c.p,{children:\"I'm still pondering the necessity of understanding the basics of LLMs, such as implementation details, architecture, and so on. Since there is so much to learn in the field of LLMs, It's impossible to study everything in depth. However, as an undergraduate student without any immediate obligation to apply them for quick results, I plan to take my time and study step by step - starting with seq2seq, transformer, and other language models.\"}),\"\\n\",l(c.p,{children:[\"This post focuses on the seq2seq model. For learning purposes, I referred to \",n(c.a,{href:\"https://github.com/bentrevett/pytorch-seq2seq\",children:\"seq2seq tutorial link\"}),\".\"]}),\"\\n\",n(c.hr,{}),\"\\n\",n(c.p,{children:\"LSTM is more advanced type of RNN that can retain long-term information better than the original RNN. Unlike RNNs, which only have a hidden state, LSTMs include a cell state - a vector that stores information about the previous state of the LSTM cell.\"}),\"\\n\",n(c.p,{children:\"Leveraging LSTMs, many researchers have developed various techniques for translating one language to another.\"}),\"\\n\",n(c.p,{children:n(c.img,{src:\"/study/paper-review/tutorial-seq2seq/rnn1.png\",alt:\"\"})}),\"\\n\",n(c.p,{children:\"This is the original RNN architecture. The gray boxes represent cells, which take an input and produce an output, a hidden state, and a cell state.\\nThe mathematical expression is as follows:\"}),\"\\n\",n(c.p,{children:\"(h_t, c_t) = LSTM(e(x_t), h_t-1, c_t-1)\"}),\"\\n\",n(c.p,{children:\"However, the architecture above has a limitation: the dimensions of the inputs and outputs are fixed and identical. This poses a significant challenge for translation tasks, as most languages vary in length when expressing the same meaning.\"}),\"\\n\",l(c.p,{children:[n(c.img,{src:\"/study/paper-review/tutorial-seq2seq/lstm1.png\",alt:\"\"}),\"\\n\",n(c.img,{src:\"/study/paper-review/tutorial-seq2seq/lstm2.png\",alt:\"\"})]}),\"\\n\",n(c.p,{children:\"But this paper separates LSTM to two parts: encoder and decoder. Encoder LSTM takes the input sequence and produces a context vector, which has a information about the input sequence. Decoder LSTM takes the context vector and produces output one token at a time.\"}),\"\\n\",n(c.p,{children:n(c.img,{src:\"/study/paper-review/tutorial-seq2seq/seq2seq1.png\",alt:\"\"})}),\"\\n\",n(c.p,{children:\"the code implementing the paper is as follows (be cautious about the dimensions):\"}),\"\\n\",n(c.p,{children:\"Encoder\"}),\"\\n\",n(e,{children:n(c.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(c.code,{children:[l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"class\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" Encoder\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),n(c.span,{style:{color:\"#24292E\"},children:\".\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),n(c.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        super\"}),n(c.span,{style:{color:\"#24292E\"},children:\"().\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".hidden_dim \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" hidden_dim\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".n_layers \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" n_layers\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".embedding \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.Embedding(input_dim, embedding_dim)\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".rnn \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.LSTM(embedding_dim, hidden_dim, n_layers, \"}),n(c.span,{style:{color:\"#E36209\"},children:\"dropout\"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".dropout \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.Dropout(dropout)\"})]}),\"\\n\",n(c.span,{className:\"line\"}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, src):\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # src = [src length, batch size]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        embedded \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".dropout(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".embedding(src))\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # embedded = [src length, batch size, embedding dim]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        outputs, (hidden, cell) \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".rnn(embedded)\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # outputs = [src length, batch size, hidden dim * n directions]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # cell = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # outputs are always from the top hidden layer\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(c.span,{style:{color:\"#24292E\"},children:\" hidden, cell\"})]})]})})}),\"\\n\",l(c.p,{children:[n(c.code,{children:\"outputs, (hidden, cell) = self.rnn(embedded)\"}),\" represents an input and an output of LSTM structure.\"]}),\"\\n\",n(c.p,{children:\"Decoder\"}),\"\\n\",n(e,{children:n(c.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(c.code,{children:[l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"class\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" Decoder\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),n(c.span,{style:{color:\"#24292E\"},children:\".\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),n(c.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        super\"}),n(c.span,{style:{color:\"#24292E\"},children:\"().\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".output_dim \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" output_dim\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".hidden_dim \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" hidden_dim\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".n_layers \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" n_layers\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".embedding \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.Embedding(output_dim, embedding_dim)\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".rnn \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.LSTM(embedding_dim, hidden_dim, n_layers, \"}),n(c.span,{style:{color:\"#E36209\"},children:\"dropout\"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\"dropout)\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".fc_out \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.Linear(hidden_dim, output_dim)\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".dropout \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" nn.Dropout(dropout)\"})]}),\"\\n\",n(c.span,{className:\"line\"}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, input, hidden, cell):\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # input = [batch size]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # cell = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # n directions in the decoder will both always be 1, therefore:\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # context = [n layers, batch size, hidden dim]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        input\"}),n(c.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" input\"}),n(c.span,{style:{color:\"#24292E\"},children:\".unsqueeze(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(c.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # input = [1, batch size]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        embedded \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".dropout(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".embedding(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"input\"}),n(c.span,{style:{color:\"#24292E\"},children:\"))\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # embedded = [1, batch size, embedding dim]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        output, (hidden, cell) \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".rnn(embedded, (hidden, cell))\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # output = [seq length, batch size, hidden dim * n directions]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # cell = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # seq length and n directions will always be 1 in this decoder, therefore:\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # output = [1, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # cell = [n layers, batch size, hidden dim]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        prediction \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".fc_out(output.squeeze(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(c.span,{style:{color:\"#24292E\"},children:\"))\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # prediction = [batch size, output dim]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(c.span,{style:{color:\"#24292E\"},children:\" prediction, hidden, cell\"})]})]})})}),\"\\n\",l(c.p,{children:[n(c.code,{children:\"output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\"}),\" is similar to the one of encoder.\"]}),\"\\n\",n(c.p,{children:\"Model\"}),\"\\n\",n(e,{children:n(c.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(c.code,{children:[l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"class\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" Seq2Seq\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"nn\"}),n(c.span,{style:{color:\"#24292E\"},children:\".\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\"Module\"}),n(c.span,{style:{color:\"#24292E\"},children:\"):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" __init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, encoder, decoder, device):\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        super\"}),n(c.span,{style:{color:\"#24292E\"},children:\"().\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"__init__\"}),n(c.span,{style:{color:\"#24292E\"},children:\"()\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".encoder \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" encoder\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".decoder \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" decoder\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".device \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" device\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        assert\"}),n(c.span,{style:{color:\"#24292E\"},children:\" (\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            encoder.hidden_dim \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"==\"}),n(c.span,{style:{color:\"#24292E\"},children:\" decoder.hidden_dim\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        ), \"}),n(c.span,{style:{color:\"#032F62\"},children:'\"Hidden dimensions of encoder and decoder must be equal!\"'})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        assert\"}),n(c.span,{style:{color:\"#24292E\"},children:\" (\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            encoder.n_layers \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"==\"}),n(c.span,{style:{color:\"#24292E\"},children:\" decoder.n_layers\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        ), \"}),n(c.span,{style:{color:\"#032F62\"},children:'\"Encoder and decoder must have equal number of layers!\"'})]}),\"\\n\",n(c.span,{className:\"line\"}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"    def\"}),n(c.span,{style:{color:\"#6F42C1\"},children:\" forward\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(self, src, trg, teacher_forcing_ratio):\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # src = [src length, batch size]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # trg = [trg length, batch size]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # teacher_forcing_ratio is probability to use teacher forcing\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        batch_size \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" trg.shape[\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(c.span,{style:{color:\"#24292E\"},children:\"]\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        trg_length \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" trg.shape[\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(c.span,{style:{color:\"#24292E\"},children:\"]\"})]}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        trg_vocab_size \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".decoder.output_dim\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # tensor to store decoder outputs\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        outputs \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" torch.zeros(trg_length, batch_size, trg_vocab_size).to(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".device)\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # last hidden state of the encoder is used as the initial hidden state of the decoder\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"        hidden, cell \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".encoder(src)\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # hidden = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # cell = [n layers * n directions, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # first input to the decoder is the <sos> tokens\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"        input\"}),n(c.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(c.span,{style:{color:\"#24292E\"},children:\" trg[\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"0\"}),n(c.span,{style:{color:\"#24292E\"},children:\", :]\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"        # input = [batch size]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        for\"}),n(c.span,{style:{color:\"#24292E\"},children:\" t \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"in\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" range\"}),n(c.span,{style:{color:\"#24292E\"},children:\"(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(c.span,{style:{color:\"#24292E\"},children:\", trg_length):\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # insert input token embedding, previous hidden and previous cell states\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # receive output tensor (predictions) and new hidden and cell states\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            output, hidden, cell \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#005CC5\"},children:\" self\"}),n(c.span,{style:{color:\"#24292E\"},children:\".decoder(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"input\"}),n(c.span,{style:{color:\"#24292E\"},children:\", hidden, cell)\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # output = [batch size, output dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # hidden = [n layers, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # cell = [n layers, batch size, hidden dim]\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # place predictions in a tensor holding predictions for each token\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            outputs[t] \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" output\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # decide if we are going to use teacher forcing or not\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            teacher_force \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" random.random() \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"<\"}),n(c.span,{style:{color:\"#24292E\"},children:\" teacher_forcing_ratio\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # get the highest predicted token from our predictions\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#24292E\"},children:\"            top1 \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(c.span,{style:{color:\"#24292E\"},children:\" output.argmax(\"}),n(c.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(c.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # if teacher forcing, use actual next token as next input\"})}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # if not, use predicted token\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#005CC5\"},children:\"            input\"}),n(c.span,{style:{color:\"#D73A49\"},children:\" =\"}),n(c.span,{style:{color:\"#24292E\"},children:\" trg[t] \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"if\"}),n(c.span,{style:{color:\"#24292E\"},children:\" teacher_force \"}),n(c.span,{style:{color:\"#D73A49\"},children:\"else\"}),n(c.span,{style:{color:\"#24292E\"},children:\" top1\"})]}),\"\\n\",n(c.span,{className:\"line\",children:n(c.span,{style:{color:\"#6A737D\"},children:\"            # input = [batch size]\"})}),\"\\n\",l(c.span,{className:\"line\",children:[n(c.span,{style:{color:\"#D73A49\"},children:\"        return\"}),n(c.span,{style:{color:\"#24292E\"},children:\" outputs\"})]})]})})}),\"\\n\",n(c.p,{children:\"I would recommend you to read minutely about the flow of the code, if you are begginer of Neural Network and Language model.\"}),\"\\n\",n(c.hr,{})]})}return{default:function(e={}){const{wrapper:l}=e.components||{};return l?n(l,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/tutorial-seq2seq"
  },
  {
    "title": "VAE",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - Auto-Encoding Variational Bayes",
    "slug": "vae",
    "publishDate": "2025-07-18",
    "thumbnailUrl": "/study/paper-review/vae/vae_thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:l}=arguments[0];function _createMdxContent(r){const s={a:\"a\",br:\"br\",code:\"code\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",pre:\"pre\",span:\"span\",ul:\"ul\",...r.components};return l(e,{children:[l(s.p,{children:[\"Reference:\\n\",n(s.a,{href:\"https://www.youtube.com/watch?v=qJeaCHQ1k2w\",children:\"Youtube\"}),n(s.br,{}),\"\\n\",n(s.a,{href:\"https://kyujinpy.tistory.com/88\",children:\"blog (in Korean)\"})]}),\"\\n\",l(s.p,{children:[\"Implementation from scratch:\\n\",n(s.a,{href:\"https://github.com/goranikin/DSBA-intern-study/tree/main/VAE\",children:\"GitHub\"})]}),\"\\n\",n(s.hr,{}),\"\\n\",n(s.h2,{children:\"1. Introduction\"}),\"\\n\",l(s.ol,{children:[\"\\n\",n(s.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"image generation\"}),\"\\n\",n(s.li,{children:\"latent representation\"}),\"\\n\",n(s.li,{children:\"autoencoders\"}),\"\\n\"]}),\"\\n\",l(s.ol,{start:\"2\",children:[\"\\n\",n(s.li,{children:\"Limitations of previous studies in this task\"}),\"\\n\"]}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"Limitations of existing generative models such as RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), and GMM (Gaussian Mixture Model) By GPT-4.1:\"}),\"\\n\",n(s.li,{children:\"Training these models is difficult.\"}),\"\\n\",n(s.li,{children:\"Inference becomes intractable as model size increases.\"}),\"\\n\",n(s.li,{children:\"Sampling is slow and often produces low-quality results\"}),\"\\n\"]}),\"\\n\",l(s.ol,{start:\"3\",children:[\"\\n\",n(s.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",n(s.p,{children:\"The authors claim two main contributions:\"}),\"\\n\",l(s.ol,{children:[\"\\n\",n(s.li,{children:\"A reparameterization of the variational lower bound (which makes the ELBO)\"}),\"\\n\",n(s.li,{children:\"A posterior inference can be made especially efficient by fitting an approximate inference model\"}),\"\\n\"]}),\"\\n\",n(s.h2,{children:\"2. Related Work\"}),\"\\n\",n(s.p,{children:\"RBM, GMM, DBN... and there are algorithms or mathematical backgrounds for this paper.\"}),\"\\n\",n(s.h2,{children:\"3. Methodology\"}),\"\\n\",n(s.h3,{children:\"Main Idea\"}),\"\\n\",n(s.p,{children:\"Each data point is assumed to be independnet and sampled from the same distribution, and there exists a unique latent variable corresponding to each data point. The data is generated from this latent variable.\"}),\"\\n\",n(s.p,{children:\"(본문 내용)  We assume that the data are generated by some random process, involving an unobserved continuous random variable z.\"}),\"\\n\",n(s.p,{children:\"Assumption:\"}),\"\\n\",n(s.p,{children:\"i.i.d. dataset consist of N samples:\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/1.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"Unobserved random variable: z\"}),\"\\n\",n(s.p,{children:\"prior distribution: p_theta(z)\"}),\"\\n\",n(s.p,{children:\"conditional distribution(likelihood) to generate x: p_theta^*(x|z)\"}),\"\\n\",n(s.p,{children:\"→ All they are come from parametric distribution of theta.\"}),\"\\n\",n(s.p,{children:\"Problem\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/2.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"Why?)\"}),\"\\n\",n(s.p,{children:\"We use a neural network to generate the latent representations (z) through training. However, because the model represents a highly complex non-linear function, there is no closed-form solution for the required integration—it is analytically intractable.\"}),\"\\n\",l(s.p,{children:[\"Three solutions proposed by the authors are following:\",n(s.br,{}),\"\\n\",n(s.img,{src:\"/study/paper-review/vae/3.png\",alt:\"\"})]}),\"\\n\",l(s.ol,{children:[\"\\n\",n(s.li,{children:\"We can’t maximize p_theta(x) directly during training theta → Maximize the ELBO\"}),\"\\n\",n(s.li,{children:\"We can’t estimate true posterior p_theta(z|x) → an Approximate neural network q_phi(z|x)\"}),\"\\n\",n(s.li,{children:\"We can’t estimate true p_theta(x) → Approximating with ELBO and sampling\"}),\"\\n\"]}),\"\\n\",n(s.p,{children:\"recab\"}),\"\\n\",l(s.ol,{children:[\"\\n\",n(s.li,{children:\"We want to generate image data x using a model with parameters theta.\"}),\"\\n\",n(s.li,{children:\"We cannot directly calculate the marginal likelihood, which is the distribution from which the model generates data. (Since a neural network model is a non-linear function and does not have a closed-form solution for integration, it is impossible to obtain the loss required for backpropagation.)\"}),\"\\n\",n(s.li,{children:\"Therefore, we do not have any information about the distribution of the latent representation z.\"}),\"\\n\"]}),\"\\n\",n(s.p,{children:\"-> How to solve them?\"}),\"\\n\",l(s.ol,{children:[\"\\n\",n(s.li,{children:\"By maximizing the objective function ELBO, the parameters can be trained indirectly.\"}),\"\\n\",n(s.li,{children:\"The posterior is approximated using an encoder neural network to estimate (z) from (x).\"}),\"\\n\",n(s.li,{children:\"Ultimately, we can approximate the marginal likelihood using the ELBO.\"}),\"\\n\"]}),\"\\n\",n(s.p,{children:\"First, we use q_phi(z|x) to approximate the porsterior, and then we train the model so that its distribution p_theta(z|x) matches this approximation.\"}),\"\\n\",n(s.h3,{children:\"The variational bound\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/4.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"There are many blogs that try to explain this topic. However, what I want to ask is: why don’t they start with the KL divergence? Our goal is to minimize the KL divergence between (q) and (p), but starting from the marginal likelihood doesn’t sufficiently explain this equation.\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/5.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"Re-represent RHS term\"}),\"\\n\",l(s.p,{children:[n(s.img,{src:\"/study/paper-review/vae/6.png\",alt:\"\"}),\"\\n\",n(s.img,{src:\"/study/paper-review/vae/7.png\",alt:\"\"})]}),\"\\n\",n(s.p,{children:\"The loss what we desired! But if we look a bit more closely...\"}),\"\\n\",n(s.p,{children:\"KL-D term measures how similar the distribution of (z) produced by our encoder from (x) is to the prior. The smaller this term is, the better the latent space is regularized. In VAE, we typically set the prior to a normal distribution, so this term acts as a regularizer. This is why the normal distribution is increasingly reflected in the encoder's information compression, and it helps prevent meaningless sampling when generating new images through sampling later on. This demonstrates the power of setting a prior in Bayesian methods.\"}),\"\\n\",n(s.p,{children:\"The second expectation term evaluates how well the model can reconstruct (x) from the (z) produced by our encoder. Our goal is to maximize this term! (The KL-D term can be adjusted with an appropriate hyperparameter)\"}),\"\\n\",n(s.p,{children:\"At this point, since we set the prior as a Guassian, we also set q_phi(z|x) to be Gaussian.\"}),\"\\n\",n(s.p,{children:\"For each image (x), q_phi(z|x) outputs the values mu_phi and sigma_phi, which define a normal distribution. We then sample (z) from this distribution.\"}),\"\\n\",n(s.p,{children:\"Q) How to backpropagate through the sampling process? (Due to randomness, the gradient is meaningless)\"}),\"\\n\",n(s.p,{children:\"A) Reparameterization trick\"}),\"\\n\",n(s.p,{children:\"The solution is to sampling not directly from q_phi(z|x), but instead from g_phi(epsilon, x) → then we can change integration space from z space to epsilon space\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/9.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"So, for a given value of epsilon, we can deterministically compute the gradient of phi w.r.t. mu and sigma! 🤘\"}),\"\\n\",n(s.p,{children:\"Therefore, the original expectation is approximated to Monte Carlo estimating using l samples of  epsilon.\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/10.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"Visualization\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/11.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/12.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"This is pseudo code to train:\"}),\"\\n\",n(e,{children:n(s.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(s.code,{children:[l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"L \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" 5\"}),n(s.span,{style:{color:\"#6A737D\"},children:\"  # The number of Monte Carlo samples\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"recon_losses \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" []\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#D73A49\"},children:\"for\"}),n(s.span,{style:{color:\"#24292E\"},children:\" _ \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"in\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" range\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(L):\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"    eps \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" torch.randn_like(std)\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"    z \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" mu \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"+\"}),n(s.span,{style:{color:\"#24292E\"},children:\" std \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"*\"}),n(s.span,{style:{color:\"#24292E\"},children:\" eps\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"    x_recon \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" decoder(z)\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"    loss \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" F.binary_cross_entropy(x_recon, x, \"}),n(s.span,{style:{color:\"#E36209\"},children:\"reduction\"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#032F62\"},children:\"'sum'\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#24292E\"},children:\"    recon_losses.append(loss)\"})}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"recon_loss \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" torch.stack(recon_losses).mean()\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"kl_loss \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#D73A49\"},children:\" -\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"0.5\"}),n(s.span,{style:{color:\"#D73A49\"},children:\" *\"}),n(s.span,{style:{color:\"#24292E\"},children:\" torch.sum(\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(s.span,{style:{color:\"#D73A49\"},children:\" +\"}),n(s.span,{style:{color:\"#24292E\"},children:\" logvar \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"-\"}),n(s.span,{style:{color:\"#24292E\"},children:\" mu.pow(\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"2\"}),n(s.span,{style:{color:\"#24292E\"},children:\") \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"-\"}),n(s.span,{style:{color:\"#24292E\"},children:\" logvar.exp())\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#24292E\"},children:\"total_loss \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(s.span,{style:{color:\"#24292E\"},children:\" recon_loss \"}),n(s.span,{style:{color:\"#D73A49\"},children:\"+\"}),n(s.span,{style:{color:\"#24292E\"},children:\" kl_loss\"})]}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#24292E\"},children:\"total_loss.backward()\"})}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#24292E\"},children:\"optimizer.step()\"})})]})})}),\"\\n\",n(s.p,{children:\"There are a lot of ways to choose q_phi(z|x), g_phi(.), epsilon that the authors written on the paper.\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/13.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"Honestly, I totally don’t have any idea of them. Also, The only method used in application is Gaussian. → I’m gonna pass!\"}),\"\\n\",n(s.h3,{children:\"Contribution\"}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"A combination latent variable generation model and Autoencoder architecture\"}),\"\\n\",n(s.li,{children:\"It uses neural network which can do variational inference\"}),\"\\n\",n(s.li,{children:\"It can be applied to variety of datasets and problems\"}),\"\\n\"]}),\"\\n\",n(s.h2,{children:\"4. Experiments and Results\"}),\"\\n\",n(s.h3,{children:\"Datasets\"}),\"\\n\",l(s.ul,{children:[\"\\n\",l(s.li,{children:[\"MNIST\",\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"handwritten digit images from 0 to 9\"}),\"\\n\",n(s.li,{children:\"60000 training, 10000 test\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(s.li,{children:[\"Frey Face\",\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"grayscale images of a single individual’s face\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(s.h3,{children:\"Results\"}),\"\\n\",n(s.p,{children:n(s.img,{src:\"/study/paper-review/vae/14.png\",alt:\"\"})}),\"\\n\",n(s.p,{children:\"기존 Wake-Sleep algorithm보다 좋은 결과를 냈다는데… 해당 알고리듬이 뭔지 모름\"}),\"\\n\",n(e,{children:n(s.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:l(s.code,{children:[l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"1.\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Wake-Sleep\"}),n(s.span,{style:{color:\"#032F62\"},children:\" Algorithm이란?\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 1995년\"}),n(s.span,{style:{color:\"#032F62\"},children:\" Geoffrey\"}),n(s.span,{style:{color:\"#032F62\"},children:\" Hinton\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 등이\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 제안한\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"비지도\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"unsupervised\"}),n(s.span,{style:{color:\"#032F62\"},children:\" learning\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 알고리즘이야.\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Deep\"}),n(s.span,{style:{color:\"#032F62\"},children:\" generative\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"특히\"}),n(s.span,{style:{color:\"#032F62\"},children:\" Helmholtz\"}),n(s.span,{style:{color:\"#032F62\"},children:\" Machine\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#032F62\"},children:\"에서\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"잠재\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 변수\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"latent\"}),n(s.span,{style:{color:\"#032F62\"},children:\" variable\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 이용해\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데이터를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 생성하고,\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 그\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 구조를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습하는\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 사용됨.\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"## 2. **구조와 동작 방식**\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"### **모델 구성**\"})}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Recognition\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#24292E\"},children:\" (또는 \"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\"inference\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#24292E\"},children:\"):  \"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  데이터\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" x\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"로부터\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 잠재\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 변수\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" z\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 추정하는\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 모델\"}),n(s.span,{style:{color:\"#24292E\"},children:\" (VAE의 \"}),n(s.span,{style:{color:\"#032F62\"},children:\"encoder와\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 유사\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Generative\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\":\"}),n(s.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  잠재\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 변수\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" z\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"로부터\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데이터를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 생성하는\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 모델\"}),n(s.span,{style:{color:\"#24292E\"},children:\" (VAE의 \"}),n(s.span,{style:{color:\"#032F62\"},children:\"decoder와\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 유사\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"### **학습 단계**\"})}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Wake\"}),n(s.span,{style:{color:\"#032F62\"},children:\" phase\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\":\"}),n(s.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 실제\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데이터\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" x\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 관측\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" recognition\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model로\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" z\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 추정\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" generative\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model의\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 파라미터를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 업데이트\"}),n(s.span,{style:{color:\"#24292E\"},children:\" (데이터를 \"}),n(s.span,{style:{color:\"#032F62\"},children:\"더\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 잘\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 생성하도록\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"Sleep\"}),n(s.span,{style:{color:\"#032F62\"},children:\" phase\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\":\"}),n(s.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" generative\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model에서\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" z\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 샘플링해\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\(\"}),n(s.span,{style:{color:\"#032F62\"},children:\" x\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" \\\\)\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 생성\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" recognition\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model의\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 파라미터를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 업데이트\"}),n(s.span,{style:{color:\"#24292E\"},children:\" (생성된 \"}),n(s.span,{style:{color:\"#032F62\"},children:\"데이터를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 더\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 잘\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 인식하도록\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"## 3. **한계점**\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"최적화가\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 어렵고,\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 근사치가\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 부정확할\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 있음\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#032F62\"},children:\" recognition\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model과\"}),n(s.span,{style:{color:\"#032F62\"},children:\" generative\"}),n(s.span,{style:{color:\"#032F62\"},children:\" model이\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 따로따로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 업데이트되어,\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 서로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 잘\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 맞지\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 않을\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 있음\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 변분\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 하한\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"ELBO\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#032F62\"},children:\"을\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 직접적으로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 최대화하지\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 않음\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"## 4. **VAE와의 차이점**\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"VAE는\"}),n(s.span,{style:{color:\"#032F62\"},children:\" ELBO\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"변분\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 하한\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#032F62\"},children:\"를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 직접적으로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 최대화\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\"하며,\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 인코더/디코더를\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 동시에\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 엔드-투-엔드로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습함\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"wake-sleep은\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 두\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 모델을\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 번갈아가며\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 따로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 업데이트\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\"함\"})]}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6F42C1\"},children:\"---\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",n(s.span,{className:\"line\",children:n(s.span,{style:{color:\"#6A737D\"},children:\"## 5. **요약**\"})}),\"\\n\",n(s.span,{className:\"line\"}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"-\"}),n(s.span,{style:{color:\"#005CC5\"},children:\" **\"}),n(s.span,{style:{color:\"#032F62\"},children:\"wake-sleep\"}),n(s.span,{style:{color:\"#032F62\"},children:\" algorithm\"}),n(s.span,{style:{color:\"#005CC5\"},children:\"**\"}),n(s.span,{style:{color:\"#032F62\"},children:\":\"}),n(s.span,{style:{color:\"#24292E\"},children:\"  \"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 고전적인\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 생성\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 모델\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습법\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" wake\"}),n(s.span,{style:{color:\"#032F62\"},children:\" phase\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"실제\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데이터\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 기반\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#032F62\"},children:\"와\"}),n(s.span,{style:{color:\"#032F62\"},children:\" sleep\"}),n(s.span,{style:{color:\"#032F62\"},children:\" phase\"}),n(s.span,{style:{color:\"#24292E\"},children:\"(\"}),n(s.span,{style:{color:\"#6F42C1\"},children:\"생성\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 데이터\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 기반\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 학습\"}),n(s.span,{style:{color:\"#24292E\"},children:\")\"}),n(s.span,{style:{color:\"#032F62\"},children:\"로\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 번갈아가며\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 파라미터\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 업데이트\"})]}),\"\\n\",l(s.span,{className:\"line\",children:[n(s.span,{style:{color:\"#6F42C1\"},children:\"  -\"}),n(s.span,{style:{color:\"#032F62\"},children:\" VAE보다\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 최적화가\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 덜\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 효율적이고,\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 근사치가\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 부정확할\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 수\"}),n(s.span,{style:{color:\"#032F62\"},children:\" 있음\"})]})]})})}),\"\\n\",n(s.h2,{children:\"5. Conclusions\"}),\"\\n\",l(s.ul,{children:[\"\\n\",l(s.li,{children:[\"\\n\",n(s.p,{children:\"Training latent representations with neural networks has been attempted for a long time.\"}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"However, VAE combines the latent variable model itself with neural networks.\"}),\"\\n\",n(s.li,{children:\"Moreover, it allows us to observe continuous changes through interpolation in the latent space--making it interpretable!\"}),\"\\n\",n(s.li,{children:\"Plus, after implementating the code, I found the structure to be very simple. There's nothing special: just an encoder that learns mu and sigma with a neural network, and a decoder that takes the latent representation and outputs an image.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(s.li,{children:[\"\\n\",n(s.p,{children:\"After reading about Diffusion models...\"}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"The notaiton is very different.\"}),\"\\n\",n(s.li,{children:\"There are countless blogs explaining ELBO, but they all use exactly the same notation. It feels like everyone is copying explanations from someone who tried to make it easier to understand. The problem is, these notations are not from the VAE paper, but from Diffusion model papers. 🤔\"}),\"\\n\",n(s.li,{children:\"I found the trick of making the probabilistic sampling distribution learnable by slightly sidestepping it very interesting. Also, making the prior a normal distribution so that the latent space generated by the parameters is mapped to the prior--this really felt like the ultimate in Bayesian thinking.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(s.li,{children:[\"\\n\",n(s.p,{children:\"Various ticks for estimating with Monte Carlo methods\"}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"But in practice, they're not really used... -> Even without multiple samples, the performance is good enough.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",l(s.li,{children:[\"\\n\",l(s.p,{children:[\"I implemented it from scratch my self. (\",n(s.a,{href:\"https://github.com/goranikin/DSBA-intern-study/tree/main/VAE\",children:\"https://github.com/goranikin/DSBA-intern-study/tree/main/VAE\"}),\")\"]}),\"\\n\",l(s.ul,{children:[\"\\n\",n(s.li,{children:\"2 layers vs 5 layers... the results seem a bit clearer?\"}),\"\\n\",n(s.li,{children:n(s.img,{src:\"/study/paper-review/vae/15.png\",alt:\"\"})}),\"\\n\",n(s.li,{children:\"Interpolation like this is also possible!\"}),\"\\n\",n(s.li,{children:n(s.img,{src:\"/study/paper-review/vae/16.png\",alt:\"\"})}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(s.hr,{}),\"\\n\",n(s.p,{children:\"역시 고전은 직접 읽는 게 맞다는 생각이... VAE를 읽으니 확실히 GAN도 읽어봐야 한다는 생각이 팍팍 든다. 어차피 뭐 당장에 연구를 해야만 하는 상황도 아니고, 좀 더 근본적인 논문들을 천천히 읽어도 되는 시기인 듯하니... 차라리 이미지 생성쪽에 있는 다양한 것들을 읽어보자!\"})]})}return{default:function(e={}){const{wrapper:l}=e.components||{};return l?n(l,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/vae"
  },
  {
    "title": "ViT and MLP-Mixer",
    "description": "What is inductive bias? Then, what is the key difference between ViT and CNN?",
    "slug": "vit-mlpmixer",
    "publishDate": "2025-05-02",
    "thumbnailUrl": "/study/paper-review/vit-mlpmixer/2.png",
    "content": "const{Fragment:e,jsx:i,jsxs:t}=arguments[0];function _createMdxContent(n){const r={a:\"a\",blockquote:\"blockquote\",code:\"code\",hr:\"hr\",img:\"img\",p:\"p\",...n.components};return t(e,{children:[i(r.p,{children:\"NLP를 공부하던 중, Vision에 대해 알아보게 된 배경을 짧막하게 한글로 작성하겠습니다!\"}),\"\\n\",i(r.p,{children:\"NLP를 다루던 중 갑자기 ViT인가 싶을텐데요. 실은 이번 주에 가졌던 커피챗에서 ViT와 CNN의 차이가 뭔지에 대한 질문을 받았습니다. 눈 앞에 놓인 목적만을 최대한 잘 완수하기 위해 Vision field를 완전히 배제하다보니 image processing을 모른 채 답변을 했습니다.\"}),\"\\n\",t(r.p,{children:[\"처음에는 \",i(r.code,{children:\"CNN은 filter size로 제한된 convolution 연산을 하고, ViT는 transformer 구조로 입력값의 관계를 전부 계산한다. 그러므로 context window 크기가 근본적인 차이다.\"}),\" 라고 말했습니다. (물론 이렇게 깔끔하게 이야기하기보다 벙벙 돌려가며 이야기했습니다.)\"]}),\"\\n\",t(r.p,{children:[\"그러자 2차 질문으로 \",i(r.code,{children:\"그렇다면 transformer input context size를 인위적으로 제한해 CNN과 동일하게 만든다면 어떻게 될까요? 다시 말해 context size가 동일한 상황이라면 ViT와 CNN은 무슨 차이일까요?\"}),\" 가 돌아왔습니다. 질문을 주신 분이 느긋하게 화장실까지 다녀오시는 동안 머리가 깨져라 생각했는데요. Convolution 연산 방식이 어떻게 이루어지는지만 알고 있을 뿐, 실제로 응용 이론이나 CNN만의 특징은 모르는 상태였습니다. 그러나 질문자분은 제가 ViT를 모른다는 걸 알고 계시기에 오히려 의도 파악을 하고 transformer의 근본적인 구조를 고민했습니다.\"]}),\"\\n\",i(r.p,{children:\"처음에는 당연히 Query, Key, Value라는 attention operation을 떠올렸습니다. convolution operation은 filter의 입력으로 들어온 값들의 relationship을 계산하도록 설계된 게 아니므로 이 차이인가 싶었는데, 실은 동일한 context size에서 deep neural network를 통해 학습하다보면 Transformer만큼은 아니더라도 당연히 filter size 안의 관계성이 parameter들에 담길 것이라 생각했습니다. 그렇다면 어느정도는 Q, K, V를 흉내내는 거나 다름 없으므로 근본적 차이는 아니라 생각했습니다. (실제로 아래 내용에서 후술될 inductive bias 관점에서 locality가 그러한 관계성을 보입니다.) 그러나 Cross-attention이라고 하기엔... 애초에 Encoder, Decoder 구조라는 다른 차원의 이야기를 꺼내오는 거라 이 또한 설득력이 낮다고 생각했습니다.\"}),\"\\n\",t(r.p,{children:[\"위와 같은 생각의 흐름을 쭉 전달드리고서 결국 \",i(r.code,{children:\"잘 모르겠다.\"}),\"라고 말씀드렸습니다. 그러자 여기에는 정답이 없지만, 그럼에도 근본적인 차이는 \",i(r.code,{children:\"Multi-head attention\"}),\"이라 생각한다는 말에 '아차' 싶었습니다. Transformer 구조에서 head의 개수가 늘어날수록 다양한 특징을 학습해 표현력이 높아지고, 또 해석 가능한 AI를 연구하시는 분들도 각각의 Head가 가진 latent representation이 뭔지 탐색한다는 어딘가 박혀있던 기억들이 새록새록 떠올랐습니다. Convolution 연산은 대상이 image든 natural language든 대상이 가진 잠재적 특성을 '다양하게' 고려하지 않습니다. 그저 하나의 값으로 표현할 뿐이죠.\"]}),\"\\n\",i(r.p,{children:\"이 물음에 대해서 ViT와 MLP-Mixer를 공부했더라도 답변하지 못했던 건 매 한가지였을 겁니다. 왜냐하면 두 논문에서도 주요하게 다뤄지는 건 Context size와 이로 인해 생기는 inductive bias의 차이니까요. 오히려 context size 관점에서 벗어나지 못해 더 엉뚱한 이야기를 했을지도 모릅니다. 그러나 MLP-Mixer가 가져다주는 통찰도 있습니다. 바로 inductive bias든 multi-head attention이든 두 대상이 잠재적인 특성을 다룬다는 겁니다.\"}),\"\\n\",i(r.p,{children:\"이와 같은 implicit한 의미들을 신경망 구조에서 어떻게 이끌어낼 수 있을까요? 수학적 논리력도 중요하겠지만, 무엇보다도 언어와 인간의 지각 처리 과정에 대한 깊은 이해, 그리고 탁월한 직관력이지 않을까 생각해봅니다. 알면 알수록 참 재미있는 세계인 듯해요!\"}),\"\\n\",i(r.hr,{}),\"\\n\",t(r.p,{children:[\"Before diving into this topic, I'd like to highlight an exceptional \",i(r.a,{href:\"https://www.youtube.com/watch?v=Y-isY31Thkw\",children:\"YouTube Video\"}),\" that brilliantly illustrates the concepts of image processing and what is 'inductive bias'.\"]}),\"\\n\",t(r.p,{children:[\"Initially, I'll examine the groundbreaking \",i(r.a,{href:\"https://arxiv.org/pdf/2010.11929.pdf\",children:\"Vit Paper\"}),\".\"]}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/1.png\",alt:\"\"})}),\"\\n\",i(r.p,{children:\"In abstract,\"}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\"}),\"\\n\"]}),\"\\n\",i(r.p,{children:\"A critical factor in this approach is the necessity for training on extensive datasets. Due to the nature of inductive bias, transformer architectures require substantial amounts of data to achieve optimal performance.\"}),\"\\n\",i(r.p,{children:\"Following the transformer's remarkable success in NLP, numerous attempts have been made to adapt this architecture to the image processing domain.\"}),\"\\n\",i(r.p,{children:\"However, these initial efforts yielded disappointing results that fell short of expectations. The researchers of ViT attributed this underperformance to a fundamental architectural difference:\"}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\"}),\"\\n\"]}),\"\\n\",i(r.p,{children:\"This analysis led to a pivotal breakthrough in their research:\"}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"We find that large scale training trumps inductive bias.\"}),\"\\n\"]}),\"\\n\",i(r.p,{children:\"So, what does this actually imply? what precisely is 'inductive bias', and why is a substantial volume of data necessary?\"}),\"\\n\",i(r.p,{children:\"Before exploring these questions, let's first understand the ViT architecture.\"}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/2.png\",alt:\"\"})}),\"\\n\",i(r.p,{children:\"ViT requires the input to be divided into fixed-size segments, known as PxP patches.\"}),\"\\n\",i(r.p,{children:\"The original image dimensions are represented as (H, W, C), corresponding to Height, Width, and Channel respectively. The N patches each have dimensions of (P, P, C), where N = H*W/P^2\"}),\"\\n\",i(r.p,{children:\"After splitting the image into the patches:\"}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\"}),\"\\n\"]}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/3.png\",alt:\"\"})}),\"\\n\",i(r.p,{children:\"In equation (1), xE represents the embedding vector that serves as input for the self-attention operation. Furthermore, the subsequent equations elucidate the operation performed by the architecture, as illustrated in Figure 1.\"}),\"\\n\",i(r.p,{children:\"For those already familiar with 'self-attention' mechanisms, the fundamental distinction between CNN and ViT becomes evident: context scope. While ViT computes relationships across all patches simultaneously during self-attention, CNNs only process interactions between adjacent patches during convolution. This distinction represents the essence of inductive bias.\"}),\"\\n\",i(r.p,{children:\"Inductive bias refers to the inherent predispositions or assumptions within a model that facilitate generalization, thereby enhancing performance. How does this mechanism function? In machine learning contexts, the objective is to identify a predictor that minimizes loss. Throughout this optimization process, inductive bias serves a vital function in mitigating overfitting. Calibrating an appropriate degree of inductive bias is crucial, as suboptimal values can either lead to overfitting or compromise overall performance.\"}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/4.png\",alt:\"\"})}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.\"}),\"\\n\"]}),\"\\n\",i(r.p,{children:\"The authors explain that CNNs inherently possess locality and translation inductive biases, while ViT lacks these specific predispositions. At this point, this distinction prompts a crucial question: Is transformer completely devoid of inductive bias?\"}),\"\\n\",i(r.p,{children:\"Well, I explained inductive bias as 'predispositions or assumptions within a model'. It means that inductive bias enhance a model's predictive capabilities when confronted with novel scenarios-situations the model hasn't encountered during its training phase. Following this rationale, both CNNs and transformers inherently possess inductive biases, although the former has structural inductive bias, while the latter has global inductive bias.\"}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/5.png\",alt:\"\"})}),\"\\n\",i(r.p,{children:\"Structural inductive bias can be effectively learned from comparatively smaller datasets than those required for global inductive bias. This distinction is intuitively comprehensible: establishing meaningful correlations across all input elements and identifying appropriate patterns for model generalization presents a substantially more complex challenge.\"}),\"\\n\",i(r.p,{children:\"MLP-Mixer effectively exemplifies this intuition through its linear computational complexity processes: Token-mixing MLP and Channel-mixing MLP.\"}),\"\\n\",i(r.p,{children:i(r.img,{src:\"/study/paper-review/vit-mlpmixer/6.png\",alt:\"\"})}),\"\\n\",i(r.p,{children:\"This architecture represents not merely a novel approach that is considerably more streamlined than either ViT or CNNs, but also an evolutionary synthesis derived from both CNNs and transformers. The authors characterize MLP-Mixer as an expanded conceptualization of CNNs.\"}),\"\\n\",i(r.p,{children:\"Through this architectural attempt, the presence of inductive bias becomes evident, enabling us to understand its attributes.\"}),\"\\n\",t(r.blockquote,{children:[\"\\n\",i(r.p,{children:\"we would like to understand the inductive biases hidden in these various features and eventually their role in generalization\"}),\"\\n\"]}),\"\\n\",i(r.hr,{}),\"\\n\",i(r.p,{children:\"Reference\"}),\"\\n\",t(r.p,{children:[\"Paper Review MLP-Mixer: An all-MLP Architecture for Vision - \",i(r.a,{href:\"https://www.youtube.com/watch?v=Y-isY31Thkw\",children:\"https://www.youtube.com/watch?v=Y-isY31Thkw\"}),\"\\nViT Paper: \",i(r.a,{href:\"https://arxiv.org/pdf/2010.11929.pdf\",children:\"https://arxiv.org/pdf/2010.11929.pdf\"}),\"\\nMLP-Mixer Paper: \",i(r.a,{href:\"https://arxiv.org/abs/2105.01601\",children:\"https://arxiv.org/abs/2105.01601\"})]}),\"\\n\",i(r.hr,{})]})}return{default:function(e={}){const{wrapper:t}=e.components||{};return t?i(t,{...e,children:i(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/vit-mlpmixer"
  },
  {
    "title": "VQ-VAE",
    "description": "DSBA 연구실 사전학습 논문 리뷰 - Neural Discrete Representation Learning",
    "slug": "vq-vae",
    "publishDate": "2025-07-19",
    "thumbnailUrl": "/study/paper-review/vq-vae/vq-vae_thm1.jpeg",
    "content": "const{Fragment:e,jsx:n,jsxs:i}=arguments[0];function _createMdxContent(t){const r={a:\"a\",annotation:\"annotation\",br:\"br\",code:\"code\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",math:\"math\",mi:\"mi\",mrow:\"mrow\",msub:\"msub\",ol:\"ol\",p:\"p\",pre:\"pre\",semantics:\"semantics\",span:\"span\",strong:\"strong\",ul:\"ul\",...t.components};return i(e,{children:[i(r.p,{children:[\"스크래치 구현(Original VAE와 같이 작성됨): \",n(r.a,{href:\"https://github.com/goranikin/DSBA-intern-study/tree/main/VAE\",children:\"GitHub\"})]}),\"\\n\",n(r.hr,{}),\"\\n\",n(r.h2,{children:\"1. Introduction\"}),\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:\"The research area covered by the paper\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"Latent representation\"}),\"\\n\",n(r.li,{children:\"Vector Quantization\"}),\"\\n\"]}),\"\\n\",i(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"Limitations of previous studies in this task\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"Language and speech are inherently discrete in nature.\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"This raises the question: since images can often be described using language, is it beneficial to represent images with discrete latent variables?\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"Traditional VAEs suffer from posterior collapse problem.\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"This occurs when the encoder becomes too closely aligned with the prior distribution, causing the latent variables to carry little or no meaningful information about the input.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(r.ol,{start:\"3\",children:[\"\\n\",n(r.li,{children:\"Contributions\"}),\"\\n\"]}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"It shows a wide variety of applications such as speech and video generation.\"}),\"\\n\",n(r.li,{children:\"It doesn’t suffer from the posterior collapse problem.\"}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"2. Related Work\"}),\"\\n\",n(r.p,{children:\"VAE\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"it shows an architecture of VAE (inferring a variational latent)\"}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"3. Methodology\"}),\"\\n\",n(r.h3,{children:\"Main Idea\"}),\"\\n\",n(r.h3,{children:\"1. Discrete Latent variables\"}),\"\\n\",n(r.p,{children:n(r.strong,{children:\"notations\"})}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"e: a embedding space\"}),\"\\n\",n(r.li,{children:\"K: the size of the discrete latent space (K-way categorical)\"}),\"\\n\",i(r.li,{children:[\"D: the dimensionality of each latent embedding vector \",i(r.span,{className:\"katex\",children:[n(r.span,{className:\"katex-mathml\",children:n(r.math,{xmlns:\"http://www.w3.org/1998/Math/MathML\",children:i(r.semantics,{children:[n(r.mrow,{children:i(r.msub,{children:[n(r.mi,{children:\"e\"}),n(r.mi,{children:\"i\"})]})}),n(r.annotation,{encoding:\"application/x-tex\",children:\"e_i\"})]})})}),n(r.span,{className:\"katex-html\",\"aria-hidden\":\"true\",children:i(r.span,{className:\"base\",children:[n(r.span,{className:\"strut\",style:{height:\"0.5806em\",verticalAlign:\"-0.15em\"}}),i(r.span,{className:\"mord\",children:[n(r.span,{className:\"mord mathnormal\",children:\"e\"}),n(r.span,{className:\"msupsub\",children:i(r.span,{className:\"vlist-t vlist-t2\",children:[i(r.span,{className:\"vlist-r\",children:[n(r.span,{className:\"vlist\",style:{height:\"0.3117em\"},children:i(r.span,{style:{top:\"-2.55em\",marginLeft:\"0em\",marginRight:\"0.05em\"},children:[n(r.span,{className:\"pstrut\",style:{height:\"2.7em\"}}),n(r.span,{className:\"sizing reset-size6 size3 mtight\",children:n(r.span,{className:\"mord mathnormal mtight\",children:\"i\"})})]})}),n(r.span,{className:\"vlist-s\",children:\"​\"})]}),n(r.span,{className:\"vlist-r\",children:n(r.span,{className:\"vlist\",style:{height:\"0.15em\"},children:n(r.span,{})})})]})})]})]})})]}),\" and there are K embedding vectors.\"]}),\"\\n\",n(r.li,{children:\"z_e(x): the output of the encoder\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Q) How to move to discrete latent?\"}),\"\\n\",n(r.p,{children:\"A) a nearest neighbour look-up!\"}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"posterior categorical distribution\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/vq-vae/1.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"pseudo code:\"}),\"\\n\",n(e,{children:n(r.pre,{className:\"shiki github-light\",style:{backgroundColor:\"#fff\",color:\"#24292e\"},tabIndex:\"0\",children:i(r.code,{children:[i(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"def\"}),n(r.span,{style:{color:\"#6F42C1\"},children:\" quantize\"}),n(r.span,{style:{color:\"#24292E\"},children:\"(z_e, codebook):\"})]}),\"\\n\",i(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"    distances \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" ((z_e. \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"-\"}),n(r.span,{style:{color:\"#24292E\"},children:\" codebook) \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"**\"}),n(r.span,{style:{color:\"#005CC5\"},children:\" 2\"}),n(r.span,{style:{color:\"#24292E\"},children:\").sum(\"}),n(r.span,{style:{color:\"#D73A49\"},children:\"-\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",i(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"    indices \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" distances.argmin(\"}),n(r.span,{style:{color:\"#005CC5\"},children:\"1\"}),n(r.span,{style:{color:\"#24292E\"},children:\")\"})]}),\"\\n\",i(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#24292E\"},children:\"    z_q \"}),n(r.span,{style:{color:\"#D73A49\"},children:\"=\"}),n(r.span,{style:{color:\"#24292E\"},children:\" codebook[indices]\"})]}),\"\\n\",i(r.span,{className:\"line\",children:[n(r.span,{style:{color:\"#D73A49\"},children:\"    return\"}),n(r.span,{style:{color:\"#24292E\"},children:\" z_q, indices\"})]})]})})}),\"\\n\",n(r.p,{children:\"how to update parameter with this discrete codebook?\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/vq-vae/2.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"The right image means that a loss relocate the representation to other embedding vectors, then the result changes in the next process.\"}),\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Loss\"}),\"\\n\",n(r.img,{src:\"/study/paper-review/vq-vae/3.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"sg: stop-gradient\"}),\"\\n\",n(r.p,{children:\"A first term is reconstruction loss\"}),\"\\n\",n(r.p,{children:\"2nd: codebook loss\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"it updates codebook vector only, moving to the encoder’s output.\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"3th: Commitment loss\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"it updates the output of the encoder only, moving to the codebook vector.\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Q) I’ve understood why does loss contains only the output of the encoder and embedding vectors (due to an non-differentiable of quantization operation). But why they are divided into two terms as codebook loss and commitment loss?\"}),\"\\n\",n(r.p,{children:\"A) (The paper states that)  To make sure the encoder commits to an embedding and its output does not grow, we add a commitment loss, the third term in equation\"}),\"\\n\",n(r.p,{children:\"Q) Waiiiiiiiiit… the reconstruction loss contains quantization loss which is not differentiable.\"}),\"\\n\",i(r.p,{children:[\"A)\\n\",n(r.img,{src:\"/study/paper-review/vq-vae/4.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"it just passes the gradient: STE(Straight-Through Estimator)\"}),\"\\n\",n(r.p,{children:\"So… it updates the decoder and the encoder parameters, but not codebook. → codebook parameter only updates by codebook loss term.\"}),\"\\n\",i(r.ol,{start:\"2\",children:[\"\\n\",n(r.li,{children:\"with PixelCNN\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"Original VAE practices a Gaussian distribution to sample z. When the VQ-VAE paper had written, many of researches and applications have found a better performance to exploit VAE architecture. One of them is to use PixelCNN, which models the distribution of z autoregressively. Some experiments in the paper adopt the PixelCNN to generate image with VQ-VAE.\"}),\"\\n\",n(r.h3,{children:\"Contribution\"}),\"\\n\",n(r.p,{children:\"VQ-VAE presents a Vector-Quantization method at VAE with a codebook(embedding vector space).\"}),\"\\n\",n(r.h2,{children:\"4. Experiments and Results\"}),\"\\n\",n(r.h3,{children:\"Datasets\"}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"ImageNet\",\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[\"Reconstruction\\n\",n(r.img,{src:\"/study/paper-review/vq-vae/5.png\",alt:\"\"})]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"VCTK(The Voice Cloning Toolkit)\",\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"110 people’s audio samples.\"}),\"\\n\",n(r.li,{children:\"Most of them are from news or wikipedia.\"}),\"\\n\",n(r.li,{children:\"A variety of English pronounce.\"}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h3,{children:\"Results\"}),\"\\n\",i(r.ul,{children:[\"\\n\",n(r.li,{children:\"다양한 데이터 종류에 대해 적용 가능한 모델을 보여줌 (다른 표현들은 이산적이므로)\"}),\"\\n\",n(r.li,{children:\"VQ 방식을 통해 기존 VAE가 가진 posterior collapse 문제를 해결\"}),\"\\n\"]}),\"\\n\",n(r.h2,{children:\"5. Conclusions\"}),\"\\n\",i(r.p,{children:[\"I modified the original VAE code by simply adding the VQ process.\",n(r.br,{}),\"\\n\",n(r.img,{src:\"/study/paper-review/vq-vae/6.png\",alt:\"\"})]}),\"\\n\",n(r.p,{children:\"When I set the number of embeddings to 512, as used for ImageNet generation in the paper, the results weren't very good. In fact, even for the image reconstruction task, the generated images were much blurrier than those from the traditional VAE.\"}),\"\\n\",n(r.p,{children:\"However, samples generated from the same embedding vector showed identical appearances.\"}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/study/paper-review/vq-vae/7.png\",alt:\"\"})}),\"\\n\",n(r.p,{children:\"This is the result when the number of embeddings is limited to 10. It looks a bit cleaner than before.\"}),\"\\n\",n(r.p,{children:\"I'm sure the results would improve if I found the right hyperparameters, but I'll just stop here!\"})]})}return{default:function(e={}){const{wrapper:i}=e.components||{};return i?n(i,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "permalink": "/research/vq-vae"
  }
]