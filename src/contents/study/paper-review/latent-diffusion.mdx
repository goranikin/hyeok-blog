---
title: Latent Diffusion
publishDate: 2025-07-21
description: DSBA ì—°êµ¬ì‹¤ ì‚¬ì „í•™ìŠµ ë…¼ë¬¸ ë¦¬ë·° - High-Resolution Image Synthesis with Latent Diffusion Models
thumbnailUrl: /study/paper-review/latent-diffusion/thm1.jpeg
---

ì˜ì–´ë¡œ ë‹¤ ë°”ê¾¸ê¸° ë„ˆë¬´ ê·€ì°®ì•„ì„œ... ë‹¬íŒŒ ì…ì‚¬ ì „ì— í•´ì•¼ í•  ê²ƒë“¤ì´ ë§ìœ¼ë¯€ë¡œ Latent Diffusionì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µë¶™.

---

## 1. Introduction

1) The research area covered by the paper

- Image generation
- Auto encoder + Diffusion
- Conditional model (sampling + other info. about an image)

2) Limitations of previous studies in this task

- Diffusion Model(DM)ì€ pixel ë‹¨ìœ„ì˜ noise ì¶”ê°€ ë° ì œê±° â†’ ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ë‹¤!
- ì—°ì‚°ëŸ‰ì„ ì¤„ì´ë©´ì„œ DMì˜ ìƒì„± ëŠ¥ë ¥ì€ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?
- í•™ìŠµ ì‹œ ì´ˆê¸° ì´ë¯¸ì§€ ë°ì´í„° xë¥¼ latent spaceë¡œ ì¶•ì†Œì‹œí‚¨ ë‹¤ìŒ, DM processë¥¼ ê±°ì³ë³´ì!

3) Contributions

- Auto Encoder â†’ Latent Representation + Diffusion process êµ¬ì¡° ì œì‹œ
- Latent Representationìœ¼ë¡œ noising/denoisingì„ í†µí•œ Efficiency of the Computational Cost
- ë³¸ì¸ë“¤ì´ ì œì•ˆí•œ Latent DMì— Condition(Guidance)ë¥¼ ì œê³µí•˜ë„ë¡ U-Netì— Cross-Attention êµ¬ì¡° ì œì‹œ

## 2. Related Work

DDPM
- Latent DMì˜ denoising U-net êµ¬ì¡° ì„¤ê³„ ë° ì œì•ˆ

Auto Encoder
- original represenationì„ ë” ë‚®ì€ ì°¨ì›ì˜ latentë¡œ ì¶•ì†Œì‹œì¼œ ìœ ì˜ë¯¸í•œ feature í•™ìŠµ
- ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ VAE(Variational Auto Encoder)ë¼ëŠ” ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì¡´ì¬
    - Encoding ì‹œ a standard normal distributionì˜ variational valueë¥¼ í†µí•´ latent spaceë¥¼ standard normal distributionìœ¼ë¡œ ê·¼ì‚¬
    - ì´ ê³¼ì •ì—ì„œ Bayesian backgroundë¥¼ í†µí•´ ELBO(Evidence Loss Bound)ë¥¼ ì œì‹œ â†’ ì›ë³¸ ì´ë¯¸ì§€ ë¶„í¬ë¥¼ ëª°ë¼ë„ ê°„ì ‘ì ìœ¼ë¡œ denoisingì˜ sampling distribution ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” KL-Divergenceì„ í†µí•´ í•™ìŠµ

GAN
- Adversarial architectureë¥¼ í†µí•´ ì´ë¯¸ì§€ ìƒì„±
    - Generatorê°€ Discriminatorë¥¼ ì••ë„í•˜ë©´ í•™ìŠµ ì¢…ë£Œ
    

## 3. Methodology

### Main Idea

- DDPM + latent space architecture
![](/study/paper-review/latent-diffusion/1.png)
- ì´ë¯¸ì§€ë¥¼ AutoEncoderë¡œ ì••ì¶•í•œ ë’¤, Latent representationì„ noise ì¶”ê°€/ì œê±°í•´ë³´ì!
- Auto Encoderë¥¼ í†µí•´ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ latent representationì„ ì–»ê³ , í•´ë‹¹ spaceì—ì„œ noise ì¶”ê°€/ì œê±° ê³¼ì •ì„ í•™ìŠµí•˜ê¸° â†’ ì´í›„ sampling ë˜í•œ latent spaceì—ì„œ ì‹œí–‰
- Condition(Guidance)ë¥¼ Cross-Attentionìœ¼ë¡œ ì œì‹œí•˜ì—¬ í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„± ë„ì›€

 **â†’ Computational Costë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì´ë¯¸ì§€ í’ˆì§ˆ ì†ìƒì„ ë§‰ê±°ë‚˜ ë˜ë ¤ ë” ì˜¬ë ¤ë³´ì!**

- Training loss
![](/study/paper-review/latent-diffusion/2.png)
ê¸°ì¡´ lossì—ì„œ
![](/study/paper-review/latent-diffusion/3.png)
ë¡œ noiseì˜ distributionì„ latent spaceë¡œ ë³€ê²½ë§Œ í•œ ê¼´

- Conditional loss
![](/study/paper-review/latent-diffusion/4.png)

- Ï„Î¸(y): Domain specific embedding by Ï„Î¸ â†’ condition yë¥¼ latent
- Ï•i(zt): intermediate representation of the UNet implementing ÏµÎ¸ â†’ noiseì˜ ì¤‘ê°„ embedding

### Contributions

- Latent DMì€ Computational cost ë©´ì—ì„œ ë§¤ìš° íš¨ìœ¨ì ! (noise ì¶”ê°€, ì œê±° ì‹œ í›¨ì”¬ ë‚®ì€ ì°¨ì›ì—ì„œ ì´ë£¨ì–´ì§€ë¯€ë¡œâ€¦)
    - (ë³¸ë¬¸ ë‚´ìš©) training the most powerful DMs often takes hundreds of GPU days (e.g. 150 - 1000 V100 days in)
    - (ë³¸ë¬¸ ë‚´ìš©) producing 50k samples takes approximately 5 days on a single A100 GPU
    - ![](/study/paper-review/latent-diffusion/5.png)
    - LDM-4-Gì˜ ê²½ìš° inference Throughput (Samples/sec)ì´ 0.4ë¡œ LDM ì¤‘ ê°€ì¥ ë¬´ê±°ì›€. ê·¸ëŸ°ë° ë³¸ë¬¸ ë‚´ìš©ëŒ€ë¡œ ë‹¨ìˆœ ê³„ì‚°í•˜ë©´ DDPM ê¸°ë°˜ ëª¨ë¸ì¼ ê²½ìš° 0.116ìœ¼ë¡œ ì•½ 3.7ë°° ì°¨ì´!
- latent space ì™¸ì—ë„ Text, Image, Class ë“± Conditionì„ ë°›ì•„ Cross-attention êµ¬ì¡°ë¡œ ì´ë¯¸ì§€ ìƒì„± í’ˆì§ˆì„ ë†’ì„
    - (ë°”ë¡œ ìœ„ ì¥í‘œì˜ LDM-O-G ì—ì„œ Gê°€ Guidance(condition)ì„ ì§€ì¹­
    - FIDê°€ ë§¤ìš° ë‚®ê³  ISëŠ” ë§¤ìš° ë†’ìŒì„ ì•Œ ìˆ˜ ìˆìŒ

## 4. Experiments and Results

### Model architecture

Factor: the compression ratio of the latent representation

KL: Indicates that the model does not use the VQ method to compress a latent representation.

G: Guidance, which is equivalent to a condition. They use a BERT tokenizer and a transformer to infer a latent code.

### Datasets

CelebA-HQ: A celebrity face image dataset

FFHQ(Flickr-Faces-HQ): A normal face image dataset

LSUN-Churches: church images

LSUN-Bedrooms: bedroom images

ImageNet

### Baseline

w/o condition
![](/study/paper-review/latent-diffusion/6.png)
 
Summary table  
| Model | Type | Key Idea/Role |
| --- | --- | --- |
| DC-VAE | VAE | Deep conv. VAE for image synthesis |
| VQGAN+T | Hybrid | VQGAN + Transformer for text/image generation |
| PGGAN | GAN | Progressive training for high-res images |
| LSGM | Diffusion | Latent space score-based diffusion |
| UDM | Diffusion | Unified framework for diffusion models |
| ImageBART | Transformer | BART for image token sequence generation |
| U-Net GAN | GAN | U-Net generator for detail preservation |
| StyleGAN | GAN | Style-based control over image features |
| ProjectedGAN | GAN | Discriminator on projected features |

w text-condition
![](/study/paper-review/latent-diffusion/7.png)

Summary table  
| Model | Type | Key Idea/Role | Application |
| --- | --- | --- | --- |
| CogView | Transformer | Text-to-image via image token transformer | Text-to-image (Chinese) |
| LAFITE | GAN | Text-to-image without explicit text supervision | Zero-shot text-to-image |
| GLIDE | Diffusion | Text-guided diffusion, classifier-free guidance | High-quality text-to-image |
| Make-A-Scene | Diffusion | Text + layout for compositional generation | Scene-aware image synthesis |

w class-condition
![](/study/paper-review/latent-diffusion/8.png)

Summary Table  
| Model | Type | Key Feature |
| --- | --- | --- |
| BigGAN-deep | GAN | Deep, class-conditional |
| ADM | Diffusion | Class-conditional denoising |
| ADM-G | Diffusion | Classifier-guided denoising |

Inpainting
![](/study/paper-review/latent-diffusion/9.png)

Summary Table  
| Model | Type | Key Feature/Idea |
| --- | --- | --- |
| LaMa | CNN (FFC-based) | Fourier Convolutions, large masks |
| CoModGAN | GAN | Contextual modulation |
| RegionWise | CNN | Region-wise normalization/attention |
| DeepFillv2 | GAN (Gated Conv) | Gated conv, contextual attention |
| EdgeConnect | Two-stage (Edge + GAN) | Edge prediction + inpainting |

### ê²°ê³¼

Already, LDMs show the performances mentioned above, achieving SOTA results on the CelebA-HQ 256x256 dataset. Therefore, the text-conditional image synthesis model achieved better FID and IS scores than the original DMs. LDMs also prove the quality on class-conditional generating and inpainting tasks.

![](/study/paper-review/latent-diffusion/10.png)

These results show that factor values of 4, 8, and 16 are suitible. 32 is too high to adequately represent the original information in the latent representation.

I assume that Pixel-based models do not focus on important feature.

![](/study/paper-review/latent-diffusion/11.png)
For CelebA-Hq, LDM-32 performs better than other models, but it performs poorly on ImageNet.

Q) Why does a higher factor lead to better performance for CelebA-HQ?

A) CelebA-HQ is a simple face image dataset. Therefore, it can be generated well with lower-dimensional latent representation.

LDM-1 consistently receives poor scores. I suppose that pixel-based model have to deal with too many features to generate high-quality images.

**Generating images on each dataset.**
![](/study/paper-review/latent-diffusion/12.png)

**Super-Resolution**
![](/study/paper-review/latent-diffusion/13.png)
The SR3 model is a type of diffusion model for super-resolution. The figure shows that SR3 is able to capture fine structures.

**Inpainting**
![](/study/paper-review/latent-diffusion/14.png)
As mentioned above, LDMs achieved SOTA FID scores on the inpainting task.

**human evaluation on SR and Inpainting**
![](/study/paper-review/latent-diffusion/15.png)
LDM showed dominant results. ğŸ˜®

**Total organization of the architectures.**
![](/study/paper-review/latent-diffusion/16.png)
They trained on OpenImages, evaluated on ImageNet-Val.
We can see that there is a huge gap between f-32 and f-16 models.

## 5. Conclusions  

ìƒë‹¹íˆ ë§ì€ ì‹¤í—˜ë“¤
- VAE, DDPM ë“±ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ëŠ” í’ë¶€í•œ task performanceì™€ ë‹¤ì–‘í•œ Baselineê³¼ì˜ ë¹„êµ
- DDPMì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ì•„í‚¤í…ì³ë¥¼ ì œì‹œí•œ ë§Œí¼, ê°ê°ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ìƒì„¸í•œ ablation study
---
