---
title: VAE
publishDate: 2025-07-18
description: DSBA 연구실 사전학습 논문 리뷰 - Auto-Encoding Variational Bayes
thumbnailUrl: /study/paper-review/vae/vae_thm1.jpeg
---

참고자료:
[유튜브 영상](https://www.youtube.com/watch?v=qJeaCHQ1k2w)  
[블로그](https://kyujinpy.tistory.com/88)

스크래치 구현:
[GitHub](https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)

---

## 1. Introduction

1) 논문이 다루는 분야

- image generation
- latent representation
- autoencoders

2) 해당 task에서 기존 연구 한계점

- Limitations of existing generative models such as RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), and GMM (Gaussian Mixture Model) By GPT-4.1:
- Training these models is difficult.
- Inference becomes intractable as model size increases.
- Sampling is slow and often produces low-quality results

3) 논문의 contributions

The authors claim two main contributions:

1. A reparameterization of the variational lower bound (which makes the ELBO)
2. A posterior inference can be made especially efficient by fitting an approximate inference model

## 2. Related Work

RBM, GMM, DBN… 외에는 Algorithm이나 수학적 수식을 전개하는데 필요한 배경 지식만 언급되어 있었음.

## 3. Methodology

### Main Idea

→ 각 데이터는 독립이고 같은 분포에서 sampling되었으며, 각각의 datapoint마다 대응되는 고유한 latent variable이 존재한다고 가정. (해당 latent variable로부터 data가 생성)

(본문 내용)  We assume that the data are generated by some random process, involving an unobserved continuous random variable z.

Assumption:

i.i.d. dataset consist of N samples:

![](/study/paper-review/vae/1.png)

Unobserved random variable: z

prior distribution: p_theta(z)

conditional distribution(likelihood) to generate x: p_theta^*(x|z)

→ All they are come from parametric distribution of theta.

Problem

![](/study/paper-review/vae/2.png)

Why?)

We use a neural network to generate the latent representations (z) through training. However, because the model represents a highly complex non-linear function, there is no closed-form solution for the required integration—it is analytically intractable.

Three solutions proposed by the authors are following:  
![](/study/paper-review/vae/3.png)

1. We can’t maximize p_theta(x) directly during training theta → Maximize the ELBO
2. We can’t estimate true posterior p_theta(z|x) → an Approximate neural network q_phi(z|x)
3. We can’t estimate true p_theta(x) → Approximating with ELBO and sampling


다시 한 번 정리

1. parameter theta 라는 모델을 통해 특정 image data x를 생성하도록 만들고 싶음
2. 모델이 데이터를 생성할 확률 분포인 marginal likelihood를 직접 계산할 수가 없고, 따라서 parameter를 최적화할 수도 없음. (model은 closed-form integration이 주어지지 않은 복잡한 비선형 함수이므로… 계산을 해야 loss를 계산해서 back-propagation을 하든 말든!)
3. 그럼 당연히 latent representation z의 분포인 posterior도 모름…

→ 어떻게 해결할 수 있을까?

1. ELBO라는 목적함수를 최대화함으로써 간접적으로 parameter를 학습
2. encoder neural network로 posterior를 근사해서 x로부터 z를 추정
3. 궁극적으로 ELBO를 통해 marginal likelihood를 근사적으로 추정

우선적으로 posterior 를 근사하는 q_phi(z|x)를 만들고, 해당 모델이 보이는 분포가 최대한 p_theta(z|x)를 근사하도록 만들어보자.

### The variational bound

![](/study/paper-review/vae/4.png)

There are many blogs that try to explain this topic. However, what I want to ask is: why don’t they start with the KL divergence? Our goal is to minimize the KL divergence between (q) and (p), but starting from the marginal likelihood doesn’t sufficiently explain this equation.

![](/study/paper-review/vae/5.png)

Re-represent RHS term

![](/study/paper-review/vae/6.png)
![](/study/paper-review/vae/7.png)

우리가 원하는 Loss가 나왔다! 그런데 조금만 더 자세히 살펴보면…

KL-D는 우리의 encoder가 x로부터 뽑아내는 z의 분포가 prior 와 얼마나 유사한지를 나타내는 항이다. 이 항이 작을수록 잠재공간이 잘 정규화되어있다는 뜻이고, 앞으로 VAE에서는 prior를 정규분포로 두고 regularization 역할을 하게 만들 것! encoder의 정보 압축에서 정규분포가 점점 반영되는 이유고, 이후 sampling을 통해 새롭게 이미지를 생성할 때 무의미한 sampling을 하지 않도록 도와준다. → 베이지안이 갖는 prior 설정의 힘…

두 번째 기댓값 항은 우리의 인코더가 x로 부터 뽑아낸 z를 가지고, model이 복구해낸 x가 얼마나 유사한지 평가하는 항. 우리는 이 항을 최대화하는 게 목표! (KL-D 항은 적절한 hyperparameter 조정)


이때 prior를 Gaussian으로 두므로, q_phi(z|x)도 Gaussian으로 설정… 
![](/study/paper-review/vae/8.png)

이미지 x에 대해 q_phi(z|x)가 mu_phi, sigma_phi값을 각각 출력하도록 만들어 정규분포를 만들고, 해당 분포에서 z를 샘플링하도록 만듦.

Q) How to backpropagate through the sampling process? (Due to randomness, the gradient is meaningless)

A) Reparameterization trick

The solution is to sampling not directly from q_phi(z|x), but instead from g_phi(epsilon, x) → then we can change integration space from z space to epsilon space

![](/study/paper-review/vae/9.png)

So, for a given value of epsilon, we can deterministically compute the gradient of phi w.r.t. mu and sigma! 🤘

Therefore, the original expectation is approximated to Monte Carlo estimating using l samples of  epsilon.

![](/study/paper-review/vae/10.png)

Visualization

![](/study/paper-review/vae/11.png)

![](/study/paper-review/vae/12.png)

This is pseudo code to train:
```python
L = 5  # The number of Monte Carlo samples
recon_losses = []
for _ in range(L):
    eps = torch.randn_like(std)
    z = mu + std * eps
    x_recon = decoder(z)
    loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
    recon_losses.append(loss)
recon_loss = torch.stack(recon_losses).mean()

kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

total_loss = recon_loss + kl_loss
total_loss.backward()
optimizer.step()
```

There are a lot of ways to choose q_phi(z|x), g_phi(.), epsilon that the authors written on the paper.

![](/study/paper-review/vae/13.png)


Honestly, I totally don’t have any idea of them. Also, The only method used in application is Gaussian. → I’m gonna pass!

### Contribution

- A combination latent variable generation model and Autoencoder architecture
- It uses neural network which can do variational inference
- It can be applied to variety of datasets and problems


## 4. Experiments and Results

### Datasets
- MNIST
    - handwritten digit images from 0 to 9
    - 60000 training, 10000 test
- Frey Face
    - grayscale images of a single individual’s face

### Results

![](/study/paper-review/vae/14.png)

기존 Wake-Sleep algorithm보다 좋은 결과를 냈다는데… 해당 알고리듬이 뭔지 모름

```bash
1. **Wake-Sleep Algorithm이란?**

- 1995년 Geoffrey Hinton 등이 제안한 **비지도 학습(unsupervised learning)** 알고리즘이야.
- **Deep generative model**(특히 Helmholtz Machine)에서 **잠재 변수(latent variable)**를 이용해 데이터를 생성하고, 그 구조를 학습하는 데 사용됨.

---

## 2. **구조와 동작 방식**

### **모델 구성**
- **Recognition model** (또는 **inference model**):  
  데이터 \( x \)로부터 잠재 변수 \( z \)를 추정하는 모델 (VAE의 encoder와 유사)
- **Generative model**:  
  잠재 변수 \( z \)로부터 데이터를 생성하는 모델 (VAE의 decoder와 유사)

### **학습 단계**
- **Wake phase**:  
  - 실제 데이터 \( x \)를 관측
  - recognition model로 \( z \)를 추정
  - generative model의 파라미터를 업데이트 (데이터를 더 잘 생성하도록)
- **Sleep phase**:  
  - generative model에서 \( z \)를 샘플링해 \( x \)를 생성
  - recognition model의 파라미터를 업데이트 (생성된 데이터를 더 잘 인식하도록)

---

## 3. **한계점**

- **최적화가 어렵고, 근사치가 부정확할 수 있음**
- recognition model과 generative model이 따로따로 업데이트되어, 서로 잘 맞지 않을 수 있음
- 변분 하한(ELBO)을 직접적으로 최대화하지 않음

---

## 4. **VAE와의 차이점**

- **VAE는 ELBO(변분 하한)를 직접적으로 최대화**하며, 인코더/디코더를 동시에 엔드-투-엔드로 학습함
- **wake-sleep은 두 모델을 번갈아가며 따로 업데이트**함

---

## 5. **요약**

- **wake-sleep algorithm**:  
  - 고전적인 생성 모델 학습법
  - wake phase(실제 데이터 기반 학습)와 sleep phase(생성 데이터 기반 학습)로 번갈아가며 파라미터 업데이트
  - VAE보다 최적화가 덜 효율적이고, 근사치가 부정확할 수 있음
```

## 5. Conclusions

- Latent representation을 신경망으로 학습하는 건 오래전부터 있었던 시도
    - 그러나 Latent variable model 자체를 신경망으로 결합시켜버림
    - 심지어 해당 latent space에서 interpolation을 통한 연속적인 변화를 관찰 가능 → 해석 가능!
    - 게다가 코드 구현해보니 매우 간단한 구조임. 딴 거 없고 신경망으로 mu, sigma만 학습하는 encoder, 그리고 latent representation을 먹어서 image를 출력하는 decoder.
- Diffusion을 읽고 읽으니…
    - 매우 다른 Notation
    - ELBO에 대해 접근하는 수많은 블로그들… 그런데 다들 완전히 똑같은 표기. 심지어 쉽게 이해하기 위해 누군가가 설명한 내용을 그대로 똑같이 베껴쓰는 느낌. 게다가 문제는 해당 표기들이 VAE 논문이 아니라 Diffusion 논문용 표기들. 🤔
    - 확률적인 sampling distribution을 학습가능하도록 옆으로 살짝 제쳐두는 식의 trick이 상당히 재미있었음. prior를 정규분포로 만들어서 파라미터가 만들어내는 latent space를 prior로 보내도록 하는 것도 그렇고… 베이지안적 사고의 끝판왕을 경험한 느낌.
- Monte Carlo estimating으로 추정하기 위한 갖가지 비틀기 작업
    - 근데 실제 구현에선 안 씀… → 여러 개 샘플링 안 해도 충분히 좋은 성능
- 직접 scratch로 구현해봤음 (https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)
    - 2 layers vs 5 layers… 좀 더 선명해진 듯..?
![](/study/paper-review/vae/15.png)
    - 이런 식으로 interpolation도 가능!
![](/study/paper-review/vae/16.png)

---
