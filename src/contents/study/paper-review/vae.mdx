---
title: VAE
publishDate: 2025-07-18
description: DSBA ì—°êµ¬ì‹¤ ì‚¬ì „í•™ìŠµ ë…¼ë¬¸ ë¦¬ë·° - Auto-Encoding Variational Bayes
thumbnailUrl: /study/paper-review/vae/vae_thm1.jpeg
---

ì°¸ê³ ìë£Œ:
[ìœ íŠœë¸Œ ì˜ìƒ](https://www.youtube.com/watch?v=qJeaCHQ1k2w)  
[ë¸”ë¡œê·¸](https://kyujinpy.tistory.com/88)

ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„:
[GitHub](https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)

---

## 1. Introduction

1) ë…¼ë¬¸ì´ ë‹¤ë£¨ëŠ” ë¶„ì•¼

- image generation
- latent representation
- autoencoders

2) í•´ë‹¹ taskì—ì„œ ê¸°ì¡´ ì—°êµ¬ í•œê³„ì 

- Limitations of existing generative models such as RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), and GMM (Gaussian Mixture Model) By GPT-4.1:
- Training these models is difficult.
- Inference becomes intractable as model size increases.
- Sampling is slow and often produces low-quality results

3) ë…¼ë¬¸ì˜ contributions

The authors claim two main contributions:

1. A reparameterization of the variational lower bound (which makes the ELBO)
2. A posterior inference can be made especially efficient by fitting an approximate inference model

## 2. Related Work

RBM, GMM, DBNâ€¦ ì™¸ì—ëŠ” Algorithmì´ë‚˜ ìˆ˜í•™ì  ìˆ˜ì‹ì„ ì „ê°œí•˜ëŠ”ë° í•„ìš”í•œ ë°°ê²½ ì§€ì‹ë§Œ ì–¸ê¸‰ë˜ì–´ ìˆì—ˆìŒ.

## 3. Methodology

### Main Idea

â†’ ê° ë°ì´í„°ëŠ” ë…ë¦½ì´ê³  ê°™ì€ ë¶„í¬ì—ì„œ samplingë˜ì—ˆìœ¼ë©°, ê°ê°ì˜ datapointë§ˆë‹¤ ëŒ€ì‘ë˜ëŠ” ê³ ìœ í•œ latent variableì´ ì¡´ì¬í•œë‹¤ê³  ê°€ì •. (í•´ë‹¹ latent variableë¡œë¶€í„° dataê°€ ìƒì„±)

(ë³¸ë¬¸ ë‚´ìš©)  We assume that the data are generated by some random process, involving an unobserved continuous random variable z.

Assumption:

i.i.d. dataset consist of N samples:

![](/study/paper-review/vae/1.png)

Unobserved random variable: z

prior distribution: p_theta(z)

conditional distribution(likelihood) to generate x: p_theta^*(x|z)

â†’ All they are come from parametric distribution of theta.

Problem

![](/study/paper-review/vae/2.png)

Why?)

We use a neural network to generate the latent representations (z) through training. However, because the model represents a highly complex non-linear function, there is no closed-form solution for the required integrationâ€”it is analytically intractable.

Three solutions proposed by the authors are following:  
![](/study/paper-review/vae/3.png)

1. We canâ€™t maximize p_theta(x) directly during training theta â†’ Maximize the ELBO
2. We canâ€™t estimate true posterior p_theta(z|x) â†’ an Approximate neural network q_phi(z|x)
3. We canâ€™t estimate true p_theta(x) â†’ Approximating with ELBO and sampling


ë‹¤ì‹œ í•œ ë²ˆ ì •ë¦¬

1. parameter theta ë¼ëŠ” ëª¨ë¸ì„ í†µí•´ íŠ¹ì • image data xë¥¼ ìƒì„±í•˜ë„ë¡ ë§Œë“¤ê³  ì‹¶ìŒ
2. ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ìƒì„±í•  í™•ë¥  ë¶„í¬ì¸ marginal likelihoodë¥¼ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ê°€ ì—†ê³ , ë”°ë¼ì„œ parameterë¥¼ ìµœì í™”í•  ìˆ˜ë„ ì—†ìŒ. (modelì€ closed-form integrationì´ ì£¼ì–´ì§€ì§€ ì•Šì€ ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜ì´ë¯€ë¡œâ€¦ ê³„ì‚°ì„ í•´ì•¼ lossë¥¼ ê³„ì‚°í•´ì„œ back-propagationì„ í•˜ë“  ë§ë“ !)
3. ê·¸ëŸ¼ ë‹¹ì—°íˆ latent representation zì˜ ë¶„í¬ì¸ posteriorë„ ëª¨ë¦„â€¦

â†’ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ?

1. ELBOë¼ëŠ” ëª©ì í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•¨ìœ¼ë¡œì¨ ê°„ì ‘ì ìœ¼ë¡œ parameterë¥¼ í•™ìŠµ
2. encoder neural networkë¡œ posteriorë¥¼ ê·¼ì‚¬í•´ì„œ xë¡œë¶€í„° zë¥¼ ì¶”ì •
3. ê¶ê·¹ì ìœ¼ë¡œ ELBOë¥¼ í†µí•´ marginal likelihoodë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ ì¶”ì •

ìš°ì„ ì ìœ¼ë¡œ posterior ë¥¼ ê·¼ì‚¬í•˜ëŠ” q_phi(z|x)ë¥¼ ë§Œë“¤ê³ , í•´ë‹¹ ëª¨ë¸ì´ ë³´ì´ëŠ” ë¶„í¬ê°€ ìµœëŒ€í•œ p_theta(z|x)ë¥¼ ê·¼ì‚¬í•˜ë„ë¡ ë§Œë“¤ì–´ë³´ì.

### The variational bound

![](/study/paper-review/vae/4.png)

There are many blogs that try to explain this topic. However, what I want to ask is: why donâ€™t they start with the KL divergence? Our goal is to minimize the KL divergence between (q) and (p), but starting from the marginal likelihood doesnâ€™t sufficiently explain this equation.

![](/study/paper-review/vae/5.png)

Re-represent RHS term

![](/study/paper-review/vae/6.png)
![](/study/paper-review/vae/7.png)

ìš°ë¦¬ê°€ ì›í•˜ëŠ” Lossê°€ ë‚˜ì™”ë‹¤! ê·¸ëŸ°ë° ì¡°ê¸ˆë§Œ ë” ìì„¸íˆ ì‚´í´ë³´ë©´â€¦

KL-DëŠ” ìš°ë¦¬ì˜ encoderê°€ xë¡œë¶€í„° ë½‘ì•„ë‚´ëŠ” zì˜ ë¶„í¬ê°€ prior ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•­ì´ë‹¤. ì´ í•­ì´ ì‘ì„ìˆ˜ë¡ ì ì¬ê³µê°„ì´ ì˜ ì •ê·œí™”ë˜ì–´ìˆë‹¤ëŠ” ëœ»ì´ê³ , ì•ìœ¼ë¡œ VAEì—ì„œëŠ” priorë¥¼ ì •ê·œë¶„í¬ë¡œ ë‘ê³  regularization ì—­í• ì„ í•˜ê²Œ ë§Œë“¤ ê²ƒ! encoderì˜ ì •ë³´ ì••ì¶•ì—ì„œ ì •ê·œë¶„í¬ê°€ ì ì  ë°˜ì˜ë˜ëŠ” ì´ìœ ê³ , ì´í›„ samplingì„ í†µí•´ ìƒˆë¡­ê²Œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ ë¬´ì˜ë¯¸í•œ samplingì„ í•˜ì§€ ì•Šë„ë¡ ë„ì™€ì¤€ë‹¤. â†’ ë² ì´ì§€ì•ˆì´ ê°–ëŠ” prior ì„¤ì •ì˜ í˜â€¦

ë‘ ë²ˆì§¸ ê¸°ëŒ“ê°’ í•­ì€ ìš°ë¦¬ì˜ ì¸ì½”ë”ê°€ xë¡œ ë¶€í„° ë½‘ì•„ë‚¸ zë¥¼ ê°€ì§€ê³ , modelì´ ë³µêµ¬í•´ë‚¸ xê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ í‰ê°€í•˜ëŠ” í•­. ìš°ë¦¬ëŠ” ì´ í•­ì„ ìµœëŒ€í™”í•˜ëŠ” ê²Œ ëª©í‘œ! (KL-D í•­ì€ ì ì ˆí•œ hyperparameter ì¡°ì •)


ì´ë•Œ priorë¥¼ Gaussianìœ¼ë¡œ ë‘ë¯€ë¡œ, q_phi(z|x)ë„ Gaussianìœ¼ë¡œ ì„¤ì •â€¦ 
![](/study/paper-review/vae/8.png)

ì´ë¯¸ì§€ xì— ëŒ€í•´ q_phi(z|x)ê°€ mu_phi, sigma_phiê°’ì„ ê°ê° ì¶œë ¥í•˜ë„ë¡ ë§Œë“¤ì–´ ì •ê·œë¶„í¬ë¥¼ ë§Œë“¤ê³ , í•´ë‹¹ ë¶„í¬ì—ì„œ zë¥¼ ìƒ˜í”Œë§í•˜ë„ë¡ ë§Œë“¦.

Q) How to backpropagate through the sampling process? (Due to randomness, the gradient is meaningless)

A) Reparameterization trick

The solution is to sampling not directly from q_phi(z|x), but instead from g_phi(epsilon, x) â†’ then we can change integration space from z space to epsilon space

![](/study/paper-review/vae/9.png)

So, for a given value of epsilon, we can deterministically compute the gradient of phi w.r.t. mu and sigma! ğŸ¤˜

Therefore, the original expectation is approximated to Monte Carlo estimating using l samples of  epsilon.

![](/study/paper-review/vae/10.png)

Visualization

![](/study/paper-review/vae/11.png)

![](/study/paper-review/vae/12.png)

This is pseudo code to train:
```python
L = 5  # The number of Monte Carlo samples
recon_losses = []
for _ in range(L):
    eps = torch.randn_like(std)
    z = mu + std * eps
    x_recon = decoder(z)
    loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
    recon_losses.append(loss)
recon_loss = torch.stack(recon_losses).mean()

kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

total_loss = recon_loss + kl_loss
total_loss.backward()
optimizer.step()
```

There are a lot of ways to choose q_phi(z|x), g_phi(.), epsilon that the authors written on the paper.

![](/study/paper-review/vae/13.png)


Honestly, I totally donâ€™t have any idea of them. Also, The only method used in application is Gaussian. â†’ Iâ€™m gonna pass!

### Contribution

- A combination latent variable generation model and Autoencoder architecture
- It uses neural network which can do variational inference
- It can be applied to variety of datasets and problems


## 4. Experiments and Results

### Datasets
- MNIST
    - handwritten digit images from 0 to 9
    - 60000 training, 10000 test
- Frey Face
    - grayscale images of a single individualâ€™s face

### Results

![](/study/paper-review/vae/14.png)

ê¸°ì¡´ Wake-Sleep algorithmë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒˆë‹¤ëŠ”ë°â€¦ í•´ë‹¹ ì•Œê³ ë¦¬ë“¬ì´ ë­”ì§€ ëª¨ë¦„

```bash
1. **Wake-Sleep Algorithmì´ë€?**

- 1995ë…„ Geoffrey Hinton ë“±ì´ ì œì•ˆí•œ **ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning)** ì•Œê³ ë¦¬ì¦˜ì´ì•¼.
- **Deep generative model**(íŠ¹íˆ Helmholtz Machine)ì—ì„œ **ì ì¬ ë³€ìˆ˜(latent variable)**ë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ê·¸ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë¨.

---

## 2. **êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹**

### **ëª¨ë¸ êµ¬ì„±**
- **Recognition model** (ë˜ëŠ” **inference model**):  
  ë°ì´í„° \( x \)ë¡œë¶€í„° ì ì¬ ë³€ìˆ˜ \( z \)ë¥¼ ì¶”ì •í•˜ëŠ” ëª¨ë¸ (VAEì˜ encoderì™€ ìœ ì‚¬)
- **Generative model**:  
  ì ì¬ ë³€ìˆ˜ \( z \)ë¡œë¶€í„° ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ (VAEì˜ decoderì™€ ìœ ì‚¬)

### **í•™ìŠµ ë‹¨ê³„**
- **Wake phase**:  
  - ì‹¤ì œ ë°ì´í„° \( x \)ë¥¼ ê´€ì¸¡
  - recognition modelë¡œ \( z \)ë¥¼ ì¶”ì •
  - generative modelì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ë°ì´í„°ë¥¼ ë” ì˜ ìƒì„±í•˜ë„ë¡)
- **Sleep phase**:  
  - generative modelì—ì„œ \( z \)ë¥¼ ìƒ˜í”Œë§í•´ \( x \)ë¥¼ ìƒì„±
  - recognition modelì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ìƒì„±ëœ ë°ì´í„°ë¥¼ ë” ì˜ ì¸ì‹í•˜ë„ë¡)

---

## 3. **í•œê³„ì **

- **ìµœì í™”ê°€ ì–´ë µê³ , ê·¼ì‚¬ì¹˜ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ**
- recognition modelê³¼ generative modelì´ ë”°ë¡œë”°ë¡œ ì—…ë°ì´íŠ¸ë˜ì–´, ì„œë¡œ ì˜ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- ë³€ë¶„ í•˜í•œ(ELBO)ì„ ì§ì ‘ì ìœ¼ë¡œ ìµœëŒ€í™”í•˜ì§€ ì•ŠìŒ

---

## 4. **VAEì™€ì˜ ì°¨ì´ì **

- **VAEëŠ” ELBO(ë³€ë¶„ í•˜í•œ)ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìµœëŒ€í™”**í•˜ë©°, ì¸ì½”ë”/ë””ì½”ë”ë¥¼ ë™ì‹œì— ì—”ë“œ-íˆ¬-ì—”ë“œë¡œ í•™ìŠµí•¨
- **wake-sleepì€ ë‘ ëª¨ë¸ì„ ë²ˆê°ˆì•„ê°€ë©° ë”°ë¡œ ì—…ë°ì´íŠ¸**í•¨

---

## 5. **ìš”ì•½**

- **wake-sleep algorithm**:  
  - ê³ ì „ì ì¸ ìƒì„± ëª¨ë¸ í•™ìŠµë²•
  - wake phase(ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í•™ìŠµ)ì™€ sleep phase(ìƒì„± ë°ì´í„° ê¸°ë°˜ í•™ìŠµ)ë¡œ ë²ˆê°ˆì•„ê°€ë©° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
  - VAEë³´ë‹¤ ìµœì í™”ê°€ ëœ íš¨ìœ¨ì ì´ê³ , ê·¼ì‚¬ì¹˜ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ
```

## 5. Conclusions

- Latent representationì„ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê±´ ì˜¤ë˜ì „ë¶€í„° ìˆì—ˆë˜ ì‹œë„
    - ê·¸ëŸ¬ë‚˜ Latent variable model ìì²´ë¥¼ ì‹ ê²½ë§ìœ¼ë¡œ ê²°í•©ì‹œì¼œë²„ë¦¼
    - ì‹¬ì§€ì–´ í•´ë‹¹ latent spaceì—ì„œ interpolationì„ í†µí•œ ì—°ì†ì ì¸ ë³€í™”ë¥¼ ê´€ì°° ê°€ëŠ¥ â†’ í•´ì„ ê°€ëŠ¥!
    - ê²Œë‹¤ê°€ ì½”ë“œ êµ¬í˜„í•´ë³´ë‹ˆ ë§¤ìš° ê°„ë‹¨í•œ êµ¬ì¡°ì„. ë”´ ê±° ì—†ê³  ì‹ ê²½ë§ìœ¼ë¡œ mu, sigmaë§Œ í•™ìŠµí•˜ëŠ” encoder, ê·¸ë¦¬ê³  latent representationì„ ë¨¹ì–´ì„œ imageë¥¼ ì¶œë ¥í•˜ëŠ” decoder.
- Diffusionì„ ì½ê³  ì½ìœ¼ë‹ˆâ€¦
    - ë§¤ìš° ë‹¤ë¥¸ Notation
    - ELBOì— ëŒ€í•´ ì ‘ê·¼í•˜ëŠ” ìˆ˜ë§ì€ ë¸”ë¡œê·¸ë“¤â€¦ ê·¸ëŸ°ë° ë‹¤ë“¤ ì™„ì „íˆ ë˜‘ê°™ì€ í‘œê¸°. ì‹¬ì§€ì–´ ì‰½ê²Œ ì´í•´í•˜ê¸° ìœ„í•´ ëˆ„êµ°ê°€ê°€ ì„¤ëª…í•œ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë˜‘ê°™ì´ ë² ê»´ì“°ëŠ” ëŠë‚Œ. ê²Œë‹¤ê°€ ë¬¸ì œëŠ” í•´ë‹¹ í‘œê¸°ë“¤ì´ VAE ë…¼ë¬¸ì´ ì•„ë‹ˆë¼ Diffusion ë…¼ë¬¸ìš© í‘œê¸°ë“¤. ğŸ¤”
    - í™•ë¥ ì ì¸ sampling distributionì„ í•™ìŠµê°€ëŠ¥í•˜ë„ë¡ ì˜†ìœ¼ë¡œ ì‚´ì§ ì œì³ë‘ëŠ” ì‹ì˜ trickì´ ìƒë‹¹íˆ ì¬ë¯¸ìˆì—ˆìŒ. priorë¥¼ ì •ê·œë¶„í¬ë¡œ ë§Œë“¤ì–´ì„œ íŒŒë¼ë¯¸í„°ê°€ ë§Œë“¤ì–´ë‚´ëŠ” latent spaceë¥¼ priorë¡œ ë³´ë‚´ë„ë¡ í•˜ëŠ” ê²ƒë„ ê·¸ë ‡ê³ â€¦ ë² ì´ì§€ì•ˆì  ì‚¬ê³ ì˜ ëíŒì™•ì„ ê²½í—˜í•œ ëŠë‚Œ.
- Monte Carlo estimatingìœ¼ë¡œ ì¶”ì •í•˜ê¸° ìœ„í•œ ê°–ê°€ì§€ ë¹„í‹€ê¸° ì‘ì—…
    - ê·¼ë° ì‹¤ì œ êµ¬í˜„ì—ì„  ì•ˆ ì”€â€¦ â†’ ì—¬ëŸ¬ ê°œ ìƒ˜í”Œë§ ì•ˆ í•´ë„ ì¶©ë¶„íˆ ì¢‹ì€ ì„±ëŠ¥
- ì§ì ‘ scratchë¡œ êµ¬í˜„í•´ë´¤ìŒ (https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)
    - 2 layers vs 5 layersâ€¦ ì¢€ ë” ì„ ëª…í•´ì§„ ë“¯..?
![](/study/paper-review/vae/15.png)
    - ì´ëŸ° ì‹ìœ¼ë¡œ interpolationë„ ê°€ëŠ¥!
![](/study/paper-review/vae/16.png)

---
