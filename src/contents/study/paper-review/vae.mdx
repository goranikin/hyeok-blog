---
title: VAE
publishDate: 2025-07-18
description: DSBA ì—°êµ¬ì‹¤ ì‚¬ì „í•™ìŠµ ë…¼ë¬¸ ë¦¬ë·° - Auto-Encoding Variational Bayes
thumbnailUrl: /study/paper-review/vae/vae_thm1.jpeg
---

Reference:
[Youtube](https://www.youtube.com/watch?v=qJeaCHQ1k2w)  
[blog (in Korean)](https://kyujinpy.tistory.com/88)

Implementation from scratch:
[GitHub](https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)

---

## 1. Introduction

1) The research area covered by the paper

- image generation
- latent representation
- autoencoders

2) Limitations of previous studies in this task

- Limitations of existing generative models such as RBM (Restricted Boltzmann Machine), DBN (Deep Belief Network), and GMM (Gaussian Mixture Model) By GPT-4.1:
- Training these models is difficult.
- Inference becomes intractable as model size increases.
- Sampling is slow and often produces low-quality results

3) Contributions

The authors claim two main contributions:

1. A reparameterization of the variational lower bound (which makes the ELBO)
2. A posterior inference can be made especially efficient by fitting an approximate inference model

## 2. Related Work

RBM, GMM, DBN... and there are algorithms or mathematical backgrounds for this paper.

## 3. Methodology

### Main Idea

Each data point is assumed to be independnet and sampled from the same distribution, and there exists a unique latent variable corresponding to each data point. The data is generated from this latent variable.

(ë³¸ë¬¸ ë‚´ìš©)  We assume that the data are generated by some random process, involving an unobserved continuous random variable z.

Assumption:

i.i.d. dataset consist of N samples:

![](/study/paper-review/vae/1.png)

Unobserved random variable: z

prior distribution: p_theta(z)

conditional distribution(likelihood) to generate x: p_theta^*(x|z)

â†’ All they are come from parametric distribution of theta.

Problem

![](/study/paper-review/vae/2.png)

Why?)

We use a neural network to generate the latent representations (z) through training. However, because the model represents a highly complex non-linear function, there is no closed-form solution for the required integrationâ€”it is analytically intractable.

Three solutions proposed by the authors are following:  
![](/study/paper-review/vae/3.png)

1. We canâ€™t maximize p_theta(x) directly during training theta â†’ Maximize the ELBO
2. We canâ€™t estimate true posterior p_theta(z|x) â†’ an Approximate neural network q_phi(z|x)
3. We canâ€™t estimate true p_theta(x) â†’ Approximating with ELBO and sampling


recab

1. We want to generate image data x using a model with parameters theta.
2. We cannot directly calculate the marginal likelihood, which is the distribution from which the model generates data. (Since a neural network model is a non-linear function and does not have a closed-form solution for integration, it is impossible to obtain the loss required for backpropagation.)
3. Therefore, we do not have any information about the distribution of the latent representation z.

-> How to solve them?

1. By maximizing the objective function ELBO, the parameters can be trained indirectly.
2. The posterior is approximated using an encoder neural network to estimate (z) from (x).
3. Ultimately, we can approximate the marginal likelihood using the ELBO.

First, we use q_phi(z|x) to approximate the porsterior, and then we train the model so that its distribution p_theta(z|x) matches this approximation.

### The variational bound

![](/study/paper-review/vae/4.png)

There are many blogs that try to explain this topic. However, what I want to ask is: why donâ€™t they start with the KL divergence? Our goal is to minimize the KL divergence between (q) and (p), but starting from the marginal likelihood doesnâ€™t sufficiently explain this equation.

![](/study/paper-review/vae/5.png)

Re-represent RHS term

![](/study/paper-review/vae/6.png)
![](/study/paper-review/vae/7.png)

The loss what we desired! But if we look a bit more closely...

KL-D term measures how similar the distribution of (z) produced by our encoder from (x) is to the prior. The smaller this term is, the better the latent space is regularized. In VAE, we typically set the prior to a normal distribution, so this term acts as a regularizer. This is why the normal distribution is increasingly reflected in the encoder's information compression, and it helps prevent meaningless sampling when generating new images through sampling later on. This demonstrates the power of setting a prior in Bayesian methods.

The second expectation term evaluates how well the model can reconstruct (x) from the (z) produced by our encoder. Our goal is to maximize this term! (The KL-D term can be adjusted with an appropriate hyperparameter)

At this point, since we set the prior as a Guassian, we also set q_phi(z|x) to be Gaussian.

For each image (x), q_phi(z|x) outputs the values mu_phi and sigma_phi, which define a normal distribution. We then sample (z) from this distribution.

Q) How to backpropagate through the sampling process? (Due to randomness, the gradient is meaningless)

A) Reparameterization trick

The solution is to sampling not directly from q_phi(z|x), but instead from g_phi(epsilon, x) â†’ then we can change integration space from z space to epsilon space

![](/study/paper-review/vae/9.png)

So, for a given value of epsilon, we can deterministically compute the gradient of phi w.r.t. mu and sigma! ğŸ¤˜

Therefore, the original expectation is approximated to Monte Carlo estimating using l samples of  epsilon.

![](/study/paper-review/vae/10.png)

Visualization

![](/study/paper-review/vae/11.png)

![](/study/paper-review/vae/12.png)

This is pseudo code to train:
```python
L = 5  # The number of Monte Carlo samples
recon_losses = []
for _ in range(L):
    eps = torch.randn_like(std)
    z = mu + std * eps
    x_recon = decoder(z)
    loss = F.binary_cross_entropy(x_recon, x, reduction='sum')
    recon_losses.append(loss)
recon_loss = torch.stack(recon_losses).mean()

kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

total_loss = recon_loss + kl_loss
total_loss.backward()
optimizer.step()
```

There are a lot of ways to choose q_phi(z|x), g_phi(.), epsilon that the authors written on the paper.

![](/study/paper-review/vae/13.png)


Honestly, I totally donâ€™t have any idea of them. Also, The only method used in application is Gaussian. â†’ Iâ€™m gonna pass!

### Contribution

- A combination latent variable generation model and Autoencoder architecture
- It uses neural network which can do variational inference
- It can be applied to variety of datasets and problems


## 4. Experiments and Results

### Datasets
- MNIST
    - handwritten digit images from 0 to 9
    - 60000 training, 10000 test
- Frey Face
    - grayscale images of a single individualâ€™s face

### Results

![](/study/paper-review/vae/14.png)

ê¸°ì¡´ Wake-Sleep algorithmë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒˆë‹¤ëŠ”ë°â€¦ í•´ë‹¹ ì•Œê³ ë¦¬ë“¬ì´ ë­”ì§€ ëª¨ë¦„

```bash
1. **Wake-Sleep Algorithmì´ë€?**

- 1995ë…„ Geoffrey Hinton ë“±ì´ ì œì•ˆí•œ **ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning)** ì•Œê³ ë¦¬ì¦˜ì´ì•¼.
- **Deep generative model**(íŠ¹íˆ Helmholtz Machine)ì—ì„œ **ì ì¬ ë³€ìˆ˜(latent variable)**ë¥¼ ì´ìš©í•´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ê·¸ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë¨.

---

## 2. **êµ¬ì¡°ì™€ ë™ì‘ ë°©ì‹**

### **ëª¨ë¸ êµ¬ì„±**
- **Recognition model** (ë˜ëŠ” **inference model**):  
  ë°ì´í„° \( x \)ë¡œë¶€í„° ì ì¬ ë³€ìˆ˜ \( z \)ë¥¼ ì¶”ì •í•˜ëŠ” ëª¨ë¸ (VAEì˜ encoderì™€ ìœ ì‚¬)
- **Generative model**:  
  ì ì¬ ë³€ìˆ˜ \( z \)ë¡œë¶€í„° ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ (VAEì˜ decoderì™€ ìœ ì‚¬)

### **í•™ìŠµ ë‹¨ê³„**
- **Wake phase**:  
  - ì‹¤ì œ ë°ì´í„° \( x \)ë¥¼ ê´€ì¸¡
  - recognition modelë¡œ \( z \)ë¥¼ ì¶”ì •
  - generative modelì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ë°ì´í„°ë¥¼ ë” ì˜ ìƒì„±í•˜ë„ë¡)
- **Sleep phase**:  
  - generative modelì—ì„œ \( z \)ë¥¼ ìƒ˜í”Œë§í•´ \( x \)ë¥¼ ìƒì„±
  - recognition modelì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ìƒì„±ëœ ë°ì´í„°ë¥¼ ë” ì˜ ì¸ì‹í•˜ë„ë¡)

---

## 3. **í•œê³„ì **

- **ìµœì í™”ê°€ ì–´ë µê³ , ê·¼ì‚¬ì¹˜ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ**
- recognition modelê³¼ generative modelì´ ë”°ë¡œë”°ë¡œ ì—…ë°ì´íŠ¸ë˜ì–´, ì„œë¡œ ì˜ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- ë³€ë¶„ í•˜í•œ(ELBO)ì„ ì§ì ‘ì ìœ¼ë¡œ ìµœëŒ€í™”í•˜ì§€ ì•ŠìŒ

---

## 4. **VAEì™€ì˜ ì°¨ì´ì **

- **VAEëŠ” ELBO(ë³€ë¶„ í•˜í•œ)ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìµœëŒ€í™”**í•˜ë©°, ì¸ì½”ë”/ë””ì½”ë”ë¥¼ ë™ì‹œì— ì—”ë“œ-íˆ¬-ì—”ë“œë¡œ í•™ìŠµí•¨
- **wake-sleepì€ ë‘ ëª¨ë¸ì„ ë²ˆê°ˆì•„ê°€ë©° ë”°ë¡œ ì—…ë°ì´íŠ¸**í•¨

---

## 5. **ìš”ì•½**

- **wake-sleep algorithm**:  
  - ê³ ì „ì ì¸ ìƒì„± ëª¨ë¸ í•™ìŠµë²•
  - wake phase(ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í•™ìŠµ)ì™€ sleep phase(ìƒì„± ë°ì´í„° ê¸°ë°˜ í•™ìŠµ)ë¡œ ë²ˆê°ˆì•„ê°€ë©° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
  - VAEë³´ë‹¤ ìµœì í™”ê°€ ëœ íš¨ìœ¨ì ì´ê³ , ê·¼ì‚¬ì¹˜ê°€ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ
```

## 5. Conclusions

- Training latent representations with neural networks has been attempted for a long time.
  - However, VAE combines the latent variable model itself with neural networks.
  - Moreover, it allows us to observe continuous changes through interpolation in the latent space--making it interpretable!
  - Plus, after implementating the code, I found the structure to be very simple. There's nothing special: just an encoder that learns mu and sigma with a neural network, and a decoder that takes the latent representation and outputs an image.
  
- After reading about Diffusion models...
  - The notaiton is very different.
  - There are countless blogs explaining ELBO, but they all use exactly the same notation. It feels like everyone is copying explanations from someone who tried to make it easier to understand. The problem is, these notations are not from the VAE paper, but from Diffusion model papers. ğŸ¤”
  - I found the trick of making the probabilistic sampling distribution learnable by slightly sidestepping it very interesting. Also, making the prior a normal distribution so that the latent space generated by the parameters is mapped to the prior--this really felt like the ultimate in Bayesian thinking.
  
- Various ticks for estimating with Monte Carlo methods
  - But in practice, they're not really used... -> Even without multiple samples, the performance is good enough.

- I implemented it from scratch my self. (https://github.com/goranikin/DSBA-intern-study/tree/main/VAE)
  - 2 layers vs 5 layers... the results seem a bit clearer?
  - ![](/study/paper-review/vae/15.png)
  - Interpolation like this is also possible!
  - ![](/study/paper-review/vae/16.png)

---

ì—­ì‹œ ê³ ì „ì€ ì§ì ‘ ì½ëŠ” ê²Œ ë§ë‹¤ëŠ” ìƒê°ì´... VAEë¥¼ ì½ìœ¼ë‹ˆ í™•ì‹¤íˆ GANë„ ì½ì–´ë´ì•¼ í•œë‹¤ëŠ” ìƒê°ì´ íŒíŒ ë“ ë‹¤. ì–´ì°¨í”¼ ë­ ë‹¹ì¥ì— ì—°êµ¬ë¥¼ í•´ì•¼ë§Œ í•˜ëŠ” ìƒí™©ë„ ì•„ë‹ˆê³ , ì¢€ ë” ê·¼ë³¸ì ì¸ ë…¼ë¬¸ë“¤ì„ ì²œì²œíˆ ì½ì–´ë„ ë˜ëŠ” ì‹œê¸°ì¸ ë“¯í•˜ë‹ˆ... ì°¨ë¼ë¦¬ ì´ë¯¸ì§€ ìƒì„±ìª½ì— ìˆëŠ” ë‹¤ì–‘í•œ ê²ƒë“¤ì„ ì½ì–´ë³´ì!
