---
title: CLIP
publishDate: 2025-07-14
description: DSBA ì—°êµ¬ì‹¤ ì‚¬ì „í•™ìŠµ ë…¼ë¬¸ ë¦¬ë·° - Learning Transferable Visual Models From Natural Language Supervision
thumbnailUrl: /study/paper-review/clip/thm1.jpeg
---

7ì›”ë¶€í„° DSBA ì—°êµ¬ì‹¤ì—ì„œ ì¸í„´ì„ í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì €ì™€ ê°™ì€ AI ë¬´ì§€ë­ì´ë¥¼ ìœ„í•´ ê°ì‚¬í•˜ê²Œë„ ì‚¬ì „í•™ìŠµì´ë€ ì œë„ê°€ ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì‹¤ì˜ ê´€ì‹¬ ë¶„ì•¼ì¸ CV(Computer Vision), NLP(Natural Language Process), TS(Time Series)ì— ëŒ€í•´ ê°ê° ë¶„ì•¼ ë³„ ë…¼ë¬¸ë“¤ì„ ì½ê³ , Notionì— ì •ë¦¬í•œ ë’¤, ê¸°ì¡´ ì—°êµ¬ì› ë¶„ë“¤ ì•ì—ì„œ ì½ì€ ë‚´ìš©ì— ëŒ€í•´ 1) ì–´ë–»ê²Œ ì½ì—ˆëŠ”ì§€, 2) ëŠë‚€ ì ì€ ë¬´ì—‡ì¸ì§€, 3) ì½ì€ ë’¤ ê¶ê¸ˆí•œ ì ì€ ë¬´ì—‡ì¸ì§€ ë“¤ì„ ì´ì•¼ê¸°í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì´ì™€ ê´€ë ¨í•˜ì—¬ ì¸í„´ ë™ì•ˆ Notionì— ì ì—ˆë˜ ë‚´ìš©ì„ ë¸”ë¡œê·¸ì— ì˜®ê¸°ë ¤ í•©ë‹ˆë‹¤. ì´í›„ ë§ˆì§€ë§‰ ë‹¨ë½ì—ì„œ ì§§ì€ ê°œì¸ì ì¸ ìƒê°ì„ ë‚¨ê²¨ë³´ê² ìŠµë‹ˆë‹¤.

---

*This is a same content with the above paragraph.

I've been interning at the DSBA lab at SNU sine July. Thankfully, there is a preparatory program for freshman and intern who have little to no background in AI research. The program focuses on DSBA's main research interests: Computer Vision (CV), Natural Language Process (NLP), and Time Series (TS). As part of the program, I read papers in each field, summarize them in Notion, and then present to the current researchers about 1) how I approached the paper, 2) what I thought or felt about it, and 3) any questions that came up during my reading.

I'm planning to transfer the contents I wrote in Notion to my blog. At the end of each post, I'll also share some brief personal thoughts.

---

ë‹¤ìŒ êµ¬ë¶„ ì„ ê¹Œì§€ Notion ë‚´ìš©ì…ë‹ˆë‹¤.


[Paper Link](https://arxiv.org/abs/2103.00020)

## 1. Introduction

1) ë…¼ë¬¸ì´ ë‹¤ë£¨ëŠ” ë¶„ì•¼

- Vision model training with natural language
- Multi-modal model
- Natural Language Supervision & Weak-Supervision Training
- Transfer learning
- Zero-show or few-shot transfer task capability

2) í•´ë‹¹ taskì—ì„œ ê¸°ì¡´ ì—°êµ¬ í•œê³„ì  (ì •ìƒì ì¸ ë…¼ë¬¸ì´ë¼ë©´ introductionì—ì„œ ê°„ëµíˆ ì–¸ê¸‰í•¨)

- ê¸°ì¡´ Vision modelì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ Datasetë“¤ì€ ëŒ€ë¶€ë¶„ labeled data â†’ ì–‘ì´ ì ìŒ
- ë‹¤ì–‘í•œ downstream taskì— ëŒ€í•œ ê¸°ì¡´ modelì˜ zero-shot transfer ëŠ¥ë ¥ ë¶€ì¡±

3) ë…¼ë¬¸ì˜ contributions 

- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ í†µí•œ Weak-supervised training concept ì œì‹œ
    - (ë³¸ë¬¸ ë‚´ìš©) Could scalable pre-training methods which learn directly from web text result in a similar breakthrough in computer vision?
- zero-shot transferë¥¼ í†µí•œ task ìˆ˜í–‰ ëŠ¥ë ¥ì— ì´ˆì  â†’ ê¸°ì¡´ì—ëŠ” representationì— ì´ˆì 
    - ex) ê³ ì–‘ì´ ì´ë¯¸ì§€ë¥¼ ì–¼ë§ˆë‚˜ ì˜ í‘œí˜„í•˜ëŠ”ê°€? â†’ ê³ ì–‘ì´ì„ì„ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¥˜í•˜ëŠ”ê°€?
    - (ë³¸ë¬¸ ë‚´ìš©) While much research in the field of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task-learning capabilities of machine learning systems.


## 2. Related Work

- Improved deep metric learning with multi-class n-pair loss objective, Advances in neural information processing systems 2016
    - N-pair lossë¡œ í•œ ë²ˆì— ì—¬ëŸ¬ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ ë” íš¨ìœ¨ì ìœ¼ë¡œ â€œê±°ë¦¬â€ë¥¼ í•™ìŠµ
    - CLIPì˜ trainingì—ë„ í•´ë‹¹ N-pair contrastive loss ì‚¬ìš©
- Learning Visual Features from Large Weakly Supervised Data, ECCV 2016
    - ëŒ€ê·œëª¨ Weakly supervised ë°ì´í„°ì…‹ì„ í™œìš©í•´ CNN ê¸°ë°˜ ëª¨ë¸ì„ í•™ìŠµ
    - ë¼ë²¨ì´ ì•½í•˜ë”ë¼ë„ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ ì¼ë°˜í™”ëœ ëŠ¥ë ¥ì„ ì–»ì„ ìˆ˜ ìˆìŒ
    - CLIPì˜ Weak-supervised datasetì˜ motivation.
- Learning Visual N-grams from Web Data, IEEE 2017
    - ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ N-gramì„ ì—°ê²°í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµ
    - ë‹¨ì¼ ë‹¨ì–´ê°€ ì•„ë‹Œ, êµ¬ë‚˜ ë¬¸ì¥ ë‹¨ìœ„ ê°œë…ì„ ëª¨ë¸ë§
    - CLIP í•™ìŠµ ë°©ë²•ì˜ Motivation
- VirTex, ICMLM, ConVIRT
    - CLIPê³¼ ë¹„ìŠ·í•˜ê²Œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ í†µí•´ í•™ìŠµì‹œí‚¨ ëª¨ë¸ë“¤


## 3. ì œì•ˆ ë°©ë²•ë¡ 

### Main Idea & Contribution

1. Architecture

![image.png](/study/paper-review/clip/1.png)

Text encoder: Transformer - 63M parameter, 12 layer, 512 wide model with 8 heads

Image encoder

|  | Base style | EfficientNet style |  |
| --- | --- | --- | --- |
| ResNe | ResNet-50, ResNet-101 | RN50x4, RN50x16, RN50x64 |  |

|  | Base | Large |
| --- | --- | --- |
| ViT | 16, 32 | 14, 14@336px |

CLIP modelì€ ViT-L/14@336px ì‚¬ìš© (Best performance)

ê°€ì¥ í° ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ RN50x64ëŠ” 592ëŒ€ì˜ V100ìœ¼ë¡œ 18ì¼ê°„ í•™ìŠµ, ViT-L/14ëŠ” 256ëŒ€ì˜ V100ìœ¼ë¡œ 12ì¼ê°„ í•™ìŠµ ğŸ¤”

ì½”ë“œ êµ¬í˜„ (https://github.com/openai/CLIP)

ë¶ˆí•„ìš”í•œ ì½”ë“œ ìƒëµ

```python
class CLIP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 # vision
                 image_resolution: int,
                 vision_layers: Union[Tuple[int, int, int, int], int],
                 vision_width: int,
                 vision_patch_size: int,
                 # text
                 context_length: int,
                 vocab_size: int,
                 transformer_width: int,
                 transformer_heads: int,
                 transformer_layers: int
                 ):
        super().__init__()

        if isinstance(vision_layers, (tuple, list)):
            vision_heads = vision_width * 32 // 64
            self.visual = ModifiedResNet(
                layers=vision_layers,
                output_dim=embed_dim,
                heads=vision_heads,
                input_resolution=image_resolution,
                width=vision_width
            )
        else:
            vision_heads = vision_width // 64
            self.visual = VisionTransformer(
                input_resolution=image_resolution,
                patch_size=vision_patch_size,
                width=vision_width,
                layers=vision_layers,
                heads=vision_heads,
                output_dim=embed_dim
            )

        self.transformer = Transformer(
            width=transformer_width,
            layers=transformer_layers,
            heads=transformer_heads,
            attn_mask=self.build_attention_mask()
            
    def encode_image(self, image):
        return self.visual(image.type(self.dtype))

    def encode_text(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

        return x
            
    def forward(self, image, text):
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)

        # normalized features
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # cosine similarity as logits
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()

        # shape = [global_batch_size, global_batch_size]
        return logits_per_image, logits_per_text
```

ì„¸ë¶€ì ì¸ ResNet, ViT, Transformer ì˜ ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„ì€ ê¹ƒí—™ CLIP/clip/model.py ì°¸ê³  (êµ‰ì¥íˆ ê¹”ë”í•˜ê²Œ ì˜ ì •ë¦¬ë˜ì–´ìˆê³ , scratchë¶€í„° êµ¬í˜„ëœ ì½”ë“œë¥¼ ë³´ê¸°ì— ì¢‹ì€ ë“¯!)

1. Natural Language Supervision

(ë³¸ë¬¸ ë‚´ìš©) Itâ€™s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification. (â€¦ì¤‘ëµ) methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet.

GPT í•™ìŠµì— í•„ìš”í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ê¸ì–´ëª¨ì€ OpenAI ë‹µê²Œ ì—„ì²­ë‚œ í¬ë¡¤ë§ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ. 

1. 50ë§Œê°œì˜ Query ì„¤ì • í›„ (image, text) pair í¬ë¡¤ë§
2. ê° Queryë§ˆë‹¤ ìµœëŒ€ 2ë§Œê°œì˜ pair (í´ë˜ìŠ¤ ë¶„í¬ ë°¸ëŸ°ìŠ¤)
3. ì´ 400M â†’ GPT-2 í•™ìŠµì— ì“°ì¸ WebText datasetê³¼ ë¹„ìŠ·í•œ ì´ ë‹¨ì–´ ìˆ˜

1. Training Method

(ë³¸ë¬¸ ë‚´ìš©) Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N Ã—N possible (image, text) pairings across a batch actually occurred.

Architectureì— ì†Œê°œëœ ê²ƒì²˜ëŸ¼ NxN matrix of image & text pairë¥¼ ì–»ìŒ. ì—¬ê¸°ì„œ Diagonalì— ëŒ€í•´ì„œë§Œ positive, ë‚˜ë¨¸ì§€ëŠ” negativeë¡œ í•™ìŠµ

Q) False negativeì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ëŒ€ì‘í•˜ëŠ”ê°€?

A) ì¶©ë¶„í•œ ë°ì´í„°ì…‹ì´ í™•ë³´ë˜ë©´ ì €í’ˆì§ˆì´ë”ë¼ë„ í•™ìŠµì— ì§€ì¥ X (Weak-supervisionì˜ ì§„ê°€)

ì‹¤í—˜ì—ì„œ í™•ì¸í•´ì•¼ í•  ë¬¸ì œ â†’ pre-training ê³¼ì •ì—ì„œ ì •ë§ zero-shot transfer ì„±ëŠ¥ì´ ì¦ê°€í•  ê²ƒì¸ê°€? (GPTì²˜ëŸ¼)


## 4. ì‹¤í—˜ ë° ê²°ê³¼

ë³¸ ë…¼ë¬¸ì€ ë§¤ìš° ê¸´ ë¶„ëŸ‰ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , ëŒ€ë¶€ë¶„ ì‹¤í—˜ ì„¤ê³„ì™€ ê²°ê³¼, ê·¸ë¦¬ê³  ì˜ì˜ë¡œ ì´ë£¨ì–´ì§. ë”°ë¼ì„œ ê°ê°ì˜ ì‹¤í—˜ë“¤ì„ í•˜ë‚˜ì”© ì‚´í´ë³¼ ê²ƒ.

### 1) vs Visual N-Grams

![](/study/paper-review/clip/2.png)
Visual N-Gramsì™€ CLIPì€ ì„±ëŠ¥ì„ ë™ë“±í•˜ê²Œ ë¹„êµí•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ (ë³¸ë¬¸ì—ì„œ ì–¸ê¸‰). ë‹¤ë§Œ Zero-shot ì„±ëŠ¥ì— ìˆì–´ì„œ ì–¼ë§ˆë‚˜ í° ë°œì „ì„ ì´ë£¨ì—ˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì„±ëŠ¥í‘œ.

### 2) Image encodingì„ ìœ„í•œ Prompt Engineering

CLIPì€ pre-training ê³¼ì •ì—ì„œ datasetì˜ textê°€ imageì— ëŒ€í•œ ì„¤ëª…ì¸ â€˜êµ¬â€™ í˜¹ì€ â€˜ë¬¸ì¥â€™ìœ¼ë¡œ ì´ë£¨ì–´ì§(caption). ê·¸ëŸ¬ë‚˜ Evaluation datasetì—ì„œëŠ” ë‹¨ìˆœíˆ classì— ëŒ€í•œ labelë¡œë§Œ ì£¼ì–´ì§€ëŠ” ê²½ìš°ê°€ ìˆìŒ. ì´ì— ëŒ€í•´ êµ‰ì¥íˆ ê°„ë‹¨í•œ Prompt Engineeringìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ.

- ê¸°ë³¸ í…œí”Œë¦¿: â€œA photo of a labelâ€ â†’ ImageNetì—ì„œ 1.3% í–¥ìƒ
- Oxford-IIIT Pets: â€œA photo of a label}, a type of pet.â€
- ORC: ë¬¸ìë‚˜ ìˆ«ìì— quotes ì¶”ê°€ - ex) 1, a â†’ â€œ1â€, â€œaâ€

ì´ ì™¸ì—ë„ ë‹¤ì–‘í•œ context promptë¥¼ í†µí•´ ImageNetì—ì„œ 3.5% í–¥ìƒì„ ë³´ì„ (í•´ë‹¹ promtëŠ” ê¹ƒí—™ CLIP/data/prompts.mdì— ì •ë¦¬ë˜ì–´ ìˆìŒ)

Prompt ì˜ˆì‹œ

```jsx
templates = [
    'a photo of a {}.',
    'a blurry photo of a {}.',
    'a black and white photo of a {}.',
    'a low contrast photo of a {}.',
    'a high contrast photo of a {}.',
    'a bad photo of a {}.',
    'a good photo of a {}.',
    'a photo of a small {}.',
    'a photo of a big {}.',
    'a photo of the {}.',
    'a blurry photo of the {}.',
    'a black and white photo of the {}.',
    'a low contrast photo of the {}.',
    'a high contrast photo of the {}.',
    'a bad photo of the {}.',
    'a good photo of the {}.',
    'a photo of the small {}.',
    'a photo of the big {}.',
]
```

í•´ë‹¹ í”„ë¡¬í”„íŠ¸ë“¤ì„ ì•™ìƒë¸” â†’ ê°ê°ì˜ í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ì–»ì€ embeddingì˜ pooling ê°’

Q) Inference timeì´ ì¦ê°€í•˜ì§€ëŠ” ì•ŠëŠ”ì§€?

A) text encoderëŠ” ì •ë‹µ ë ˆì´ë¸”ì— ëŒ€í•´ ë¯¸ë¦¬ ê³„ì‚°í•œ ë’¤, inferenced image embeddingê³¼ì˜ similarityë§Œ ê³„ì‚°. ë”°ë¼ì„œ ìºì‹±ëœ ê°’ì„ ì‚¬ìš©í•˜ë©´ ë˜ë¯€ë¡œ prompt templateì´ ëŠ˜ì–´ë‚œë‹¤ê³  inference ë¹„ìš©ì´ ë¹„ë¡€í•´ì„œ ëŠ˜ì§€ ì•ŠìŒ!

### 3) Zero-Shot CLIP Performance

**vs fully Supervised models**
![](/study/paper-review/clip/3.png)

ê½¤ ë§ì€ Datasetì—ì„œ fully supervised baselineì„ ì›ƒë”.

ë‚˜ìœ ì„±ëŠ¥ì„ ë³´ì¸ ë°ì´í„°ì…‹ë“¤ì€ ë­˜ê¹Œ?

â†’ (ë³¸ë¬¸ ë‚´ìš©) we see that zero-shot CLIP is quite weak on several specialized, complex, or abstract tasks such as satellite image classification (EuroSAT and RESISC45), lymph node tumor detection (PatchCamelyon), counting objects in synthetic scenes (CLEVRCounts), self-driving related tasks such as German traffic sign recognition (GTSRB), recognizing distance to the nearest car (KITTI Distance).

ë§¤ìš° íŠ¹ìˆ˜í•˜ê±°ë‚˜ ë³µì¡í•œ ì´ë¯¸ì§€ ì¸ì‹ì—ëŠ” Transfer performanceê°€ ë‚˜ì˜ê²Œ ë‚˜íƒ€ë‚¨

ë‹¤ë§Œ ì´ëŸ° ì˜ˆì‹œëŠ” ì‚¬ì „í•™ìŠµì—ì„œ ê±°ì˜ ë§ˆì£¼ì¹˜ì§€ ëª»í•œ classì¼ í™•ë¥ ì´ ë†’ìœ¼ë©°, non-expert human ë˜í•œ zero-shotìœ¼ë¡œ í•´ë‹¹ taskë¥¼ ìˆ˜í–‰í•  ë•Œ ë³´ì¼ ì„±ê³¼ë¥¼ ìƒê°í•˜ë©´â€¦


**vs Few-shot models**
![](/study/paper-review/clip/4.png)
Linear Probe CLIPì€ LogisticRegressionìœ¼ë¡œ í•™ìŠµ í›„ í‰ê°€

ë†€ë¼ìš´ ì ì€ CLIPì˜ 4-shotê¹Œì§€ëŠ” Zero-shotì´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„

â†’ (ë³¸ë¬¸ ë‚´ìš©) Context-less example-based learning has the drawback that many different hypotheses can be consistent with the data, especially in the one-shot case.

Q) ì™œ ê·¸ëŸ´ê¹Œ?

A)

1. imageëŠ” textì— ë¹„í•´ ë§¤ìš° ë†’ì€ varianceì„ ë³´ì„. ê°™ì€ classì—¬ë„ ì¡°ëª…, ê°ë„, ë°°ê²½, í•´ìƒë„, ìŠ¤íƒ€ì¼, ë…¸ì´ì¦ˆ ë“±ì— ë”°ë¼ ì™„ì „íˆ ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ.
2. textëŠ” êµ¬ì¡°ì ì´ê³  ì˜ë¯¸ê°€ ëª…í™•í•˜ê²Œ í‘œí˜„ë¨.

â†’ ê³ ì–‘ì´ì— ëŒ€í•´ ì‚¬ì§„ìœ¼ë¡œ ì œì‹œí•˜ëŠ” ê²ƒê³¼ ë‹¨ì–´ë¡œ ì œì‹œí•˜ëŠ” ê²ƒì˜ ëª…í™•ì„± ì°¨ì´.

ì´ë¯¸ì§€ì˜ ì‹œê°ì  íŠ¹ì§•ì„ ì¶©ë¶„íˆ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë§ì€ few-shot samplesë¥¼ ì œê³µí•´ì•¼ í•¨.

Q) ê·¸ë ‡ë‹¤ë©´ imageì˜ pixelì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡ few-shotì— ë” ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•´ì§ˆê¹Œ?

A) (by GPT)

```bash
ë„¤, ë§ìŠµë‹ˆë‹¤!  
**ì´ë¯¸ì§€ì˜ í”½ì…€(í•´ìƒë„)ì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡, few-shot í•™ìŠµì—ì„œ ë” ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•´ì§€ëŠ” ê²½í–¥ì´ ì»¤ì§‘ë‹ˆë‹¤.**

---

## ì´ìœ ë¥¼ ìì„¸íˆ ì„¤ëª…í•˜ë©´

### 1. **ê³ ì°¨ì› ê³µê°„ì˜ ì¼ë°˜í™” ì–´ë ¤ì›€**
- ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ ë†’ì•„ì§ˆìˆ˜ë¡, ê° ìƒ˜í”Œì´ ê°€ì§€ëŠ” ì •ë³´(íŠ¹ì§•)ë„ í›¨ì”¬ ë” ë§ì•„ì§‘ë‹ˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´, 32Ã—32 í”½ì…€ ì´ë¯¸ì§€ëŠ” 1,024ì°¨ì›, 224Ã—224 í”½ì…€ ì´ë¯¸ì§€ëŠ” 50,176ì°¨ì›ì…ë‹ˆë‹¤.
- ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” ë°ì´í„°ê°€ í¬ë°•(sparse)í•´ì§€ê³ ,  
  í´ë˜ìŠ¤ ê²½ê³„ë¥¼ ì˜ ì¡ìœ¼ë ¤ë©´ ë” ë§ì€ ì˜ˆì‹œê°€ í•„ìš”í•©ë‹ˆë‹¤.

### 2. **ì‹œê°ì  ë‹¤ì–‘ì„± ì¦ê°€**
- í•´ìƒë„ê°€ ë†’ì•„ì§€ë©´,  
  ê°™ì€ í´ë˜ìŠ¤ ë‚´ì—ì„œë„ ë” ë‹¤ì–‘í•œ ì„¸ë¶€ì ì¸ íŠ¹ì§•(ë¬´ëŠ¬, ì§ˆê°, ë°°ê²½ ë“±)ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ëª¨ë¸ì´ ì´ëŸ° ì„¸ë¶€ì ì¸ ì°¨ì´ë¥¼ ì¼ë°˜í™”í•˜ë ¤ë©´ ë” ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.

### 3. **ì˜¤ë²„í”¼íŒ… ìœ„í—˜ ì¦ê°€**
- ìƒ˜í”Œ ìˆ˜ê°€ ì ì€ë° í”½ì…€(íŠ¹ì§•) ìˆ˜ê°€ ë§ìœ¼ë©´,  
  ëª¨ë¸ì´ ì†Œìˆ˜ ìƒ˜í”Œì˜ ì„¸ë¶€ì ì¸ íŠ¹ì§•ì— ê³¼ë„í•˜ê²Œ ì í•©(overfit)ë  ìœ„í—˜ì´ ì»¤ì§‘ë‹ˆë‹¤.
- ì´ë¥¼ ë°©ì§€í•˜ë ¤ë©´ ë” ë§ì€ ìƒ˜í”Œë¡œ í´ë˜ìŠ¤ì˜ ì „ì²´ì ì¸ íŠ¹ì§•ì„ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.

---

## ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ê°€?

- **í•´ìƒë„ê°€ ë†’ì„ìˆ˜ë¡**  
  - ê°™ì€ í´ë˜ìŠ¤ì˜ ë‹¤ì–‘í•œ ëª¨ìŠµ(í¬ì¦ˆ, ë°°ê²½, ì¡°ëª… ë“±)ì„ ë” ë§ì´ í¬ì°©í•  ìˆ˜ ìˆìŒ
  - ëª¨ë¸ì´ ì´ëŸ° ë‹¤ì–‘ì„±ì„ í•™ìŠµí•˜ë ¤ë©´ ë” ë§ì€ ì˜ˆì‹œê°€ í•„ìš”í•¨
- **few-shot í•™ìŠµì—ì„œ**  
  - ì €í•´ìƒë„ ì´ë¯¸ì§€(ì˜ˆ: MNIST, CIFAR-10)ëŠ” 1~5ì¥ë§Œìœ¼ë¡œë„ ì–´ëŠ ì •ë„ ì¼ë°˜í™” ê°€ëŠ¥
  - ê³ í•´ìƒë„ ì´ë¯¸ì§€(ì˜ˆ: ImageNet, ì˜ë£Œ ì˜ìƒ ë“±)ëŠ” 10ì¥ ì´ìƒ, ë•Œë¡œëŠ” ìˆ˜ì‹­~ìˆ˜ë°± ì¥ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ

---

## ê²°ë¡ 

- **ì´ë¯¸ì§€ì˜ í”½ì…€(í•´ìƒë„)ì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡, few-shot í•™ìŠµì—ì„œ ë” ë§ì€ ìƒ˜í”Œì´ í•„ìš”í•´ì§€ëŠ” ê²½í–¥ì´ ì»¤ì§„ë‹¤**ëŠ” ê²ƒì´ ë§ìŠµë‹ˆë‹¤.
- ì´ëŠ” ê³ ì°¨ì› ë°ì´í„°ì˜ ì¼ë°˜í™” ì–´ë ¤ì›€, ì‹œê°ì  ë‹¤ì–‘ì„±, ì˜¤ë²„í”¼íŒ… ìœ„í—˜ ë“± ë•Œë¬¸ì…ë‹ˆë‹¤.
```


**the # of samples to deal with zero-shot model**
![](/study/paper-review/clip/5.png)

ê° ë°ì´í„°ì…‹ë³„ zero-shot ì„±ëŠ¥ì„ ë”°ë¼ ì¡ê¸° ìœ„í•œ few-shot sampleì˜ ê°œìˆ˜

ìƒìœ„ 4ê°œ datasetì— ëŒ€í•´ ì‚´í´ë³´ì

FER2013: í‘œì • ì¸ì‹ â†’ ì‚¬ëŒë§ˆë‹¤ ë‹¤ë¥¸ í‘œí˜„ ë°©ì‹ìœ¼ë¡œ ì¸í•´ ì¼ê´€ëœ class representation í•™ìŠµ ë¶ˆë¦¬

CIFAR10: ì €í•´ìƒë„ ë° í´ë˜ìŠ¤ ë‚´ ë‹¤ì–‘í•œ ë³€ì´

OxfordPets: í’ˆì¢… ë¹„êµì—ì„œ ì–´ë ¤ì›€ì„ ê²ªì—ˆì„ ê±°ë¼ ìœ ì¶”

Food101: ê°™ì€ ìŒì‹ì´ë”ë¼ë„ ë‹¤ë¥¸ ì¡°ë¦¬ë²•ì´ë‚˜ ê°ë„ì— ë”°ë¼ ì™„ì „íˆ ë‹¬ë¼ ë³´ì¼ ìˆ˜ ìˆìŒ

ë‹¤ë§Œ ë°ì´í„°ì…‹ì„ í•˜ë‚˜í•˜ë‚˜ ìì„¸íˆ í™•ì¸í•´ë³¸ ê±´ ì•„ë‹ˆê³ â€¦ ì´ì •ë„ íŠ¹ì„±ë§Œ ìˆìŒì„ í™•ì¸í•¨. í´ë˜ìŠ¤ ë‚´ì—ì„œ í¬ì°©í•´ì•¼ í•˜ëŠ” íŠ¹ì§•ì´ ë§¤ìš° ë§ê±°ë‚˜(FER2013), class ì™¸ ë°°ê²½ ì •ë³´ê°€ ë§ì„ ê²½ìš°(Food101) few-shotì´ ë¶ˆë¦¬í•˜ë‹¤ê³  íŒë‹¨.


**A theoretical limit performance and the real performances**
![](/study/paper-review/clip/6.png)
ì´ë¡ ì  ì„±ëŠ¥ê³¼ zero-shotì˜ ë¹„êµ
![](/study/paper-review/clip/7.png)
ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ ì„±ëŠ¥ ë¹„êµ â†’ ê¾¸ì¤€íˆ ì„±ëŠ¥ ìƒìŠ¹. ë” í° ëª¨ë¸ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì„ ê±°ë€ ê¸°ëŒ€

### 4) Representation Learning

ì €ìë“¤ì€ Representation learning capabilityë³´ë‹¤ëŠ” task leaning capabilityê°€ ì¤‘ìš”í•˜ë‹¤ê³ ëŠ” í–ˆì§€ë§Œ, ì–´ì¨Œê±°ë‚˜ ê¸°ì¡´ í‰ê°€ ë°©ì‹ìœ¼ë¡œ í™•ì¸í•˜ê³ ì í•¨.

ì‹¤í—˜ ë°©ë²•

- ê°ê°ì˜ modelì—ì„œ classification spaceë¡œ projection ì œê±°
- scikit-learnâ€™s L-BFGS implementation ì‚¬ìš©
- ê°ê°ì˜ datasetë§ˆë‹¤ ì í•©í•œ metric í‰ê°€ (í•´ë‹¹ ë¶€ë¶„ì€ ì•„ë˜ ì‚¬ì§„ ì²¨ë¶€, ë³¸ë¬¸ ë¶€ë¡ Aì— ì¡´ì¬)

**Dataset and meta-information**
![](/study/paper-review/clip/8.png)
![](/study/paper-review/clip/9.png)

í•´ë‹¹ ê²°ê³¼ì˜ ì‹œê°í™” figure
![](/study/paper-review/clip/10.png)

Large parameter CLIP modelì´ ìš°ìˆ˜í•œ embedding ëŠ¥ë ¥ì„ ë³´ì„!

Q) Linear probeê°€ embedding ìš°ìˆ˜ì„±ì„ ì²´í¬í•˜ëŠ” ì¢‹ì€ ì§€í‘œì¸ê°€?
A) ë§Œì•½ embeddingì´ ë§¤ìš° ê°„ë‹¨í•œ linear layerë§Œìœ¼ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  ê°€ì •. ì´ëŠ” ì„ë² ë”©ì´ ì—¬ëŸ¬ ì´ë¯¸ì§€ ì†ì—ì„œ ë‹¤ì–‘í•œ ì‹œê°ì /ì¶”ìƒì  íŠ¹ì§•ì„ ì˜ ë‹´ê³  ìˆìœ¼ë©°, íŠ¹ì • classë§Œì„ ì˜ ì¸ì‹í•˜ê¸°ë³´ë‹¤ ì¼ë°˜ì ì¸ embeddingì„ ê°–ê³ ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ. ì´ëŠ” ë‹¤ì‹œ ë§í•´ í•´ë‹¹ ëª¨ë¸ì´ ì´ë¯¸ì§€ ë‚´ì—ì„œ ë‹¤ì–‘í•œ classë¥¼ ëŒ€ìƒìœ¼ë¡œ featureë¥¼ ë½‘ì•„ë‚´ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ â†’ ë†’ì€ embedding representation!

### 5) Robustness to Natural Distribution Shift

Introductionì—ì„œ ì¤„ê¸°ì°¨ê²Œ ì„¤ëª…í•´ì˜¨ transfer capabilityì— ëŒ€í•œ ì‹¤í—˜

ë³¸ë¬¸ì—ì„œëŠ” Robustnessë¥¼ ë‘ ì¢…ë¥˜ë¡œ ì„¤ëª…

1. Relative Robustness: it captures any improvement in out-of-distribution accuracy.
2. Effective Robustness: it measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy.

ë‹¹ì—°íˆ ë‘˜ ë‹¤ ë†’ì•„ì•¼ Robustí•œ ëª¨ë¸ì´ê² ì£ ?

![](/study/paper-review/clip/11.png)
ë‹¤ë¥¸ ë¶„í¬ì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ë²„ë¦¬ëŠ” CLIP. ImageNet-Rì„ ë³´ë©´ ImageNet ResNet101ì€ ì•½ 40% ê°€ëŸ‰ ê°ì†Œí•œ ì„±ëŠ¥ì´ CLIPì€ ë˜ë ¤ 12%ê°€ ì¦ê°€í–ˆë‹¤. ìì—°ì–´ë¡œ ì´ë¯¸ì§€ì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ì´ ì¼ë°˜í™”ì— ì–¼ë§ˆë‚˜ ê°•ë ¥í•œì§€ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œ. ì• ì´ˆì— GPTê°€ ì„ ë¡€ë¥¼ ë³´ì—¬ì£¼ê¸°ë„ í–ˆê³ â€¦

![](/study/paper-review/clip/12.png)
ImageNetì— ëŒ€í•´ í•™ìŠµí•  ê²½ìš° ë‹¤ë¥¸ datasetì— ëŒ€í•œ robustnessê°€ ë–¨ì–´ì§€ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆìŒ. ëŒ€ì‹  zero-shot CLIPì— class nameë¥¼ ë§ì¶°ì¤„ ê²½ìš° íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ì„±ëŠ¥ í–¥ìƒ.

![](/study/paper-review/clip/13.png)
Zero-shotì´ Few-shot ë³´ë‹¤ í›¨ì”¬ robustí•¨.

### 6) Comparison to Human Performance

CLIP is better

### 7) Data overlap

pre-training model í‰ê°€ê°€ ì–´ë ¤ìš´ ì´ìœ  ì¤‘ í•˜ë‚˜ë¡œ eval datasetì´ í•™ìŠµì— ì„ì—¬ ë“¤ì–´ê°€ëŠ” ì¼ì´ ë°œìƒ. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Overlap detectorë¥¼ ë§Œë“¤ì–´ì„œ Clean subsetì„ ë§Œë“¤ì—ˆê³ , ëª‡ datasetì„ ì œì™¸í•˜ë©´ í—ˆìš©í• ë§Œí•œ ë²”ìœ„ì— ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨. ë‹¤ë§Œ CLIP ì´í›„ ë§Œë“¤ì–´ì§„ eval datasetìœ¼ë¡œ ì •ë°€í•œ í‰ê°€ í•„ìš”.

(ë³¸ë¬¸ ë‚´ìš©) Creating a new benchmark of tasks designed explicitly to evaluate broad zero-shot transfer capabilities, rather than re-using existing supervised datasets, would help address these issues.


### 8) Bias

ëŒ€ê·œëª¨ ì¸í„°ë„· ë°ì´í„°ë¡œ í•™ìŠµëœ ë§Œí¼ Sexual, Race, Job bias ì¡´ì¬
![](/study/paper-review/clip/14.png)
![](/study/paper-review/clip/15.png)

InstructionGPTì²˜ëŸ¼ íš¨ê³¼ì ì¸ Intruction Tuningì´ í•„ìš”í•  ê²ƒ.

## 5. ê²°ë¡  (ë°°ìš´ ì )

ëŒ€ê·œëª¨ ìì›ì„ ë™ì›í•´ ë‚˜ì˜¨ ê°–ê°€ì§€ Insightì˜ ë²”ëŒ ë•ì—, ë§¤ìš° ê¸´ ê¸¸ì´ì˜€ìŒì—ë„ ìƒë‹¹íˆ ì¬ë¯¸ìˆê²Œ ì½ì—ˆë‹¤. íŠ¹íˆ ì‹¤í—˜ í•˜ë‚˜í•˜ë‚˜ë§ˆë‹¤ ì—°êµ¬ ì£¼ì œê¸‰.

ì´ ì™¸ì—ë„ ê°œì¸ì ìœ¼ë¡œ Speech-to-Textìª½ ë…¼ë¬¸ì„ ì‚´í´ë³´ëŠ”ë°, CLIPê³¼ ìœ ì‚¬í•˜ê²Œ Whisperë„ natural supervision datasetì„ êµ¬ì¶•í•´ pre-trainingí•œ ëª¨ë¸ì„ ì œì‹œí•˜ê³  ë‹¤ì–‘í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³µìœ í•¨. GPT-3ë„ ê·¸ë ‡ê³  ì‚¬ì‹¤ìƒ í˜„ì¬ AI ëª¨ë¸ë“¤ì˜ í† ëŒ€ë¥¼ ì „ë¶€ OpenAIê°€ ë§Œë“  ì…ˆâ€¦ íŠ¹íˆ ë³¸ë¬¸ ì•ˆì—ì„œë„ â€˜Representation learning capabilityâ€™ê°€ ì•„ë‹Œ â€˜Task learning capabilityâ€™ì— ì§‘ì¤‘í•˜ê³  â€˜í˜„ì‹¤ ì„¸ê³„ì— ì˜ë¯¸ ìˆëŠ” AI modelâ€™ì„ ë§Œë“¤ê³  ì‹¶ë‹¤ëŠ” ëšœë ·í•œ ëª©ì ì´ ë³´ì˜€ë‹¤. ì§€ê¸ˆì´ì•¼ Closed modelë¡œ ë°”ë€Œì—ˆì§€ë§Œ ì§€ê¸ˆê¹Œì§€ì˜ ê²°ê³¼ë§Œ í•´ë„ ì •ë§ OpenAIì˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‹¤ì²œí•œ ë“¯. íŠ¹íˆ 25í˜ì´ì§€ ìš°ì¸¡ì— ì íŒ ë¬¸ë‹¨ì— ê°ë™ë¨¹ìŒ.

- We hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models, and we are excited to engage with the research community on such questions.

---

ì§„ì§œ ì†Œê°
